{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5db779a0",
   "metadata": {},
   "source": [
    "# CLASE 1.3: Descomposiciones matriciales\n",
    "\n",
    "## Introducción.\n",
    "En las secciones anteriores estudiamos algunas formas de manipular y obtener ciertas métricas para los vectores, proyecciones de esos vectores con respecto a determinados subespacios vectoriales y transformaciones lineales. Las aplicaciones y transformaciones que permiten operar con vectores pueden ser convenientemente descritas por medio de matrices. Además, la mayoría de los conjuntos de datos *bien comportados* que podemos encontrar en el mundo real vienen especificados en estructuras que pueden ser arregladas y/o representadas igualmente por medio de matrices. Por ejemplo, las filas de estos conjuntos de datos suelen representar **registros** u **observaciones** (como personas, fechas, unidades, entre otras) y las columnas suelen representar diferentes **atributos** para cada fila (como edad, altura, propiedades extensivas de algún fenómeno o el valor de alguna variable en el tiempo). En esta sección, presentaremos tres aspectos relativos a las matrices: Cómo **resumirlas**, como **descomponerlas** y como utilizar tales descomposiciones para construir **aproximaciones** para determinadas matrices.\n",
    "\n",
    "A diferencia de la sección anterior, en ésta volveremos a escribir algo de código, a fin de corresponder algunos conceptos esenciales con librerías tales como <font color='purple'>Numpy</font> o <font color='purple'>Scipy</font>. No será demasiado código, pero sí el suficiente para darle algo de sentido práctico a los conceptos que desarrollaremos desde la perspectiva computacional."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e7992dc1",
   "metadata": {},
   "source": [
    "## Determinantes.\n",
    "\n",
    "### Un interludio previo.\n",
    "El concepto de determinante corresponde a otro de los elementos más importantes del álgebra lineal. Corresponde a un objeto matemático que es importante en el análisis y solución de sistemas de ecuaciones lineales (los mismos que vimos en el inicio de la [clase 1.1](https://github.com/rquezadac/udd_data_science_lectures/blob/main/PARTE%20I%20-%20Fundamentos%20matem%C3%A1ticos%20elementales/clase_1_1.ipynb)) y que puede expresarse por medio de una función, denominada como **función determinante**, y que permite aplicar cualquier matriz cuadrada de orden $n$ en el cuerpo $\\mathbb{K}$ donde sus elementos están definidos. Dicha función, para una matriz $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$, se denota como $\\det(\\mathbf{A})=|\\mathbf{A}|$, y es tal que $\\left| \\  \\cdot \\  \\right|  :\\mathbb{K}^{n} \\times \\mathbb{K}^{n} \\longrightarrow \\mathbb{K}$, pudiéndose escribir el determinante de la matriz $\\mathbf{A}$ como\n",
    "\n",
    "$$\\mathbf{A} =\\left\\{ a_{ij}\\in \\mathbb{K} \\right\\}  =\\left( \\begin{matrix}a_{11}&a_{12}&\\cdots &a_{1n}\\\\ a_{21}&a_{22}&\\cdots &a_{2n}\\\\ \\vdots &\\vdots &\\ddots &\\vdots \\\\ a_{n1}&a_{n2}&\\cdots &a_{nn}\\end{matrix} \\right)  \\in \\mathbb{K}^{n\\times n} \\wedge \\det \\left( \\mathbf{A} \\right)  =\\left| \\begin{matrix}a_{11}&a_{12}&\\cdots &a_{1n}\\\\ a_{21}&a_{22}&\\cdots &a_{2n}\\\\ \\vdots &\\vdots &\\ddots &\\vdots \\\\ a_{n1}&a_{n2}&\\cdots &a_{nn}\\end{matrix} \\right|  \\in \\mathbb{K}$$\n",
    "<p style=\"text-align: right;\">$(3.1)$</p>\n",
    "\n",
    "**Ejemplo 3.1:** Comenzaremos a motivar el estudio de los determinantes explorando la posibilidad de que una matriz cuadrada, digamos $\\mathbf{A}\\in \\mathbb{R}^{n\\times n}$, sea **invertible**. Para los casos de menor dimensión, ya conocemos los casos que aseguran que $\\mathbf{A}$ cumpla con esta condición. Por ejemplo, si $\\mathbf{A}\\in \\mathbb{R}^{1\\times 1}$ (es decir, $\\mathbf{A}$ es un escalar), sabemos que $\\mathbf{A}=a\\ \\Longrightarrow \\mathbf{A}^{-1}=1/a$, lo que implica que $\\mathbf{A}$ tiene una inversa siempre que $a\\neq 0$. Para el caso de matrices de $2\\times 2$, sabemos que la inversa $\\mathbf{A}^{-1}$ cumple con la condición de que $\\mathbf{A}\\mathbf{A}^{-1}=\\mathbf{I}_{2}$. De esta manera, podemos escribir\n",
    "\n",
    "$$\\mathbf{A} =\\left( \\begin{matrix}a_{11}&a_{12}\\\\ a_{21}&a_{22}\\end{matrix} \\right)  \\Longrightarrow \\mathbf{A} \\mathbf{A}^{-1} =\\mathbf{I}_{2} \\Longleftrightarrow \\mathbf{A}^{-1} =\\frac{1}{a_{11}a_{22}-a_{12}a_{21}} \\left( \\begin{matrix}a_{22}&-a_{12}\\\\ -a_{21}&a_{11}\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.2)$</p>\n",
    "\n",
    "Por lo tanto, $\\mathbf{A}$ es invertible si y sólo si $a_{11}a_{22}-a_{12}a_{21}\\neq 0$. Para matrices de $2\\times 2$, dicha cantidad corresponde al **determinante** de la matriz respectiva. De esta manera, para la matriz $\\mathbf{A} =\\left\\{ a_{ij}\\right\\}  \\in \\mathbb{R}^{2\\times 2}$, su determinante se define como\n",
    "\n",
    "$$\\det \\left( \\mathbf{A} \\right)  =\\det \\left( \\begin{matrix}a_{11}&a_{12}\\\\ a_{21}&a_{22}\\end{matrix} \\right)  =\\left| \\begin{matrix}a_{11}&a_{12}\\\\ a_{21}&a_{22}\\end{matrix} \\right|  =a_{11}a_{22}-a_{12}a_{21}$$\n",
    "<p style=\"text-align: right;\">$(3.3)$</p>\n",
    "◼︎\n",
    "\n",
    "El ejemplo (3.1) permite establecer un hecho que puede ser generalizado a conjuntos de mayor dimensión: Una matriz es invertible siempre que su determinante no sea nulo. Formalicemos este hecho mediante un teorema.\n",
    "\n",
    "**<font color='crimson'>Teorema 3.1 – Existencia de matriz inversa:</font>** *Sea $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$ una matriz cuadrada con elementos en el cuerpo $\\mathbb{K}$. Entonces $\\mathbf{A}$ se dirá **invertible** o **no singular** (es decir, existe la matriz inversa $\\mathbf{A}^{-1}$) si y solo si $\\det(\\mathbf{A})\\neq 0$.*\n",
    "◆\n",
    "\n",
    "Ya disponemos de una expresión cerrada que permite calcular el determinante de cualquier matriz de dimensión $2\\times 2$. Sin embargo, no es tan sencillo generalizar dicho cálculo para dimensiones superiores. Por ejemplo, para el caso de matrices de $3\\times 3$, es común el cálculo de sus determinantes mediante la llamada regla de Sarrus:\n",
    "\n",
    "$$\\det \\left( \\begin{matrix}a_{11}&a_{12}&a_{13}\\\\ a_{21}&a_{22}&a_{23}\\\\ a_{31}&a_{32}&a_{33}\\end{matrix} \\right)  =a_{11}a_{22}a_{33}+a_{21}a_{32}a_{13}+a_{31}a_{12}a_{23}-a_{31}a_{22}a_{13}-a_{11}a_{32}a_{23}-a_{21}a_{12}a_{33}$$\n",
    "<p style=\"text-align: right;\">$(3.4)$</p>\n",
    "\n",
    "Si bien, en un principio, la regla de Sarrus parece una fórmula complicada de entender, ésta no es más que un recurso mnemotécnico, ya que los triples productos involucrados en la fórmula y sus signos guardan relación con las diagonales (y subdiagonales) presentes en la matriz correspondiente, como se observa en el esquema de la Fig. (3.1)\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"figures/fig_3_1.png\" width=\"650\"></p>\n",
    "<p style=\"text-align: center;\">Fig. (3.1): Esquema que ilustra cómo opera la regla de Sarrus</p>\n",
    "\n",
    "Recordemos que, al resolver sistemas de ecuaciones lineales, nuestro objetivo era transformar la matriz ampliada de un sistema en una tal que sólo tuviera elementos no nulos en su región superior derecha (matriz triangular superior), aunque también es posible operar para llegar al caso opuesto, donde la matriz resultante tenga elementos no nulos en su región inferior izquierda (matriz triangular inferior). Este tipo de matrices fueron formalizadas previamente en la definición (1.7).\n",
    "\n",
    "Para una matriz triangular, digamos $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$, definimos su determinante como\n",
    "\n",
    "$$\\det \\left( \\mathbf{A} \\right)  =\\prod^{n}_{i=1} a_{ii}$$\n",
    "<p style=\"text-align: right;\">$(3.5)$</p>\n",
    "\n",
    "Donde $a_{ii}$ es el correspondiente elemento relativo a la diagonal principal de la matriz $\\mathbf{A}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b96e74aa",
   "metadata": {},
   "source": [
    "### Interpretación geométrica del determinante.\n",
    "Si una matriz $\\mathbf{A}\\in \\mathbb{R}^{n\\times n}$ tiene elementos $a_{ij}\\in \\mathbb{R}$, entonces puede ser utilizada para representar dos transformaciones lineales. Una que aplica la base canónica de $\\mathbb{R}^{n}$ a las filas de $\\mathbf{A}$, y otra que aplica la misma base a las columnas de $\\mathbf{A}$. Cualquiera sea el caso, si la matriz $\\mathbf{A}$ es de $2\\times 2$, las imágenes de cada uno de los vectores de la base canónica de $\\mathbb{R}^{2}$ forman un paralelógramo que representa la imagen del cuadrado unitario bajo la transformación lineal respectiva, y cuyos vértices se corresponden con combinaciones de los elementos de $\\mathbf{A}$, como se observa en la Fig. (3.2a).\n",
    "\n",
    "Si $\\mathbf{A} =\\left\\{ {}a_{ij}\\right\\}  \\in \\mathbb{R}^{2\\times 2}$ es la matriz que conforma los vértices del paralelógramo en la Fig. (3.2a), se tendrá que el área encerrada por el mismo será igual a $\\det \\left( \\mathbf{A} \\right)  =a_{11}a_{22}-a_{12}a_{21}$. Para mostrar este resultado, podemos considerar que los elementos de la matriz $\\mathbf{A}$ corresponden a vectores que representan los vértices del paralelógramo. Si uno de los vértices es el origen del sistema de coordenadas, los vectores $\\mathbf{a} =\\left( a_{11},a_{12}\\right)$ y $\\mathbf{b} =\\left( a_{21},a_{22}\\right)$ serán los vértices más cercanos al origen, mientras que el vértice opuesto será igual a la suma $\\mathbf{a} +\\mathbf{b} =\\left( a_{11}+a_{21},a_{12}+a_{22}\\right)$. El área del paralelógramo puede expresarse igualmente como $\\left\\Vert \\mathbf{a} \\right\\Vert  \\left\\Vert \\mathbf{b} \\right\\Vert  \\mathrm{sen} \\left( \\theta \\right)$, donde $\\theta$ es el ángulo formado por los vectores $\\mathbf{a}$ y $\\mathbf{b}$. Si consideramos la proyección ortogonal del vector $\\mathbf{a}$ sobre $\\mathbf{b}$ (que llamamos $\\pi_{\\mathbf{b}}(\\mathbf{a})$), podemos escribir\n",
    "\n",
    "$$\\mathrm{Area} =\\left\\Vert \\mathbf{a} \\right\\Vert  \\left\\Vert \\mathbf{b} \\right\\Vert  \\mathrm{sen} \\left( \\theta \\right)  =\\left\\Vert \\pi_{\\mathbf{b} } \\left( \\mathbf{a} \\right)  \\right\\Vert  \\left\\Vert \\mathbf{b} \\right\\Vert  \\cos \\left( \\frac{\\pi }{2} -\\theta \\right)  =a_{11}a_{12}-a_{12}a_{21}=\\det \\left( \\mathbf{A} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.6)$</p>\n",
    "\n",
    "La interpretación geométrica anterior puede extenderse al caso de matrices de $3\\times 3$, considerando en este caso un paralelepípedo generado por las submatrices columna que generan la matriz completa $\\mathbf{A} =\\left\\{ a_{ij}\\right\\}  \\in \\mathbb{R}^{3\\times 3}$. Si tales submatrices son representadas como $\\mathbf{r}_{1}$, $\\mathbf{r}_{2}$ y $\\mathbf{r}_{3}$, entonces el volumen del paralelepípedo es igual a $V=\\det \\left( \\left( \\mathbf{r}_{1} ,\\mathbf{r}_{2} ,\\mathbf{r}_{3} \\right)  \\right)  =\\det \\left( \\mathbf{A} \\right)$, tal y como se ilustra en la Fig (3.2b).\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"figures/fig_3_2.png\" width=\"900\"></p>\n",
    "<p style=\"text-align: center;\">Fig. (3.2): (a) Esquema que muestra cómo el determinante de una matriz de $2\\times 2$ permite transformar un cuadrado unitario en un paralelógramo cuyos vértices son las productos que componen dicho determinante y su área será igual al valor de tal determinante; (b) Misma transformación para el caso de una matriz de $3\\times 3$. En este caso, la transformación se aplica sobre un paralelepípedo, obteniéndose un trapezoedro cuyo volumen es igual al correspondiente determinante</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f45c8fa9",
   "metadata": {},
   "source": [
    "### Propiedades de los determinantes.\n",
    "El cálculo de un determinante para una matriz arbitraria $\\mathbf{A} =\\left\\{ a_{ij}\\right\\}  \\in \\mathbb{R}^{n\\times n}$ requiere de un algoritmo generalizado para poder resolver los casos en los cuales $n>3$. Existen varios procedimientos para ello, siendo indudablemente el más popular el **método de Laplace**, que definiremos a continuación.\n",
    "\n",
    "**<font color='blue'>Definición 3.1 – Determinante:</font>** Sea $\\mathbf{A} =\\left\\{ a_{ij}\\right\\}  \\in \\mathbb{R}^{n\\times n}$. Definimos el **determinante** de la matriz $\\mathbf{A}$ como\n",
    "\n",
    "1. Respecto a la columna $j$: $\\det \\left( \\mathbf{A} \\right)  =\\sum^{n}_{k=1} \\left( -1\\right)^{k+j}  a_{kj}\\det \\left( \\mathbf{A}_{kj} \\right)$.\n",
    "2. Respecto a la fila $j$: $\\det \\left( \\mathbf{A} \\right)  =\\sum^{n}_{k=1} \\left( -1\\right)^{k+j}  a_{jk}\\det \\left( \\mathbf{A}_{jk} \\right)$.\n",
    "\n",
    "En la fórmulas anteriores, conocidas en la práctica como **expansiones de Laplace**, $\\mathbf{A}_{jk}$ corresponde a la submatriz resultante de eliminar de $\\mathbf{A}$ la fila $j$ y la columna $k$ y se denomina como **menor complementario** en la posición $(j, k)$, mientras que el número real $\\triangle_{jk} =\\left( -1\\right)^{k+j}  a_{jk}$ es llamado **cofactor** asociado al $jk$-ésimo elemento de la matriz $\\mathbf{A}$.\n",
    "\n",
    "Resulta sencillo darnos cuenta de que, si bien las expansiones de Laplace nos permiten obtener una fórmula cerrada para el cálculo del determinante de cualquier matriz cuadrada, su tiempo de ejecución y complejidad computacional escala enormemente con la dimensión de la matriz para la cual queremos calcular su determinante. Por esa razón, en términos algebraicos, es mejor considerar ciertas propiedades que se desprenden directamente de la definición que hemos ido construyendo del determinante a fin de disponer de métodos más efectivos para su cálculo en dimensiones superiores (considerando, además, las transformaciones elementales sobre matrices que hemos aprendido previamente). Vamos, por tanto, a desarrollar tales propiedades:\n",
    "\n",
    "- **(P1) – Invariancia ante la transposición:** Si $\\mathbf{A} =\\left\\{ a_{{}ij}\\right\\}  \\in \\mathbb{R}^{n\\times n}$, entonces, de la definición (3.1), se tiene que $\\det(\\mathbf{A})=\\det(\\mathbf{A}^{\\top})$, ya que $\\det \\left( \\mathbf{A} \\right)  =\\sum^{n}_{k=1} \\triangle_{jk} \\det \\left( \\mathbf{A}_{jk} \\right)  =\\sum^{n}_{s=1} \\triangle_{sj} \\det \\left( \\mathbf{A}_{sj} \\right)$.\n",
    "- **(P2) – Columna o fila nula:** Si $\\mathbf{A} =\\left\\{ a_{{}ij}\\right\\}  \\in \\mathbb{R}^{n\\times n}$ posee una columna o fila nula (conformada únicamente por elementos iguales a cero), entonces $\\det(\\mathbf{A})=0$.\n",
    "- **(P3) – Determinante de un producto de matrices:** Si $\\mathbf{A},\\mathbf{B}\\in \\mathbb{R}^{n\\times n}$, entonces $\\det(\\mathbf{A}\\mathbf{B})=\\det(\\mathbf{A})\\det(\\mathbf{B})$.\n",
    "- **(P4) – Determinante de la matriz inversa:** Si $\\mathbf{A} =\\left\\{ a_{{}ij}\\right\\}  \\in \\mathbb{R}^{n\\times n}$ es una matriz no singular, entonces $\\det \\left( \\mathbf{A}^{-1} \\right)  =1/\\det \\left( \\mathbf{A} \\right)$.\n",
    "- **(P5) – Invariancia ante operaciones elementales:** Cualquier matriz $\\mathbf{A}\\in \\mathbb{R}^{n\\times n}$ mantiene el valor de su determinante, aunque hayamos operado sobre ella mediante cualquier de las transformaciones elementales vistas en la [clase 1.1](https://github.com/rquezadac/udd_data_science_lectures/blob/main/PARTE%20I%20-%20Fundamentos%20matem%C3%A1ticos%20elementales/clase_1_1.ipynb).\n",
    "- **(P6) – Escalamiento del determinante:** Sea $\\mathbf{A} =\\left\\{ a_{ij}\\right\\}  \\in \\mathbb{R}^{n\\times n}$ y $\\lambda \\in \\mathbb{R}$. Entonces $\\det \\left( \\lambda \\mathbf{A} \\right)  =\\lambda^{n} \\det \\left( \\mathbf{A} \\right)$.\n",
    "- **(P7) – Columna o fila repetida:** Si una matriz $\\mathbf{A} =\\left\\{ a_{ij}\\right\\}  \\in \\mathbb{R}^{n\\times n}$ tiene columnas o filas repetidas (o, más general, linealmente dependientes), entonces $\\det(\\mathbf{A})=0$. Esta propiedad generaliza **(P2)** y establece que toda matriz $\\mathbf{A}$ no singular tiene rango completo.\n",
    "\n",
    "**Ejemplo 3.2:** Calcularemos el determinante de la matriz $\\mathbf{A}\\in \\mathbb{R}^{5\\times 5}$, definida como\n",
    "\n",
    "$$\\mathbf{A} =\\left( \\begin{matrix}0&1&0&1&0\\\\ -1&a&0&0&0\\\\ 0&0&a&0&0\\\\ -1&0&0&a&0\\\\ 0&0&0&0&a\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.7)$</p>\n",
    "\n",
    "En efecto, aplicando la definición (3.1) y transformaciones elementales,\n",
    "\n",
    "$$\\begin{array}{rcl}\\det \\left( \\begin{matrix}0&1&0&1&0\\\\ -1&a&0&0&0\\\\ 0&0&a&0&0\\\\ -1&0&0&a&0\\\\ 0&0&0&0&a\\end{matrix} \\right)  &\\overbrace{=}^{\\mathrm{definicion} } &\\underbrace{a}_{\\mathrm{cofactor} \\  \\triangle_{55} } \\det \\left( \\begin{matrix}0&1&0&1\\\\ -1&a&0&0\\\\ 0&0&a&0\\\\ -1&0&0&a\\end{matrix} \\right)  \\\\ &\\overbrace{=}^{F_{42}\\left( -1\\right)  } &a\\det \\left( \\begin{matrix}0&1&0&1\\\\ -1&a&0&0\\\\ 0&0&a&0\\\\ 0&-a&0&a\\end{matrix} \\right)  \\\\ &\\overbrace{=}^{\\mathrm{definicion} } &\\underbrace{1}_{\\mathrm{cofactor} \\  \\triangle_{21} } \\cdot a\\det \\left( \\begin{matrix}1&0&1\\\\ 0&a&0\\\\ -a&0&a\\end{matrix} \\right)  \\\\ &\\overbrace{=}^{\\mathrm{definicion} } &a^{2}\\det \\left( \\begin{matrix}1&1\\\\ -a&a\\end{matrix} \\right)  \\overbrace{=}^{\\mathrm{definicion} } 2a^{3}\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.8)$</p>\n",
    "◼︎\n",
    "\n",
    "**Ejemplo 3.3 – Los determinantes en <font color='purple'>Numpy</font>:** En librerías de Python especializadas en el uso de arreglos vectorizados, como <font color='purple'>Numpy</font>, es razonable esperar que existan rutinas prefabricadas para el cálculo de determinantes. En particular, podemos usar la función `det()`, del módulo de álgebra lineal `numpy.linalg()` para calcular el determinante de cualquier matriz expresada por medio de un arreglo bidimensional. Por ejemplo, si consideramos la matriz $\\mathbf{A}\\in \\mathbb{R}^{5\\times 5}$, definida como\n",
    "\n",
    "$$\\mathbf{A} =\\left( \\begin{matrix}0&-1&2&-1&0\\\\ 3&-1&2&2&0\\\\ 6&-1&0&0&9\\\\ 0&1&4&-5&9\\\\ 2&2&-4&5&-3\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.9)$</p>\n",
    "\n",
    "Podemos calcular su determinante fácilmente en <font color='purple'>Numpy</font> definiendo, primeramente, un arreglo bidimensional, digamos `A`, donde almacenamos esta matriz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adbfde30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "618b347b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el arreglo en cuestión.\n",
    "A = np.array([\n",
    "    [0, -1, 2, -1, 0],\n",
    "    [3, -1, 2, 2, 0],\n",
    "    [6, -1, 0, 0, 9],\n",
    "    [0, 1, 4, -5, 9],\n",
    "    [2, 2, -4, 5, -3],\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb19826",
   "metadata": {},
   "source": [
    "Y luego aplicando la función `np.linalg.det()` para calcular su determinante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50c46cd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-162.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculamos el determinante de A (redondeado a 3 decimales).\n",
    "np.around(np.linalg.det(A), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de25b304",
   "metadata": {},
   "source": [
    "Vemos pues que no fue nada difícil calcular el determinante de una matriz de $5\\times 5$ en <font color='purple'>Numpy</font>. Sin embargo, tal y como comentamos previamente, el cálculo de determinantes corresponde a un esfuerzo computacional ostensiblemente grande y que escala enormemente a medida que aumentan las dimensiones de las matrices de interés. Incluso trabajando con una librería muy eficiente como <font color='purple'>Numpy</font>, los tiempos de ejecución pueden verse muy afectados. Para ejemplificar aquello, consideraremos el cálculo de los determinantes de cuatro matrices $\\mathbf{A}\\in \\mathbb{R}^{10\\times 10}$, $\\mathbf{B}\\in \\mathbb{R}^{100\\times 100}$, $\\mathbf{C}\\in \\mathbb{R}^{1000\\times 1000}$ y $\\mathbf{D}\\in \\mathbb{R}^{10000\\times 10000}$, las que representaremos mediante los arreglos bidimensionales `A`, `B`, `C` y `D`, y que estarán compuestas por números reales uniformemente distribuidos entre 0 y 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d251663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos una semilla aleatoria fija.\n",
    "rng = np.random.default_rng(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8eed46cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos algunas matrices de distintos tamaños.\n",
    "A = rng.random(size=(10, 10))\n",
    "B = rng.random(size=(100, 100))\n",
    "C = rng.random(size=(1000, 1000))\n",
    "D = rng.random(size=(10000, 10000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b1c26d",
   "metadata": {},
   "source": [
    "Vamos a estimar el tiempo de ejecución asociado al cálculo de los determinantes de estas matrices, a fin de observar qué tal escala con respecto al incremento en dimensionalidad de las mismas. Notemos que cada matriz es 10 veces más grande que su antecesora:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b8eddcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.34 µs ± 83.7 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n",
      "90 µs ± 740 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "8.37 ms ± 706 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "3.47 s ± 285 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.linalg.det(A)\n",
    "%timeit np.linalg.det(B)\n",
    "%timeit np.linalg.det(C)\n",
    "%timeit np.linalg.det(D)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9a7fbc63",
   "metadata": {},
   "source": [
    "Podemos observar que el cálculo del determinante de `B` tiene un tiempo de ejecución 12 veces mayor que el cálculo del determinante de `A`. El cálculo del determinante de `C` tiene un tiempo de ejecución de aproximadamente unas 82 veces superior al del cálculo del determinante de `B` (y, por extensión, 1038 veces más lento que el cálculo del determinante de `A`). Y el cálculo del determinante de `D` tiene un tiempo de ejecución aproximadamente unas 445 veces más lento que el cálculo del determinante de `C` (y, por extensión... ¡es más de 36000 veces más lento que el cálculo del determinante de `B`, y más de 460000 veces más lento que el cálculo del determinante de `A`!). Esto definitivamente nos hará pensarlo dos veces antes de calcular determinantes en el mundo real, donde resulta común vernos enfrentados a bases de datos con cientos de miles de registros. ◼︎ \n",
    "\n",
    "**Ejemplo 3.4:** Vamos a demostrar que\n",
    "\n",
    "$$\\det \\left( \\begin{matrix}1&1&1\\\\ x&y&z\\\\ x^{2}&y^{2}&z^{2}\\end{matrix} \\right)  =\\left( x-y\\right)  \\left( y-z\\right)  \\left( z-x\\right)$$\n",
    "<p style=\"text-align: right;\">$(3.10)$</p>\n",
    "\n",
    "En efecto, utilizando transformaciones elementales, propiedades de los determinantes y la definición (3.1), tenemos que\n",
    "\n",
    "$$\\begin{array}{rcl}\\det \\left( \\begin{matrix}1&1&1\\\\ x&y&z\\\\ x^{2}&y^{2}&z^{2}\\end{matrix} \\right)  &\\overbrace{=}^{F_{21}\\left( -x\\right)  } &\\det \\left( \\begin{matrix}1&1&1\\\\ 0&y-x&z-x\\\\ x^{2}&y^{2}&z^{2}\\end{matrix} \\right)  \\\\ &\\overbrace{=}^{F_{31}\\left( -x^{2}\\right)  } &\\det \\left( \\begin{matrix}1&1&1\\\\ 0&y-x&z-x\\\\ 0&y^{2}-x^{2}&z^{2}-x^{2}\\end{matrix} \\right)  \\\\ &\\overbrace{=}^{\\mathrm{definicion} } &\\det \\left( \\begin{matrix}y-x&z-x\\\\ y^{2}-x^{2}&z^{2}-x^{2}\\end{matrix} \\right)  \\\\ &=&\\left( y-z\\right)  \\left( z^{2}-x^{2}\\right)  -\\left( z-x\\right)  \\left( y^{2}-x^{2}\\right)  \\\\ &=&\\left( y-x\\right)  \\left( z-x\\right)  \\left( z+x-y+x\\right)  \\\\ &=&\\left( y-x\\right)  \\left( z-x\\right)  \\left( z-y\\right)  \\\\ &=&\\left( x-y\\right)  \\left( y-z\\right)  \\left( z-x\\right)  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.11)$</p>\n",
    "\n",
    "Tal como queríamos demostrar. ◼︎\n",
    "\n",
    "**Ejemplo 3.5:** Vamos a determinar todos los valores de $a\\in \\mathbb{R}$ tales que\n",
    "\n",
    "$$\\det \\left( \\begin{matrix}1&a&a^{2}&\\left( a^{3}+6\\right)  \\\\ 1&1&1&\\left( 1+6a\\right)  \\\\ 1&2&4&\\left( 8+3a\\right)  \\\\ 1&3&9&\\left( 27+2a\\right)  \\end{matrix} \\right)  =0$$\n",
    "<p style=\"text-align: right;\">$(3.12)$</p>\n",
    "\n",
    "En efecto,\n",
    "\n",
    "$$\\begin{array}{rcl}\\det \\left( \\begin{matrix}1&a&a^{2}&\\left( a^{3}+6\\right)  \\\\ 1&1&1&\\left( 1+6a\\right)  \\\\ 1&2&4&\\left( 8+3a\\right)  \\\\ 1&3&9&\\left( 27+2a\\right)  \\end{matrix} \\right)  &\\overbrace{=}^{\\begin{array}{c}F_{21}\\left( -1\\right)  \\\\ F_{31}\\left( -1\\right)  \\\\ F_{41}\\left( -1\\right)  \\end{array} } &\\det \\left( \\begin{matrix}1&a&a^{2}&\\left( a^{3}+6\\right)  \\\\ 0&\\left( 1-a\\right)  &\\left( 1-a^{2}\\right)  &\\left( 1-a^{3}+6\\left( a-1\\right)  \\right)  \\\\ 0&\\left( 2-a\\right)  &\\left( 4-a^{2}\\right)  &\\left( 8-a^{3}+3a-6\\right)  \\\\ 0&\\left( 3-a\\right)  &\\left( 9-a^{2}\\right)  &\\left( 27-a^{3}+2a-6\\right)  \\end{matrix} \\right)  \\\\ &\\overbrace{=}^{\\mathrm{definicion} } &\\det \\left( \\begin{matrix}\\left( 1-a\\right)  &\\left( 1-a^{2}\\right)  &\\left( 1-a^{3}+6\\left( a-1\\right)  \\right)  \\\\ \\left( 2-a\\right)  &\\left( 4-a^{2}\\right)  &\\left( 8-a^{3}+3a-6\\right)  \\\\ \\left( 3-a\\right)  &\\left( 9-a^{2}\\right)  &\\left( 27-a^{3}+2a-6\\right)  \\end{matrix} \\right)  \\\\ &=&\\det \\left( \\begin{matrix}\\left( 1-a\\right)  &\\left( 1-a^{2}\\right)  &\\left[ \\left( 1-a\\right)  \\left( a^{2}+a+1\\right)  +6\\left( a-1\\right)  \\right]  \\\\ \\left( 2-a\\right)  &\\left( 4-a^{2}\\right)  &\\left[ \\left( 2-a\\right)  \\left( a^{2}+2a+4\\right)  +3\\left( a-2\\right)  \\right]  \\\\ \\left( 3-a\\right)  &\\left( 9-a^{2}\\right)  &\\left[ \\left( 3-a\\right)  \\left( a^{2}+3a+9\\right)  +2\\left( a-3\\right)  \\right]  \\end{matrix} \\right)  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.13)$</p>\n",
    "\n",
    "Es claro, conforme el desarrollo anterior, que el determinante se anula cuando $a=1$, $a=2$ o $a=3$. Para $a\\neq 1$, $a\\neq 2$ y $a\\neq 3$, proseguimos con el desarrollo del determinante, con lo cual,\n",
    "\n",
    "$$\\begin{array}{rcl}\\det \\left( \\begin{matrix}1&a&a^{2}&\\left( a^{3}+6\\right)  \\\\ 1&1&1&\\left( 1+6a\\right)  \\\\ 1&2&4&\\left( 8+3a\\right)  \\\\ 1&3&9&\\left( 27+2a\\right)  \\end{matrix} \\right)  &\\overbrace{=}^{\\mathrm{propiedades} } &\\left( 1-a\\right)  \\left( 2-a\\right)  \\left( 3-a\\right)  \\det \\left( \\begin{matrix}1&\\left( 1+a\\right)  &\\left( a^{2}+a-5\\right)  \\\\ 1&\\left( 2+a\\right)  &\\left( a^{2}+2a+1\\right)  \\\\ 1&\\left( 3+a\\right)  &\\left( a^{2}+3a+7\\right)  \\end{matrix} \\right)  \\\\ &\\overbrace{=}^{\\begin{array}{c}F_{21}\\left( -1\\right)  \\\\ F_{31}\\left( -1\\right)  \\end{array} } &\\left( 1-a\\right)  \\left( 2-a\\right)  \\left( 3-a\\right)  \\det \\left( \\begin{matrix}1&\\left( 1+a\\right)  &\\left( a^{2}+a-5\\right)  \\\\ 0&1&\\left( a+6\\right)  \\\\ 0&2&\\left( a+6\\right)  \\end{matrix} \\right)  \\\\ &=&2\\left( 1-a\\right)  \\left( 2-a\\right)  \\left( 3-a\\right)  \\det \\left( \\begin{matrix}1&\\left( 1+a\\right)  &\\left( a^{2}+a-5\\right)  \\\\ 0&1&\\left( a+6\\right)  \\\\ 0&1&\\left( a+6\\right)  \\end{matrix} \\right)  \\\\ &=&0\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.14)$</p>\n",
    "\n",
    "Luego tenemos que $\\det(\\mathbf{A})=0$ para todo $a\\in \\mathbb{R}$. ◼︎"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "39d47379",
   "metadata": {},
   "source": [
    "## Diagonalización de matrices.\n",
    "Una vez estudiado el concepto de determinante, vamos a ocuparnos de un problema más general y que consiste en saber cuando, para una transformación lineal del tipo $T:\\mathbb{R}^{n}\\longrightarrow \\mathbb{R}^{n}$, es posible encontrar una base $\\alpha$ con respecto a la cual la matriz asociada $\\mathbf{A}=[T]_{\\alpha}^{\\alpha}$ sea de tipo **diagonal**. De manera equivalente, queremos determinar las condiciones para las cuales una matriz $\\mathbf{A}\\in \\mathbb{R}^{n\\times n}$ puede *descomponerse* de la forma\n",
    "\n",
    "$$\\mathbf{A} =\\mathbf{P} \\mathbf{D} \\mathbf{P}^{-1}$$\n",
    "<p style=\"text-align: right;\">$(3.15)$</p>\n",
    "\n",
    "Donde $\\mathbf{D}\\in \\mathbb{R}^{n\\times n}$ es una matriz diagonal.\n",
    "\n",
    "Este problema de naturaleza puramente algebraica tiene una cantidad significativa de aplicaciones en otras ramas de las matemáticas, como en ecuaciones diferenciales, estadística y, por supuesto, en machine learning. Puntualmente, la diagonalización es un procedimiento esencial en la derivación de la descomposición de matrices en **valores singulares** y que, a su vez, constituye la base del **análisis de componentes principales**, uno de los modelos de aprendizaje no supervisado más utilizados para la reducción de la dimensión de conjuntos de datos con un elevado número de variables, sin perder una cantidad significativa de información. Tal vez el teorema más importante de esta subsección es el que dice que toda matriz simétrica puede representarse mediante la expresión (3.15).\n",
    "\n",
    "Para comenzar con el estudio de la diagonalización de matrices, primero introduciremos algunos conceptos y resultados esenciales.\n",
    "\n",
    "**<font color='blue'>Definición 3.2 – Autovalores y autovectores:</font>** Sea $V$ un $\\mathbb{K}$-espacio vectorial y $T:V\\longrightarrow V$ una transformación lineal. Diremos que $v\\in V$ es un **autovector o vector propio** de $T$ si se cumplen las siguientes condiciones:\n",
    "\n",
    "- **(C1):** $v\\in O_{V}$.\n",
    "- **(C2):** Existe un escalar $\\lambda \\in \\mathbb{K}$ tal que $T(v)=\\lambda v$.\n",
    "\n",
    "El escalar $\\lambda$ se denomina **autovalor o valor propio** asociado al autovector $v$.\n",
    "\n",
    "Equivalentemente, diremos que $v\\in V-\\left\\{ O_{V}\\right\\}$ es un autovector asociado a la matriz $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$ si $v$ es un autovector de la transformación lineal $T:V\\longrightarrow V$ explícitamente definida como $T(v)=\\mathbf{A}v$. Es decir, existe un escalar $\\lambda \\in \\mathbb{K}$ tal que $\\mathbf{A}v=\\lambda v$. De la misma forma, diremos que $\\lambda$ es un autovalor de la matriz $\\mathbf{A}$.\n",
    "\n",
    "**<font color='crimson'>Teorema 3.2:</font>** *Dada una matriz $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$, si $\\lambda \\in \\mathbb{K}$ es un autovalor de $\\mathbf{A}$, entonces las siguientes expresiones son equivalentes:*\n",
    "\n",
    "- **(T1):** $\\exists v\\neq O_{V}\\  |\\  \\mathbf{A} v=\\lambda v$, *donde $V$ es un $\\mathbb{K}$-espacio vectorial y $v$ es un autovector de la matriz $\\mathbf{A}$.*\n",
    "- **(T2):** $\\exists v\\in V$ *tal que $v$ es una solución no trivial del sistema de ecuaciones $\\left( \\mathbf{A} -\\lambda \\mathbf{I}_{n} \\right)  v=O_{V}$.*\n",
    "- **(T3):** $\\ker \\left( \\mathbf{A} -\\lambda \\mathbf{I}_{n} \\right)  \\neq \\left\\{ O_{V}\\right\\}$.\n",
    "- **(T4):** $\\left( \\mathbf{A} -\\lambda \\mathbf{I}_{n} \\right)$ *es una matriz no invertible*.\n",
    "◆\n",
    "\n",
    "Queda claro pues que necesitamos una forma sencilla de determinar qué valores de $\\lambda \\in \\mathbb{K}$ son, en efecto, autovalores. Para ello, es útil reconocer que la condición **(T4)** en el teorema (3.2) puede expresarse como una ecuación en la variable $\\lambda$. Por supuesto, es acá donde cobra sentido el desarrollo que hicimos del concepto de determinante de una matriz $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$, puesto que, como ya verificamos con el teorema (3.1), toda matriz es no singular (invertible) si su determinante es no nulo. Por lo tanto, la condición **(T4)** puede expresarse como $\\det \\left( \\mathbf{A} -\\lambda \\mathbf{I}_{n} \\right)  =0$, donde $\\mathbf{I}_{n}$ es la matriz identidad.\n",
    "\n",
    "Tiene sentido, por lo tanto, la siguiente definición.\n",
    "\n",
    "**<font color='blue'>Definición 3.3 – Polinomio característico:</font>** Sea $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$ una matriz tal que ésta coincide con la representación matricial de una transformación lineal $T:V\\longrightarrow V$ que opera sobre el $\\mathbb{K}$-espacio vectorial $V$. La expresión $P_{T}\\left( \\lambda \\right)  =\\det \\left( \\mathbf{A} -\\lambda \\mathbf{I}_{n} \\right)  \\in \\mathbb{K}_{n} \\left[ \\lambda \\right]$ será llamada **polinomio característico** de la matriz $\\mathbf{A}$ (y, por extensión, de la transformación lineal $T$).\n",
    "\n",
    "De la definición (3.2), se tiene que $v\\in V$ es un autovector de $\\mathbf{A}$ si $v\\neq O_{V}$ y existe un escalar $\\lambda \\in \\mathbb{K}$ tal que $\\mathbf{A}v=\\lambda v$. De esta manera, podemos verificar que $\\lambda$ es un autovalor de la matriz $\\mathbf{A}$ si y sólo si es una solución no nula de la ecuación\n",
    "\n",
    "$$\\left( \\mathbf{A} -\\lambda \\mathbf{I}_{n} \\right)  v=O_{V}$$\n",
    "<p style=\"text-align: right;\">$(3.16)$</p>\n",
    "\n",
    "Así pues, todo escalar $\\lambda \\in \\mathbb{K}$ que satisfaga (3.16) será un autovalor de $\\mathbf{A}$. Notemos además que, conforme la expresión anterior, si $v$ es un autovector, también lo es cualquier otro vector que sea linealmente dependiente con respecto a $v$. Es decir, si 𝑣 es un autovector de la matriz $\\mathbf{A}$, entonces también lo es $\\alpha v;\\forall v\\in \\mathbb{K}$. Más aún, si $v$ es un autovector, cualquier autovalor asociado a $v$ es único, puesto que si $T(v)=\\lambda_{1}v=\\lambda_{2}v$, entonces $(\\lambda_{1}-\\lambda_{2})v=O_{V}$. Como $v\\neq O_{V}$, entonces $\\lambda_{1}-\\lambda_{2}=0$, lo que implica que $\\lambda_{1}=\\lambda_{2}$.\n",
    "\n",
    "**<font color='blue'>Definición 3.4 – Autoespacio:</font>** Sea $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$ una matriz tal que ésta coincide con la representación matricial de una transformación lineal $T:V\\longrightarrow V$, siendo $V$ un $\\mathbb{K}$-espacio vectorial. Para cada autovalor $\\lambda$ de $\\mathbf{A}$ definimos el **autoespacio o espacio propio** de $\\lambda$, denotado como $W_{\\lambda}$, como\n",
    "\n",
    "$$W_{\\lambda }=\\ker \\left( \\mathbf{A} -\\lambda \\mathbf{I}_{n} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.17)$</p>\n",
    "\n",
    "**<font color='blue'>Definición 3.5 – Similitud entre matrices:</font>** Sean $\\mathbf{A} ,\\mathbf{B} \\in \\mathbb{K}^{n\\times n}$ dos matrices no singulares. Diremos que $\\mathbf{A}$ y $\\mathbf{B}$ son **similares** si existe otra matriz $\\mathbf{P}\\in \\mathbb{K}^{n\\times 1}$ tal que\n",
    "\n",
    "$$\\mathbf{A} =\\mathbf{P} \\mathbf{B} \\mathbf{P}^{-1}$$\n",
    "<p style=\"text-align: right;\">$(3.18)$</p>\n",
    "\n",
    "**<font color='crimson'>Teorema 3.3 – Preservación de autovalores en matrices similares:</font>** *Sean $\\mathbf{A} ,\\mathbf{B} \\in \\mathbb{K}^{n\\times n}$ dos matrices similares entre sí. Entonces ambas matrices tienen los mismos autovalores (y, por extensión, el mismo polinomio característico).*\n",
    "\n",
    "Vamos a demostrar el teorema (3.3) a fin de entender completamente este resultado. En efecto, sean $\\mathbf{A} ,\\mathbf{B} \\in \\mathbb{K}^{n\\times n}$, tal que ambas matrices son similares (es decir, $\\mathbf{A} =\\mathbf{P} \\mathbf{B} \\mathbf{P}^{-1}$). Construyendo la expresión $\\mathbf{A} -\\lambda \\mathbf{I}_{n}$, tenemos que\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{A} -\\lambda \\mathbf{I}_{n} &=&\\mathbf{P} \\mathbf{B} \\mathbf{P}^{-1} -\\lambda \\mathbf{P} \\mathbf{P}^{-1} =\\mathbf{P} \\left( \\mathbf{B} -\\lambda \\mathbf{I}_{n} \\right)  \\mathbf{P}^{-1} \\\\ &\\Longrightarrow &\\det \\left( \\mathbf{A} -\\lambda \\mathbf{I}_{n} \\right)  =\\det \\left( \\mathbf{P} \\left( \\mathbf{B} -\\lambda \\mathbf{I}_{n} \\right)  \\mathbf{P}^{-1} \\right)  \\\\ &\\Longrightarrow &\\det \\left( \\mathbf{P} \\right)  \\det \\left( \\mathbf{B} -\\lambda \\mathbf{I}_{n} \\right)  \\det \\left( \\mathbf{P}^{-1} \\right)  \\  \\left( \\mathrm{pero} \\  \\det \\left( \\mathbf{P}^{-1} \\right)  =\\frac{1}{\\det \\left( \\mathbf{P} \\right)  } \\right)  \\\\ &\\Longrightarrow &\\det \\left( \\mathbf{B} -\\lambda \\mathbf{I}_{n} \\right)  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.19)$</p>\n",
    "\n",
    "Así, efectivamente, $\\mathbf{A}$ y $\\mathbf{B}$ tienen el mismo polinomio característico y, por tanto, los mismos autovalores.\n",
    "\n",
    "**Ejemplo 3.5:** Sea $T:\\mathbb{R}^{3}\\longrightarrow \\mathbb{R}^{3}$ una transformación lineal definida como $T(x,y,z)=(3x+2y+z,3y+2z,-z)$. Vamos a determinar los autovalores y autovectores asociados a $T$. \n",
    "\n",
    "En primer lugar, debemos construir la matriz $\\mathbf{A}$ asociada a $T$ considerando la base canónica de vectores en $\\mathbb{R}^{3}$ (que, recordemos, es $\\mathbf{e}(3)=\\left\\{ \\mathbf{e}_{1} ,\\mathbf{e}_{2} ,\\mathbf{e}_{3} \\right\\}  =\\left\\{ \\left( 1,0,0\\right)  ,\\left( 0,1,0\\right)  ,\\left( 0,0,1\\right)  \\right\\}$). Esto resulta sencillo, ya que\n",
    "\n",
    "$$T\\left( x,y,z\\right)  =\\left( 3x+2y+z,3y+2z,-z\\right)  =x\\begin{pmatrix}3\\\\ 0\\\\ 0\\end{pmatrix} +y\\begin{pmatrix}2\\\\ 3\\\\ 0\\end{pmatrix} +z\\begin{pmatrix}1\\\\ 2\\\\ -1\\end{pmatrix} \\Longrightarrow \\mathbf{A}= \\begin{pmatrix}3&2&1\\\\ 0&3&2\\\\ 0&0&-1\\end{pmatrix}$$\n",
    "<p style=\"text-align: right;\">$(3.20)$</p>\n",
    "\n",
    "Ahora debemos resolver la ecuación $P_{\\mathbf{A}}(\\lambda)=0$. En efecto,\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{A} =\\begin{pmatrix}3&2&1\\\\ 0&3&2\\\\ 0&0&-1\\end{pmatrix} &\\Longrightarrow &P_{\\mathbf{A} }\\left( \\lambda \\right)  =\\det \\left( \\begin{matrix}3-\\lambda &2&1\\\\ 0&3-\\lambda &2\\\\ 0&0&-1-\\lambda \\end{matrix} \\right)  =0\\\\ &\\Longrightarrow &P_{\\mathbf{A} }\\left( \\lambda \\right)  =\\left( \\lambda -3\\right)^{3}  \\left( \\lambda +1\\right)  =0\\Longleftrightarrow \\lambda_{1} =3\\wedge \\lambda_{2} =-1\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.21)$</p>\n",
    "\n",
    "Por lo tanto, los autovalores de la matriz $\\mathbf{A}$ (y, por extensión, de $T$) son $\\lambda_{1}=3$ y $\\lambda_{2}=-1$. Ahora determinamos los autoespacios respectivos,\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{u} \\in \\left( \\mathbb{R}^{3} \\right)_{\\lambda }  &\\Longleftrightarrow &\\mathbf{u} \\in \\mathbb{R}^{3} \\wedge T\\left( \\mathbf{u} \\right)  =\\lambda \\mathbf{u} \\\\ &\\Longleftrightarrow &\\mathbf{u} =\\left( x,y,z\\right)  \\wedge T\\left( x,y,z\\right)  =\\lambda \\left( x,y,z\\right)  \\\\ &\\Longleftrightarrow &\\mathbf{u} =\\left( x,y,z\\right)  \\wedge T\\left( x,y,z\\right)  =\\left( \\lambda x,\\lambda y,\\lambda z\\right)  \\\\ &\\Longleftrightarrow &\\mathbf{u} =\\left( x,y,z\\right)  \\wedge \\left( 3x+2y+z,3y+2z,-z\\right)  =\\left( \\lambda x,\\lambda y,\\lambda z\\right)  \\\\ &\\Longleftrightarrow &\\mathbf{u} =\\left( x,y,z\\right)  \\wedge \\begin{cases}\\begin{array}{rcl}3x+2y+z&=&\\lambda x\\\\ 3y+2z&=&\\lambda y\\\\ -z&=&\\lambda z\\end{array} &\\end{cases} \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.22)$</p>\n",
    "\n",
    "Debemos por tanto evaluar el sistema de ecuaciones determinado en (3.22) usando los autovalores determinados previamente. Así tenemos, para $\\lambda_{1}=3$,\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{u} \\in \\left( \\mathbb{R}^{3} \\right)_{\\lambda =3} &\\Longleftrightarrow &\\mathbf{u} =\\left( x,y,z\\right)  \\wedge \\begin{cases}\\begin{array}{rcl}3x+2y+z&=&\\lambda x\\\\ 3y+2z&=&\\lambda y\\\\ -z&=&\\lambda z\\end{array} &\\end{cases} \\\\ &\\Longleftrightarrow &\\mathbf{u} =\\left( x,y,z\\right)  \\wedge z=0,y=0\\\\ &\\Longleftrightarrow &\\mathbf{u} =\\left( x,0,0\\right)  ;x\\in \\mathbb{R} \\\\ &\\Longleftrightarrow &\\mathbf{u} =x\\left( 1,0,0\\right)  \\\\ &\\Longleftrightarrow &\\left( \\mathbb{R}^{3} \\right)_{\\lambda =3}  =\\left< \\left\\{ \\left( 1,0,0\\right)  \\right\\}  \\right>  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.23)$</p>\n",
    "\n",
    "Por otro lado, para $\\lambda_{2}=-1$, tenemos,\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{u} \\in \\left( \\mathbb{R}^{3} \\right)_{\\lambda =-1}  &\\Longleftrightarrow &\\mathbf{u} =\\left( x,y,z\\right)  \\wedge \\begin{cases}\\begin{array}{rcl}3x+2y+z&=&-x\\\\ 3y+2z&=&-y\\\\ -z&=&-z\\end{array} &\\end{cases} \\\\ &\\Longleftrightarrow &\\mathbf{u} =\\left( x,y,z\\right)  \\wedge z=2y,x=0\\\\ &\\Longleftrightarrow &\\mathbf{u} =\\left( 0,y,-2y\\right)  ;y\\in \\mathbb{R} \\\\ &\\Longleftrightarrow &\\mathbf{u} =y\\left( 0,1,-2\\right)  \\\\ &\\Longleftrightarrow &\\left( \\mathbb{R}^{3} \\right)_{\\lambda =-1}  =\\left< \\left\\{ \\left( 0,1,-2\\right)  \\right\\}  \\right>  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.24)$</p>\n",
    "◼︎\n",
    "\n",
    "Sea $T:\\mathbb{K}^{n}\\longrightarrow \\mathbb{K}^{n}$ una transformación lineal y sea $\\mathbf{A}=[T]_{\\alpha}^{\\alpha}$ la matriz asociada a $T$ en la base canónica $\\alpha$. Supongamos que existe una base de autovectores de $\\mathbf{A}$, definida como $\\beta =\\left\\{ \\mathbf{v}_{1},...,\\mathbf{v}_{n}\\right\\}$, siendo $\\beta$ por tanto una base de $\\mathbb{K}^{n}$. Para todo $i\\in \\mathbb{N}$, definimos $\\lambda_{i}\\in \\mathbb{K}$ tal que $\\mathbf{A}\\lambda \\mathbf{v}_{i}=\\lambda_{i} \\mathbf{v}_{i}$. Vemos que la matriz asociada a $T$ en la base $\\beta$, que denominamos como $\\mathbf{D}=[T]_{\\beta}^{\\beta}$, es diagonal, ya que\n",
    "\n",
    "$$\\begin{array}{rcl}\\mathbf{A} \\mathbf{v}_{1} =\\lambda_{1} \\mathbf{v}_{1} &\\Longrightarrow &\\lambda_{1} \\mathbf{v}_{1} =\\lambda_{1} \\mathbf{v}_{1} +0\\mathbf{v}_{2} +\\cdots +0\\mathbf{v}_{n} \\\\ \\mathbf{A} \\mathbf{v}_{2} =\\lambda_{2} \\mathbf{v}_{2} &\\Longrightarrow &\\lambda_{2} \\mathbf{v}_{2} =0\\mathbf{v}_{1} +\\lambda_{2} \\mathbf{v}_{2} +\\cdots +0\\mathbf{v}_{n} \\\\ &\\vdots &\\\\ \\mathbf{A} \\mathbf{v}_{n} =\\lambda_{n} \\mathbf{v}_{n} &\\Longrightarrow &\\lambda_{n} \\mathbf{v}_{n} =0\\mathbf{v}_{1} +0\\mathbf{v}_{2} +\\cdots +\\lambda_{n} \\mathbf{v}_{n} \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.25)$</p>\n",
    "\n",
    "Por lo tanto, podemos escribir\n",
    "\n",
    "$$\\left[ T\\right]^{\\beta }_{\\beta }  =\\left( \\left[ T\\left( \\mathbf{v}_{1} \\right)  \\right]_{\\beta }  ,\\left[ T\\left( \\mathbf{v}_{2} \\right)  \\right]_{\\beta }  ,...,\\left[ T\\left( \\mathbf{v}_{n} \\right)  \\right]_{\\beta }  \\right)  =\\left( \\begin{matrix}\\lambda_{1} &0&\\cdots &0\\\\ 0&\\lambda_{2} &\\cdots &0\\\\ \\vdots &\\vdots &\\ddots &\\vdots \\\\ 0&0&\\cdots &\\lambda_{n} \\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.26)$</p>\n",
    "\n",
    "Así que, efectivamente, la matriz $\\mathbf{D}=[ T]^{\\beta }_{\\beta }$ y, por ende, la matriz $\\mathbf{A}$ puede expresarse mediante la **descomposición propia** (o auto-descomposición) definida como $\\mathbf{A}=\\mathbf{P}\\mathbf{D}\\mathbf{P}^{-1}$. Esto resulta conveniente, ya que algunas ventajas de conocer la matriz diagonal $\\mathbf{D}$ son las siguientes:\n",
    "\n",
    "- **(V1):** $\\rho(\\mathbf{A})=\\rho(\\mathbf{D})=$ número de autovalores no nulos de $\\mathbf{A}$ (y, por extensión, de $\\mathbf{D}$). Recordemos que $\\rho(\\mathbf{A})$ denota el rango de la matriz $\\mathbf{A}$.\n",
    "- **(V2):** $\\det(\\mathbf{A})=\\det(\\mathbf{P}\\mathbf{D}\\mathbf{P}^{-1})=\\det(\\mathbf{P})\\det(\\mathbf{D})\\det(\\mathbf{P}^{-1})=\\det(\\mathbf{D})=\\prod^{n}_{k=1} \\lambda_{k}$.\n",
    "- **(V3):** Si $\\mathbf{A}$ es una matriz no singular (de decir, si $\\det(\\mathbf{A})\\neq 0$), entonces, para cada $\\lambda_{k}\\neq 0$ y $\\mathbf{A}^{-1}=\\mathbf{P}\\mathbf{D^{-1}}\\mathbf{P^{-1}}$, donde\n",
    "\n",
    "$$\\mathbf{D}^{-1} =\\left( \\begin{matrix}\\lambda^{-1}_{1} &0&\\cdots &0\\\\ 0&\\  \\lambda^{-1}_{2} &\\cdots &0\\\\ \\vdots &\\vdots &\\ddots &\\vdots \\\\ 0&0&\\cdots &\\lambda^{-1}_{n} \\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.27)$</p>\n",
    "\n",
    "- **(V4):** $\\mathbf{A}^{m} =\\left( \\mathbf{P} \\mathbf{D} \\mathbf{P}^{-1} \\right)^{m}  =\\left( \\mathbf{P} \\mathbf{D} \\mathbf{P}^{-1} \\right)  \\left( \\mathbf{P} \\mathbf{D} \\mathbf{P}^{-1} \\right)  \\overbrace{\\cdots }^{m\\  \\mathrm{veces} } \\left( \\mathbf{P} \\mathbf{D} \\mathbf{P}^{-1} \\right)$. Así que,\n",
    "\n",
    "$$\\mathbf{A}^{m} =\\mathbf{P} \\left( \\begin{matrix}\\lambda^{-m}_{1} &0&\\cdots &0\\\\ 0&\\lambda^{-m}_{2} &\\cdots &0\\\\ \\vdots &\\vdots &\\ddots &\\vdots \\\\ 0&0&\\cdots &\\lambda^{-m}_{n} \\end{matrix} \\right)  \\mathbf{P}^{-1}$$\n",
    "<p style=\"text-align: right;\">$(3.28)$</p>\n",
    "\n",
    "Como hemos visto, $\\mathbf{A} =\\mathbf{P} \\mathbf{D} \\mathbf{P}^{-1}$, donde $\\mathbf{P}=[\\mathbf{e}]_{\\beta}^{\\alpha}$ (donde $\\mathbf{e}(n)$ es la base canónica de vectores de $\\mathbb{K}^{n}$). Es decir, tenemos que expresar los correspondientes autovectores en términos de la base canónica de $\\mathbb{K}^{n}$. De esta manera, $\\mathbf{P}=(\\mathbf{v}_{1},...,\\mathbf{v}_{n})$. Tiene sentido entonces la siguiente definición.\n",
    "\n",
    "**<font color='blue'>Definición 3.6 – Matriz diagonalizable:</font>** Sea $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$ una matriz no singular. Diremos que $\\mathbf{A}$ es **diagonalizable** si $\\mathbb{K}^{n}$ admite una base de autovectores de $\\mathbf{A}$ (es decir, los autovectores de $\\mathbf{A}$ conforman un sistema de generadores para $\\mathbb{K}^{n}$ y son linealmente independientes).\n",
    "\n",
    "Con esta definición, ya podemos enunciar dos importantes teoremas relativos al proceso de diagonalización.\n",
    "\n",
    "**<font color='crimson'>Teorema 3.4:</font>** *Sea $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$ una matriz no singular. Entonces $\\mathbf{A}$ es diagonalizable si y sólo si $\\mathbf{A}$ es similar a una matriz diagonal.* ◆\n",
    "\n",
    "**<font color='crimson'>Teorema 3.5:</font>** *Sea $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$ una matriz no singular y sea $\\left\\{ \\lambda_{i} \\right\\}^{k}_{i=1}$ un conjunto de autovalores de $\\mathbf{A}$ (todos distintos). Si $\\left\\{ v_{i}\\right\\}^{k}_{i=1}$ representa el conjunto de autovectores de $\\mathbf{A}$ para el cual se tiene que $\\mathbf{A} v_{i}=\\lambda_{i} v_{i}$, entonces $\\left\\{ v_{i}\\right\\}^{k}_{i=1}$ es un conjunto linealmente independiente.* ◆\n",
    "\n",
    "Antes de proesguir, veremos la extensión natural del concepto de suma directa para más de dos subespacios vectoriales. Definamos primero los $k$ subespacios $U_{1},...,U_{k}$ de $V$. Definiremos la **suma** de tales subespacios como\n",
    "\n",
    "$$\\sum^{k}_{i=1} U_{i}=U_{1}+\\cdots +U_{k}\\triangleq \\left\\{ v=\\sum^{k}_{i=1} u_{i}\\  |\\  \\forall i\\in \\left\\{ 1,...,k\\right\\}  ,u_{i}\\in U_{i}\\right\\}$$\n",
    "<p style=\"text-align: right;\">$(3.29)$</p>\n",
    "\n",
    "La suma de subespacios así definida también es, como cabría esperar, un subespacio de $V$. Estamos en condiciones, por tanto, de establecer la siguiente definición.\n",
    "\n",
    "**<font color='blue'>Definición 3.7 – Suma directa múltiple:</font>** Sea $V$ un $\\mathbb{K}$-espacio vectorial y $\\left\\{ U_{i}\\right\\}^{k^{}}_{i=1}$ una colección de $k$ subespacios vectoriales de $V$. Diremos que el subespacio $Z=\\sum^{k}_{i=1} U_{i}$ es la **suma directa** de $\\left\\{ U_{i}\\right\\}^{k^{}}_{i=1}$, lo que denotamos como $Z=\\bigoplus^{k}_{i=1} U_{i}=U_{1}\\oplus \\cdots \\oplus U_{k}$ si, para todo $v\\in Z$, $v$ se escribe de manera única como $v=\\sum^{k}_{i=1} u_{i}$, donde $u_{i}\\in U_{i}$, para $i=1,...,k$. Es decir,\n",
    "\n",
    "$$Z=\\bigoplus^{k}_{i=1} U_{i}=U_{1}\\oplus \\cdots \\oplus U_{k}\\Longleftrightarrow v=\\sum^{k}_{i=1} u_{i}\\  ;\\  u_{i}\\in U_{i},\\forall i\\in \\left\\{ 1,...,k\\right\\}$$\n",
    "<p style=\"text-align: right;\">$(3.30)$</p>\n",
    "\n",
    "La suma directa múltiple de subespacios cumple con las siguientes propiedades:\n",
    "\n",
    "- **(P1):** $Z=\\bigoplus^{k}_{i=1} U_{i}\\Longleftrightarrow \\left( Z=\\sum^{k}_{i=1} U_{i}\\wedge \\forall j\\in \\left\\{ 1,...,k\\right\\}  ,U_{j}\\cap \\left( \\sum^{k}_{\\begin{matrix}i=1\\\\ i\\neq j\\end{matrix} } U_{i}\\right)  =\\left\\{ O_{V}\\right\\}  \\right)$.\n",
    "- **(P2):** Si $Z=\\sum^{k}_{i=1} U_{i}$ y $Z$ es de dimensión finita (lo que suele denotarse como $\\dim(Z)<\\infty$), entonces las siguientes proposiciones son equivalentes:\n",
    "    - $Z=\\bigoplus^{k}_{i=1} U_{i}$.\n",
    "    - $\\left( \\forall i=\\left\\{ 1,...,k\\right\\}  \\right)  \\left( \\forall u_{i}\\in U_{i}-\\left\\{ O_{V}\\right\\}  \\right)  ,\\left\\{ u_{1},...,u_{k}\\right\\}$ es linealmente independiente.\n",
    "    - La yuxtaposición de las bases de las bases de los subespacios $\\left\\{ U_{i}\\right\\}^{k}_{i=1}$ es una base (y no sólo un sistema de generadores) para $Z$.\n",
    "    - $\\dim(Z)=\\sum^{k}_{i=1} \\dim \\left( U_{i}\\right)$.\n",
    "\n",
    "Las propiedades que derivan de la definición (3.7) nos permiten formular el siguiente teorema.\n",
    "\n",
    "**<font color='crimson'>Teorema 3.6:</font>** *Sea $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$ una matriz no singular tal que $\\left\\{ \\lambda_{i} \\right\\}^{k}_{i=1}$ es el conjunto de autovalores de $\\mathbf{A}$ (todos distintos) y $W_{\\lambda_{i}}=\\ker(\\mathbf{A}-\\lambda \\mathbf{I}_{n})$ es el autoespacio asociado al autovalor $\\lambda_{i}$. Si $W=\\sum^{k}_{i=1} W_{\\lambda_{i} }$, entonces tenemos que*\n",
    "\n",
    "$$W=\\bigoplus^{k}_{i=1} W_{\\lambda_{i} }$$\n",
    "<p style=\"text-align: right;\">$(3.31)$</p>\n",
    "\n",
    "*En particular, $\\mathbf{A}$ es diagonalizable si y sólo si*\n",
    "\n",
    "$$\\mathbb{K}^{n} =\\bigoplus^{k}_{i=1} W_{\\lambda_{i} }$$\n",
    "<p style=\"text-align: right;\">$(3.32)$</p>\n",
    "◆\n",
    "\n",
    "**<font color='DodgerBlue'>Corolario 3.1:</font>** *Sea $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$ una matriz no singular tal que $\\left\\{ \\lambda_{i} \\right\\}^{k}_{i=1}$ es el conjunto de autovalores de $\\mathbf{A}$ (todos distintos) y $W_{\\lambda_{i}}=\\ker(\\mathbf{A}-\\lambda \\mathbf{I}_{n})$ es el autoespacio asociado al autovalor $\\lambda_{i}$. Si $W=\\sum^{k}_{i=1} W_{\\lambda_{i} }$, entonces tenemos que*\n",
    "\n",
    "- **(T1):** $W_{\\lambda_{i} }=\\ker \\left( \\mathbf{A} -\\lambda \\mathbf{I}_{n} \\right)$ *es de dimensión 1.*\n",
    "- **(T2):** *Sea $v_{i}\\in W_{\\lambda_{i}}$ con $v_{i}\\neq O_{V}$. Entonces $\\left\\{ v_{1},...,v_{n}\\right\\}$ es una base de autovectores.*\n",
    "◆\n",
    "\n",
    "Este resultado nos entrega una condición suficiente (pero no necesaria) para establecer que una matriz $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$ sea diagonalizable: En síntesis, se tiene que $\\mathbf{A}$ es diagonalizable si $\\mathbf{A}$ tiene $n$ autovalores distintos. Sin embargo, esto no excluyente.\n",
    "\n",
    "**<font color='blue'>Definición 3.8 – Multiplicidad geométrica:</font>** Sea $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$ una matriz no singular y $\\lambda$ un autovalor de $\\mathbf{A}$. Definimos la **multiplicidad geométrica** de $\\lambda$, denotada como $\\gamma_{\\mathbf{A}}(\\lambda)$, como la dimensión del autoespacio $W_{\\lambda }=\\ker \\left( \\mathbf{A} -\\lambda \\mathbf{I}_{n} \\right)$. Es decir, $\\gamma_{\\mathbf{A} } \\left( \\lambda \\right)  =\\dim \\left( W_{\\lambda }\\right)$.\n",
    "\n",
    "**<font color='blue'>Definición 3.9 – Multiplicidad algebraica:</font>** Sea $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$ una matriz no singular y $\\lambda$ un autovalor de $\\mathbf{A}$. Definimos la **multiplicidad algebraica** del autovalor $\\lambda$, denotada como $\\alpha_{\\mathbf{A}}(\\lambda)$, como la máxima potencia de $(x-\\lambda)$ que es divisor del polinomio característico de $\\mathbf{A}$.\n",
    "\n",
    "Con ambas definiciones de multiplicidad ya establecidas, y con los resultados obtenidos previamente, estamos en condiciones de establecer el siguiente teorema, que establece las condiciones necesarias y suficientes para garantizar que una matriz es diagonalizable.\n",
    "\n",
    "**<font color='crimson'>Teorema 3.7 – Criterio de diagonalización de matrices:</font>** *Sea $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$ una matriz no singular y $P_{\\mathbf{A}}(\\lambda)$ su polinomio característico. Se tiene entonces que $\\mathbf{A}$ es diagonalizable si y sólo si $P_{\\mathbf{A}}(\\lambda)$ puede descomponerse en $\\mathbb{K}$ en una serie de factores lineales. Es decir,*\n",
    "\n",
    "$$P_{\\mathbf{A} }\\left( \\lambda \\right)  =c_{\\mathbf{A} }\\left( \\lambda -\\lambda_{1} \\right)^{\\alpha_{\\mathbf{A} } \\left( \\lambda_{1} \\right)  }  \\left( \\lambda -\\lambda_{2} \\right)^{\\alpha_{\\mathbf{A} } \\left( \\lambda_{2} \\right)  }  \\cdots \\left( \\lambda -\\lambda_{k} \\right)^{\\alpha_{\\mathbf{A} } \\left( \\lambda_{k} \\right)  }  =\\prod^{k}_{i=1} c_{\\mathbf{A} }\\left( \\lambda -\\lambda_{i} \\right)^{\\alpha_{\\mathbf{A} } \\left( \\lambda_{i} \\right)  }$$\n",
    "<p style=\"text-align: right;\">$(3.33)$</p>\n",
    "\n",
    "*Además, para cada autovalor $\\lambda$ de $\\mathbf{A}$, se debe tener que $\\gamma_{\\mathbf{A}}(\\lambda)=\\alpha_{\\mathbf{A}}(\\lambda)$.* ◆\n",
    "\n",
    "**Ejemplo 3.6:** Determinaremos los valores de $a$ y $b$ para los cuales la matriz\n",
    "\n",
    "$$\\mathbf{A} =\\left( \\begin{matrix}2a-b&0&2a-2b\\\\ 1&a&2\\\\ -a+b&0&-a+2b\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.34)$</p>\n",
    "\n",
    "es diagonalizable.\n",
    "\n",
    "En efecto, partimos calculando el polinomio característico de $\\mathbf{A}$,\n",
    "\n",
    "$$\\begin{array}{rcl}P_{\\mathbf{A} }\\left( \\lambda \\right)  &=&\\det \\left( \\mathbf{A} -\\lambda \\mathbf{I}_{3} \\right)  \\\\ &=&\\det \\left( \\begin{matrix}2a-b-\\lambda &0&2a-2b\\\\ 1&a-\\lambda &2\\\\ -a+b&0&-a+2b-\\lambda \\end{matrix} \\right)  \\\\ &=&\\left( a-\\lambda \\right)  \\det \\left( \\begin{matrix}2a-b-\\lambda &2a-2b\\\\ -a+b&-a+2b-\\lambda \\end{matrix} \\right)  \\\\ &\\underbrace{=}_{F_{21}\\left( 1\\right)  } &\\left( a-\\lambda \\right)  \\det \\left( \\begin{matrix}2a-b-\\lambda &2a-2b\\\\ a-\\lambda &a-\\lambda \\end{matrix} \\right)  \\\\ &=&\\left( a-\\lambda \\right)^{2}  \\det \\left( \\begin{matrix}2a-b-\\lambda &2a-2b\\\\ 1&1\\end{matrix} \\right)  \\\\ &=&\\left( a-\\lambda \\right)^{2}  \\left( b-\\lambda \\right)  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.36)$</p>\n",
    "\n",
    "Así pues las raíces del polinomio característico $P_{\\mathbf{A} }\\left( \\lambda \\right)$ son $\\lambda=a$ y $\\lambda=b$. Por lo tanto, separamos la solución de este problema en dos casos posibles, en los cuales se puede tener que $a\\neq b$ o $a=b$. Entonces, si $a\\neq b$, los autovalores asociados a $\\mathbf{A}$ son $\\lambda_{1}=a$ y $\\lambda_{2}=b$. La multiplicidad algebraica de $\\lambda_{1}=a$ es 2. Para determinar su multiplicidad geométrica, debemos determinar la dimensión del autoespacio asociado a este autovalor. Así tenemos que,\n",
    "\n",
    "$$W_{a}=\\ker \\left( \\mathbf{A} -a\\mathbf{I}_{3} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.37)$</p>\n",
    "\n",
    "Luego,\n",
    "\n",
    "$$\\begin{array}{lll}\\left( \\mathbf{A} -a\\mathbf{I}_{3} \\right)  \\mathbf{x} =\\left( \\begin{matrix}0\\\\ 0\\\\ 0\\end{matrix} \\right)  &\\Longleftrightarrow &\\mathbf{x} =\\left( \\begin{matrix}x_{1}\\\\ x_{2}\\\\ x_{3}\\end{matrix} \\right)  \\in \\mathbb{R}^{3\\times 1} \\wedge a\\neq b;\\left( \\begin{matrix}a-b&0&2a-2b\\\\ 1&0&2\\\\ -a+b&0&-2a+2b\\end{matrix} \\right)  \\left( \\begin{matrix}x_{1}\\\\ x_{2}\\\\ x_{3}\\end{matrix} \\right)  =\\left( \\begin{matrix}0\\\\ 0\\\\ 0\\end{matrix} \\right)  \\\\ &\\Longleftrightarrow &\\mathbf{x} =\\left( \\begin{matrix}x_{1}\\\\ x_{2}\\\\ x_{3}\\end{matrix} \\right)  \\in \\mathbb{R}^{3\\times 1} \\wedge a\\neq b;\\left( \\begin{matrix}1&0&2\\\\ a-b&0&2a-2b\\\\ -a+b&0&-2a+2b\\end{matrix} \\right)  \\left( \\begin{matrix}x_{1}\\\\ x_{2}\\\\ x_{3}\\end{matrix} \\right)  =\\left( \\begin{matrix}0\\\\ 0\\\\ 0\\end{matrix} \\right)  \\\\ &\\Longleftrightarrow &\\mathbf{x} =\\left( \\begin{matrix}x_{1}\\\\ x_{2}\\\\ x_{3}\\end{matrix} \\right)  \\in \\mathbb{R}^{3\\times 1} \\wedge a\\neq b;\\left( \\begin{matrix}1&0&2\\\\ 0&0&0\\\\ 0&0&0\\end{matrix} \\right)  \\left( \\begin{matrix}x_{1}\\\\ x_{2}\\\\ x_{3}\\end{matrix} \\right)  =\\left( \\begin{matrix}0\\\\ 0\\\\ 0\\end{matrix} \\right)  \\\\ &\\Longrightarrow &\\rho \\left( \\mathbf{A} -a\\mathbf{I}_{3} \\right)  =1\\\\ &\\Longrightarrow &\\dim \\left( W_{a}\\right)  =2\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.38)$</p>\n",
    "\n",
    "Por lo tanto, la multiplicidad geométrica de $\\lambda_{1}=a$ es 2. Para $\\lambda_{2}=b$, la multiplicidad geométrica es 1, ya que su multiplicidad algebraica es también igual a 1. En resumen, si $a\\neq b$, se tiene que\n",
    "\n",
    "$$\\begin{array}{l}\\lambda_{1} =a\\  ;\\  \\gamma_{\\mathbf{A} } \\left( \\lambda_{1} \\right)  =2\\  ;\\  \\alpha_{\\mathbf{A} } \\left( \\lambda_{1} \\right)  =2\\\\ \\lambda_{2} =b\\  ;\\  \\gamma_{\\mathbf{A} } \\left( \\lambda_{2} \\right)  =1\\  ;\\  \\alpha_{\\mathbf{A} } \\left( \\lambda_{2} \\right)  =1\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.39)$</p>\n",
    "\n",
    "Así que, por el teorema (3.7), la matriz $\\mathbf{A}$ es diagonalizable cuando $a\\neq b$.\n",
    "\n",
    "Cuando $a=b$, tenemos un único autovalor 𝜆=𝑎 con multiplicidad algebraica $\\alpha_{\\mathbf{A}}(\\lambda)=3$. Calculamos por tanto su multiplicidad geométrica como sigue\n",
    "\n",
    "$$\\begin{array}{lll}\\left( \\mathbf{A} -a\\mathbf{I}_{3} \\right)  \\mathbf{x} =\\left( \\begin{matrix}0\\\\ 0\\\\ 0\\end{matrix} \\right)  &\\Longleftrightarrow &\\mathbf{x} =\\left( \\begin{matrix}x_{1}\\\\ x_{2}\\\\ x_{3}\\end{matrix} \\right)  \\in \\mathbb{R}^{3} \\wedge \\left( \\begin{matrix}0&0&0\\\\ 1&0&2\\\\ 0&0&0\\end{matrix} \\right)  \\left( \\begin{matrix}x_{1}\\\\ x_{2}\\\\ x_{3}\\end{matrix} \\right)  =\\left( \\begin{matrix}0\\\\ 0\\\\ 0\\end{matrix} \\right)  \\\\ &\\Longrightarrow &\\rho \\left( \\mathbf{A} -a\\mathbf{I}_{3} \\right)  =1\\\\ &\\Longrightarrow &\\dim \\left( W_{a}\\right)  =1\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.40)$</p>\n",
    "\n",
    "Por lo tanto, la multiplicidad geométrica de $\\lambda=a$ es igual a 1. De esta manera, conforme el teorema (3.7), dado que las multiplicidades algebraica y geométrica de $\\lambda=a$ son distintas, deducimos que $\\mathbf{A}$ no es diagonalizable cuando $a=b$. ◼︎"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
