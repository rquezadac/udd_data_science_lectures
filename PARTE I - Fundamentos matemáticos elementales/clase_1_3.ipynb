{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12635fd3",
   "metadata": {},
   "source": [
    "# CLASE 1.3: Descomposiciones matriciales\n",
    "\n",
    "## Introducción.\n",
    "En las secciones anteriores estudiamos algunas formas de manipular y obtener ciertas métricas para los vectores, proyecciones de esos vectores con respecto a determinados subespacios vectoriales y transformaciones lineales. Las aplicaciones y transformaciones que permiten operar con vectores pueden ser convenientemente descritas por medio de matrices. Además, la mayoría de los conjuntos de datos *bien comportados* que podemos encontrar en el mundo real vienen especificados en estructuras que pueden ser arregladas y/o representadas igualmente por medio de matrices. Por ejemplo, las filas de estos conjuntos de datos suelen representar **registros** u **observaciones** (como personas, fechas, unidades, entre otras) y las columnas suelen representar diferentes **atributos** para cada fila (como edad, altura, propiedades extensivas de algún fenómeno o el valor de alguna variable en el tiempo). En esta sección, presentaremos tres aspectos relativos a las matrices: Cómo **resumirlas**, como **descomponerlas** y como utilizar tales descomposiciones para construir **aproximaciones** para determinadas matrices.\n",
    "\n",
    "A diferencia de la sección anterior, en ésta volveremos a escribir algo de código, a fin de corresponder algunos conceptos esenciales con librerías tales como <font color='purple'>Numpy</font> o <font color='purple'>Scipy</font>. No será demasiado código, pero sí el suficiente para darle algo de sentido práctico a los conceptos que desarrollaremos desde la perspectiva computacional."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff217d6b",
   "metadata": {},
   "source": [
    "## Determinantes.\n",
    "\n",
    "### Un interludio previo.\n",
    "El concepto de determinante corresponde a otro de los elementos más importantes del álgebra lineal. Corresponde a un objeto matemático que es importante en el análisis y solución de sistemas de ecuaciones lineales (los mismos que vimos en el inicio de la [clase 1.1](https://github.com/rquezadac/udd_data_science_lectures/blob/main/PARTE%20I%20-%20Fundamentos%20matem%C3%A1ticos%20elementales/clase_1_1.ipynb)) y que puede expresarse por medio de una función, denominada como **función determinante**, y que permite aplicar cualquier matriz cuadrada de orden $n$ en el cuerpo $\\mathbb{K}$ donde sus elementos están definidos. Dicha función, para una matriz $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$, se denota como $\\det(\\mathbf{A})=|\\mathbf{A}|$, y es tal que $\\left| \\  \\cdot \\  \\right|  :\\mathbb{K}^{n} \\times \\mathbb{K}^{n} \\longrightarrow \\mathbb{K}$, pudiéndose escribir el determinante de la matriz $\\mathbf{A}$ como\n",
    "\n",
    "$$\\mathbf{A} =\\left\\{ a_{ij}\\in \\mathbb{K} \\right\\}  =\\left( \\begin{matrix}a_{11}&a_{12}&\\cdots &a_{1n}\\\\ a_{21}&a_{22}&\\cdots &a_{2n}\\\\ \\vdots &\\vdots &\\ddots &\\vdots \\\\ a_{n1}&a_{n2}&\\cdots &a_{nn}\\end{matrix} \\right)  \\in \\mathbb{K}^{n\\times n} \\wedge \\det \\left( \\mathbf{A} \\right)  =\\left| \\begin{matrix}a_{11}&a_{12}&\\cdots &a_{1n}\\\\ a_{21}&a_{22}&\\cdots &a_{2n}\\\\ \\vdots &\\vdots &\\ddots &\\vdots \\\\ a_{n1}&a_{n2}&\\cdots &a_{nn}\\end{matrix} \\right|  \\in \\mathbb{K}$$\n",
    "<p style=\"text-align: right;\">$(3.1)$</p>\n",
    "\n",
    "**Ejemplo 3.1:** Comenzaremos a motivar el estudio de los determinantes explorando la posibilidad de que una matriz cuadrada, digamos $\\mathbf{A}\\in \\mathbb{R}^{n\\times n}$, sea **invertible**. Para los casos de menor dimensión, ya conocemos los casos que aseguran que $\\mathbf{A}$ cumpla con esta condición. Por ejemplo, si $\\mathbf{A}\\in \\mathbb{R}^{1\\times 1}$ (es decir, $\\mathbf{A}$ es un escalar), sabemos que $\\mathbf{A}=a\\ \\Longrightarrow \\mathbf{A}^{-1}=1/a$, lo que implica que $\\mathbf{A}$ tiene una inversa siempre que $a\\neq 0$. Para el caso de matrices de $2\\times 2$, sabemos que la inversa $\\mathbf{A}^{-1}$ cumple con la condición de que $\\mathbf{A}\\mathbf{A}^{-1}=\\mathbf{I}_{2}$. De esta manera, podemos escribir\n",
    "\n",
    "$$\\mathbf{A} =\\left( \\begin{matrix}a_{11}&a_{12}\\\\ a_{21}&a_{22}\\end{matrix} \\right)  \\Longrightarrow \\mathbf{A} \\mathbf{A}^{-1} =\\mathbf{I}_{2} \\Longleftrightarrow \\mathbf{A}^{-1} =\\frac{1}{a_{11}a_{22}-a_{12}a_{21}} \\left( \\begin{matrix}a_{22}&-a_{12}\\\\ -a_{21}&a_{11}\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.2)$</p>\n",
    "\n",
    "Por lo tanto, $\\mathbf{A}$ es invertible si y sólo si $a_{11}a_{22}-a_{12}a_{21}\\neq 0$. Para matrices de $2\\times 2$, dicha cantidad corresponde al **determinante** de la matriz respectiva. De esta manera, para la matriz $\\mathbf{A} =\\left\\{ a_{ij}\\right\\}  \\in \\mathbb{R}^{2\\times 2}$, su determinante se define como\n",
    "\n",
    "$$\\det \\left( \\mathbf{A} \\right)  =\\det \\left( \\begin{matrix}a_{11}&a_{12}\\\\ a_{21}&a_{22}\\end{matrix} \\right)  =\\left| \\begin{matrix}a_{11}&a_{12}\\\\ a_{21}&a_{22}\\end{matrix} \\right|  =a_{11}a_{22}-a_{12}a_{21}$$\n",
    "<p style=\"text-align: right;\">$(3.3)$</p>\n",
    "◼︎\n",
    "\n",
    "El ejemplo (3.1) permite establecer un hecho que puede ser generalizado a conjuntos de mayor dimensión: Una matriz es invertible siempre que su determinante no sea nulo. Formalicemos este hecho mediante un teorema.\n",
    "\n",
    "**<font color='crimson'>Teorema 3.1 – Existencia de matriz inversa:</font>** *Sea $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$ una matriz cuadrada con elementos en el cuerpo $\\mathbb{K}$. Entonces $\\mathbf{A}$ se dirá **invertible** o **no singular** (es decir, existe la matriz inversa $\\mathbf{A}^{-1}$) si y solo si $\\det(\\mathbf{A})\\neq 0$.*\n",
    "◆\n",
    "\n",
    "Ya disponemos de una expresión cerrada que permite calcular el determinante de cualquier matriz de dimensión $2\\times 2$. Sin embargo, no es tan sencillo generalizar dicho cálculo para dimensiones superiores. Por ejemplo, para el caso de matrices de $3\\times 3$, es común el cálculo de sus determinantes mediante la llamada regla de Sarrus:\n",
    "\n",
    "$$\\det \\left( \\begin{matrix}a_{11}&a_{12}&a_{13}\\\\ a_{21}&a_{22}&a_{23}\\\\ a_{31}&a_{32}&a_{33}\\end{matrix} \\right)  =a_{11}a_{22}a_{33}+a_{21}a_{32}a_{13}+a_{31}a_{12}a_{23}-a_{31}a_{22}a_{13}-a_{11}a_{32}a_{23}-a_{21}a_{12}a_{33}$$\n",
    "<p style=\"text-align: right;\">$(3.4)$</p>\n",
    "\n",
    "Si bien, en un principio, la regla de Sarrus parece una fórmula complicada de entender, ésta no es más que un recurso mnemotécnico, ya que los triples productos involucrados en la fórmula y sus signos guardan relación con las diagonales (y subdiagonales) presentes en la matriz correspondiente, como se observa en el esquema de la Fig. (3.1)\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"figures/fig_3_1.png\" width=\"650\"></p>\n",
    "<p style=\"text-align: center;\">Fig. (3.1): Esquema que ilustra cómo opera la regla de Sarrus</p>\n",
    "\n",
    "Recordemos que, al resolver sistemas de ecuaciones lineales, nuestro objetivo era transformar la matriz ampliada de un sistema en una tal que sólo tuviera elementos no nulos en su región superior derecha (matriz triangular superior), aunque también es posible operar para llegar al caso opuesto, donde la matriz resultante tenga elementos no nulos en su región inferior izquierda (matriz triangular inferior). Este tipo de matrices fueron formalizadas previamente en la definición (1.7).\n",
    "\n",
    "Para una matriz triangular, digamos $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$, definimos su determinante como\n",
    "\n",
    "$$\\det \\left( \\mathbf{A} \\right)  =\\prod^{n}_{i=1} a_{ii}$$\n",
    "<p style=\"text-align: right;\">$(3.5)$</p>\n",
    "\n",
    "Donde $a_{ii}$ es el correspondiente elemento relativo a la diagonal principal de la matriz $\\mathbf{A}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2de076",
   "metadata": {},
   "source": [
    "### Interpretación geométrica del determinante.\n",
    "Si una matriz $\\mathbf{A}\\in \\mathbb{R}^{n\\times n}$ tiene elementos $a_{ij}\\in \\mathbb{R}$, entonces puede ser utilizada para representar dos transformaciones lineales. Una que aplica la base canónica de $\\mathbb{R}^{n}$ a las filas de $\\mathbf{A}$, y otra que aplica la misma base a las columnas de $\\mathbf{A}$. Cualquiera sea el caso, si la matriz $\\mathbf{A}$ es de $2\\times 2$, las imágenes de cada uno de los vectores de la base canónica de $\\mathbb{R}^{2}$ forman un paralelógramo que representa la imagen del cuadrado unitario bajo la transformación lineal respectiva, y cuyos vértices se corresponden con combinaciones de los elementos de $\\mathbf{A}$, como se observa en la Fig. (3.2a).\n",
    "\n",
    "Si $\\mathbf{A} =\\left\\{ {}a_{ij}\\right\\}  \\in \\mathbb{R}^{2\\times 2}$ es la matriz que conforma los vértices del paralelógramo en la Fig. (3.2a), se tendrá que el área encerrada por el mismo será igual a $\\det \\left( \\mathbf{A} \\right)  =a_{11}a_{22}-a_{12}a_{21}$. Para mostrar este resultado, podemos considerar que los elementos de la matriz $\\mathbf{A}$ corresponden a vectores que representan los vértices del paralelógramo. Si uno de los vértices es el origen del sistema de coordenadas, los vectores $\\mathbf{a} =\\left( a_{11},a_{12}\\right)$ y $\\mathbf{b} =\\left( a_{21},a_{22}\\right)$ serán los vértices más cercanos al origen, mientras que el vértice opuesto será igual a la suma $\\mathbf{a} +\\mathbf{b} =\\left( a_{11}+a_{21},a_{12}+a_{22}\\right)$. El área del paralelógramo puede expresarse igualmente como $\\left\\Vert \\mathbf{a} \\right\\Vert  \\left\\Vert \\mathbf{b} \\right\\Vert  \\mathrm{sen} \\left( \\theta \\right)$, donde $\\theta$ es el ángulo formado por los vectores $\\mathbf{a}$ y $\\mathbf{b}$. Si consideramos la proyección ortogonal del vector $\\mathbf{a}$ sobre $\\mathbf{b}$ (que llamamos $\\pi_{\\mathbf{b}}(\\mathbf{a})$), podemos escribir\n",
    "\n",
    "$$\\mathrm{Area} =\\left\\Vert \\mathbf{a} \\right\\Vert  \\left\\Vert \\mathbf{b} \\right\\Vert  \\mathrm{sen} \\left( \\theta \\right)  =\\left\\Vert \\pi_{\\mathbf{b} } \\left( \\mathbf{a} \\right)  \\right\\Vert  \\left\\Vert \\mathbf{b} \\right\\Vert  \\cos \\left( \\frac{\\pi }{2} -\\theta \\right)  =a_{11}a_{12}-a_{12}a_{21}=\\det \\left( \\mathbf{A} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.6)$</p>\n",
    "\n",
    "La interpretación geométrica anterior puede extenderse al caso de matrices de $3\\times 3$, considerando en este caso un paralelepípedo generado por las submatrices columna que generan la matriz completa $\\mathbf{A} =\\left\\{ a_{ij}\\right\\}  \\in \\mathbb{R}^{3\\times 3}$. Si tales submatrices son representadas como $\\mathbf{r}_{1}$, $\\mathbf{r}_{2}$ y $\\mathbf{r}_{3}$, entonces el volumen del paralelepípedo es igual a $V=\\det \\left( \\left( \\mathbf{r}_{1} ,\\mathbf{r}_{2} ,\\mathbf{r}_{3} \\right)  \\right)  =\\det \\left( \\mathbf{A} \\right)$, tal y como se ilustra en la Fig (3.2b).\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"figures/fig_3_2.png\" width=\"900\"></p>\n",
    "<p style=\"text-align: center;\">Fig. (3.2): (a) Esquema que muestra cómo el determinante de una matriz de $2\\times 2$ permite transformar un cuadrado unitario en un paralelógramo cuyos vértices son las productos que componen dicho determinante y su área será igual al valor de tal determinante; (b) Misma transformación para el caso de una matriz de $3\\times 3$. En este caso, la transformación se aplica sobre un paralelepípedo, obteniéndose un trapezoedro cuyo volumen es igual al correspondiente determinante</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517bd95f",
   "metadata": {},
   "source": [
    "### Propiedades de los determinantes.\n",
    "El cálculo de un determinante para una matriz arbitraria $\\mathbf{A} =\\left\\{ a_{ij}\\right\\}  \\in \\mathbb{R}^{n\\times n}$ requiere de un algoritmo generalizado para poder resolver los casos en los cuales $n>3$. Existen varios procedimientos para ello, siendo indudablemente el más popular el **método de Laplace**, que definiremos a continuación.\n",
    "\n",
    "**<font color='blue'>Definición 3.1 – Determinante:</font>** Sea $\\mathbf{A} =\\left\\{ a_{ij}\\right\\}  \\in \\mathbb{R}^{n\\times n}$. Definimos el **determinante** de la matriz $\\mathbf{A}$ como\n",
    "\n",
    "1. Respecto a la columna $j$: $\\det \\left( \\mathbf{A} \\right)  =\\sum^{n}_{k=1} \\left( -1\\right)^{k+j}  a_{kj}\\det \\left( \\mathbf{A}_{kj} \\right)$.\n",
    "2. Respecto a la fila $j$: $\\det \\left( \\mathbf{A} \\right)  =\\sum^{n}_{k=1} \\left( -1\\right)^{k+j}  a_{jk}\\det \\left( \\mathbf{A}_{jk} \\right)$.\n",
    "\n",
    "En la fórmulas anteriores, conocidas en la práctica como **expansiones de Laplace**, $\\mathbf{A}_{jk}$ corresponde a la submatriz resultante de eliminar de $\\mathbf{A}$ la fila $j$ y la columna $k$ y se denomina como **menor complementario** en la posición $(j, k)$, mientras que el número real $\\triangle_{jk} =\\left( -1\\right)^{k+j}  a_{jk}$ es llamado **cofactor** asociado al $jk$-ésimo elemento de la matriz $\\mathbf{A}$.\n",
    "\n",
    "Resulta sencillo darnos cuenta de que, si bien las expansiones de Laplace nos permiten obtener una fórmula cerrada para el cálculo del determinante de cualquier matriz cuadrada, su tiempo de ejecución y complejidad computacional escala enormemente con la dimensión de la matriz para la cual queremos calcular su determinante. Por esa razón, en términos algebraicos, es mejor considerar ciertas propiedades que se desprenden directamente de la definición que hemos ido construyendo del determinante a fin de disponer de métodos más efectivos para su cálculo en dimensiones superiores (considerando, además, las transformaciones elementales sobre matrices que hemos aprendido previamente). Vamos, por tanto, a desarrollar tales propiedades:\n",
    "\n",
    "- **(P1) – Invariancia ante la transposición:** Si $\\mathbf{A} =\\left\\{ a_{{}ij}\\right\\}  \\in \\mathbb{R}^{n\\times n}$, entonces, de la definición (3.1), se tiene que $\\det(\\mathbf{A})=\\det(\\mathbf{A}^{\\top})$, ya que $\\det \\left( \\mathbf{A} \\right)  =\\sum^{n}_{k=1} \\triangle_{jk} \\det \\left( \\mathbf{A}_{jk} \\right)  =\\sum^{n}_{s=1} \\triangle_{sj} \\det \\left( \\mathbf{A}_{sj} \\right)$.\n",
    "- **(P2) – Columna o fila nula:** Si $\\mathbf{A} =\\left\\{ a_{{}ij}\\right\\}  \\in \\mathbb{R}^{n\\times n}$ posee una columna o fila nula (conformada únicamente por elementos iguales a cero), entonces $\\det(\\mathbf{A})=0$.\n",
    "- **(P3) – Determinante de un producto de matrices:** Si $\\mathbf{A},\\mathbf{B}\\in \\mathbb{R}^{n\\times n}$, entonces $\\det(\\mathbf{A}\\mathbf{B})=\\det(\\mathbf{A})\\det(\\mathbf{B})$.\n",
    "- **(P4) – Determinante de la matriz inversa:** Si $\\mathbf{A} =\\left\\{ a_{{}ij}\\right\\}  \\in \\mathbb{R}^{n\\times n}$ es una matriz no singular, entonces $\\det \\left( \\mathbf{A}^{-1} \\right)  =1/\\det \\left( \\mathbf{A} \\right)$.\n",
    "- **(P5) – Invariancia ante operaciones elementales:** Cualquier matriz $\\mathbf{A}\\in \\mathbb{R}^{n\\times n}$ mantiene el valor de su determinante, aunque hayamos operado sobre ella mediante cualquier de las transformaciones elementales vistas en la [clase 1.1](https://github.com/rquezadac/udd_data_science_lectures/blob/main/PARTE%20I%20-%20Fundamentos%20matem%C3%A1ticos%20elementales/clase_1_1.ipynb).\n",
    "- **(P6) – Escalamiento del determinante:** Sea $\\mathbf{A} =\\left\\{ a_{ij}\\right\\}  \\in \\mathbb{R}^{n\\times n}$ y $\\lambda \\in \\mathbb{R}$. Entonces $\\det \\left( \\lambda \\mathbf{A} \\right)  =\\lambda^{n} \\det \\left( \\mathbf{A} \\right)$.\n",
    "- **(P7) – Columna o fila repetida:** Si una matriz $\\mathbf{A} =\\left\\{ a_{ij}\\right\\}  \\in \\mathbb{R}^{n\\times n}$ tiene columnas o filas repetidas (o, más general, linealmente dependientes), entonces $\\det(\\mathbf{A})=0$. Esta propiedad generaliza **(P2)** y establece que toda matriz $\\mathbf{A}$ no singular tiene rango completo.\n",
    "\n",
    "**Ejemplo 3.2:** Calcularemos el determinante de la matriz $\\mathbf{A}\\in \\mathbb{R}^{5\\times 5}$, definida como\n",
    "\n",
    "$$\\mathbf{A} =\\left( \\begin{matrix}0&1&0&1&0\\\\ -1&a&0&0&0\\\\ 0&0&a&0&0\\\\ -1&0&0&a&0\\\\ 0&0&0&0&a\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.7)$</p>\n",
    "\n",
    "En efecto, aplicando la definición (3.1) y transformaciones elementales,\n",
    "\n",
    "$$\\begin{array}{rcl}\\det \\left( \\begin{matrix}0&1&0&1&0\\\\ -1&a&0&0&0\\\\ 0&0&a&0&0\\\\ -1&0&0&a&0\\\\ 0&0&0&0&a\\end{matrix} \\right)  &\\overbrace{=}^{\\mathrm{definicion} } &\\underbrace{a}_{\\mathrm{cofactor} \\  \\triangle_{55} } \\det \\left( \\begin{matrix}0&1&0&1\\\\ -1&a&0&0\\\\ 0&0&a&0\\\\ -1&0&0&a\\end{matrix} \\right)  \\\\ &\\overbrace{=}^{F_{42}\\left( -1\\right)  } &a\\det \\left( \\begin{matrix}0&1&0&1\\\\ -1&a&0&0\\\\ 0&0&a&0\\\\ 0&-a&0&a\\end{matrix} \\right)  \\\\ &\\overbrace{=}^{\\mathrm{definicion} } &\\underbrace{1}_{\\mathrm{cofactor} \\  \\triangle_{21} } \\cdot a\\det \\left( \\begin{matrix}1&0&1\\\\ 0&a&0\\\\ -a&0&a\\end{matrix} \\right)  \\\\ &\\overbrace{=}^{\\mathrm{definicion} } &a^{2}\\det \\left( \\begin{matrix}1&1\\\\ -a&a\\end{matrix} \\right)  \\overbrace{=}^{\\mathrm{definicion} } 2a^{3}\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.8)$</p>\n",
    "◼︎\n",
    "\n",
    "**Ejemplo 3.3 – Los determinantes en <font color='purple'>Numpy</font>:** En librerías de Python especializadas en el uso de arreglos vectorizados, como <font color='purple'>Numpy</font>, es razonable esperar que existan rutinas prefabricadas para el cálculo de determinantes. En particular, podemos usar la función `det()`, del módulo de álgebra lineal `numpy.linalg()` para calcular el determinante de cualquier matriz expresada por medio de un arreglo bidimensional. Por ejemplo, si consideramos la matriz $\\mathbf{A}\\in \\mathbb{R}^{5\\times 5}$, definida como\n",
    "\n",
    "$$\\mathbf{A} =\\left( \\begin{matrix}0&-1&2&-1&0\\\\ 3&-1&2&2&0\\\\ 6&-1&0&0&9\\\\ 0&1&4&-5&9\\\\ 2&2&-4&5&-3\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.9)$</p>\n",
    "\n",
    "Podemos calcular su determinante fácilmente en <font color='purple'>Numpy</font> definiendo, primeramente, un arreglo bidimensional, digamos `A`, donde almacenamos esta matriz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4930228f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8ae75cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el arreglo en cuestión.\n",
    "A = np.array([\n",
    "    [0, -1, 2, -1, 0],\n",
    "    [3, -1, 2, 2, 0],\n",
    "    [6, -1, 0, 0, 9],\n",
    "    [0, 1, 4, -5, 9],\n",
    "    [2, 2, -4, 5, -3],\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edb371a",
   "metadata": {},
   "source": [
    "Y luego aplicando la función `np.linalg.det()` para calcular su determinante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "babed61a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-162.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculamos el determinante de A (redondeado a 3 decimales).\n",
    "np.around(np.linalg.det(A), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808cdb32",
   "metadata": {},
   "source": [
    "Vemos pues que no fue nada difícil calcular el determinante de una matriz de $5\\times 5$ en <font color='purple'>Numpy</font>. Sin embargo, tal y como comentamos previamente, el cálculo de determinantes corresponde a un esfuerzo computacional ostensiblemente grande y que escala enormemente a medida que aumentan las dimensiones de las matrices de interés. Incluso trabajando con una librería muy eficiente como <font color='purple'>Numpy</font>, los tiempos de ejecución pueden verse muy afectados. Para ejemplificar aquello, consideraremos el cálculo de los determinantes de cuatro matrices $\\mathbf{A}\\in \\mathbb{R}^{10\\times 10}$, $\\mathbf{B}\\in \\mathbb{R}^{100\\times 100}$, $\\mathbf{C}\\in \\mathbb{R}^{1000\\times 1000}$ y $\\mathbf{D}\\in \\mathbb{R}^{10000\\times 10000}$, las que representaremos mediante los arreglos bidimensionales `A`, `B`, `C` y `D`, y que estarán compuestas por números reales uniformemente distribuidos entre 0 y 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b78e9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos una semilla aleatoria fija.\n",
    "rng = np.random.default_rng(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16246b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos algunas matrices de distintos tamaños.\n",
    "A = rng.random(size=(10, 10))\n",
    "B = rng.random(size=(100, 100))\n",
    "C = rng.random(size=(1000, 1000))\n",
    "D = rng.random(size=(10000, 10000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc009552",
   "metadata": {},
   "source": [
    "Vamos a estimar el tiempo de ejecución asociado al cálculo de los determinantes de estas matrices, a fin de observar qué tal escala con respecto al incremento en dimensionalidad de las mismas. Notemos que cada matriz es 10 veces más grande que su antecesora:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d33dea85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.04 µs ± 85.6 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n",
      "91.4 µs ± 2.22 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/felipequezada/opt/anaconda3/lib/python3.9/site-packages/numpy/linalg/linalg.py:2158: RuntimeWarning: overflow encountered in det\n",
      "  r = _umath_linalg.det(a, signature=signature)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.18 ms ± 603 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "3.93 s ± 444 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.linalg.det(A)\n",
    "%timeit np.linalg.det(B)\n",
    "%timeit np.linalg.det(C)\n",
    "%timeit np.linalg.det(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e00d0d3",
   "metadata": {},
   "source": [
    "Podemos observar que el cálculo del determinante de `B` tiene un tiempo de ejecución 12 veces mayor que el cálculo del determinante de `A`. El cálculo del determinante de `C` tiene un tiempo de ejecución de aproximadamente unas 82 veces superior al del cálculo del determinante de `B` (y, por extensión, 1038 veces más lento que el cálculo del determinante de `A`). Y el cálculo del determinante de `D` tiene un tiempo de ejecución aproximadamente unas 445 veces más lento que el cálculo del determinante de `C` (y, por extensión... ¡es más de 36000 veces más lento que el cálculo del determinante de `B`, y más de 460000 veces más lento que el cálculo del determinante de `A`!). Esto definitivamente nos hará pensarlo dos veces antes de calcular determinantes en el mundo real, donde resulta común vernos enfrentados a bases de datos con cientos de miles de registros. ◼︎ \n",
    "\n",
    "**Ejemplo 3.4:** Vamos a demostrar que\n",
    "\n",
    "$$\\det \\left( \\begin{matrix}1&1&1\\\\ x&y&z\\\\ x^{2}&y^{2}&z^{2}\\end{matrix} \\right)  =\\left( x-y\\right)  \\left( y-z\\right)  \\left( z-x\\right)$$\n",
    "<p style=\"text-align: right;\">$(3.10)$</p>\n",
    "\n",
    "En efecto, utilizando transformaciones elementales, propiedades de los determinantes y la definición (3.1), tenemos que\n",
    "\n",
    "$$\\begin{array}{rcl}\\det \\left( \\begin{matrix}1&1&1\\\\ x&y&z\\\\ x^{2}&y^{2}&z^{2}\\end{matrix} \\right)  &\\overbrace{=}^{F_{21}\\left( -x\\right)  } &\\det \\left( \\begin{matrix}1&1&1\\\\ 0&y-x&z-x\\\\ x^{2}&y^{2}&z^{2}\\end{matrix} \\right)  \\\\ &\\overbrace{=}^{F_{31}\\left( -x^{2}\\right)  } &\\det \\left( \\begin{matrix}1&1&1\\\\ 0&y-x&z-x\\\\ 0&y^{2}-x^{2}&z^{2}-x^{2}\\end{matrix} \\right)  \\\\ &\\overbrace{=}^{\\mathrm{definicion} } &\\det \\left( \\begin{matrix}y-x&z-x\\\\ y^{2}-x^{2}&z^{2}-x^{2}\\end{matrix} \\right)  \\\\ &=&\\left( y-z\\right)  \\left( z^{2}-x^{2}\\right)  -\\left( z-x\\right)  \\left( y^{2}-x^{2}\\right)  \\\\ &=&\\left( y-x\\right)  \\left( z-x\\right)  \\left( z+x-y+x\\right)  \\\\ &=&\\left( y-x\\right)  \\left( z-x\\right)  \\left( z-y\\right)  \\\\ &=&\\left( x-y\\right)  \\left( y-z\\right)  \\left( z-x\\right)  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.11)$</p>\n",
    "\n",
    "Tal como queríamos demostrar. ◼︎\n",
    "\n",
    "**Ejemplo 3.5:** Vamos a determinar todos los valores de $a\\in \\mathbb{R}$ tales que\n",
    "\n",
    "$$\\det \\left( \\begin{matrix}1&a&a^{2}&\\left( a^{3}+6\\right)  \\\\ 1&1&1&\\left( 1+6a\\right)  \\\\ 1&2&4&\\left( 8+3a\\right)  \\\\ 1&3&9&\\left( 27+2a\\right)  \\end{matrix} \\right)  =0$$\n",
    "<p style=\"text-align: right;\">$(3.12)$</p>\n",
    "\n",
    "En efecto,\n",
    "\n",
    "$$\\begin{array}{rcl}\\det \\left( \\begin{matrix}1&a&a^{2}&\\left( a^{3}+6\\right)  \\\\ 1&1&1&\\left( 1+6a\\right)  \\\\ 1&2&4&\\left( 8+3a\\right)  \\\\ 1&3&9&\\left( 27+2a\\right)  \\end{matrix} \\right)  &\\overbrace{=}^{\\begin{array}{c}F_{21}\\left( -1\\right)  \\\\ F_{31}\\left( -1\\right)  \\\\ F_{41}\\left( -1\\right)  \\end{array} } &\\det \\left( \\begin{matrix}1&a&a^{2}&\\left( a^{3}+6\\right)  \\\\ 0&\\left( 1-a\\right)  &\\left( 1-a^{2}\\right)  &\\left( 1-a^{3}+6\\left( a-1\\right)  \\right)  \\\\ 0&\\left( 2-a\\right)  &\\left( 4-a^{2}\\right)  &\\left( 8-a^{3}+3a-6\\right)  \\\\ 0&\\left( 3-a\\right)  &\\left( 9-a^{2}\\right)  &\\left( 27-a^{3}+2a-6\\right)  \\end{matrix} \\right)  \\\\ &\\overbrace{=}^{\\mathrm{definicion} } &\\det \\left( \\begin{matrix}\\left( 1-a\\right)  &\\left( 1-a^{2}\\right)  &\\left( 1-a^{3}+6\\left( a-1\\right)  \\right)  \\\\ \\left( 2-a\\right)  &\\left( 4-a^{2}\\right)  &\\left( 8-a^{3}+3a-6\\right)  \\\\ \\left( 3-a\\right)  &\\left( 9-a^{2}\\right)  &\\left( 27-a^{3}+2a-6\\right)  \\end{matrix} \\right)  \\\\ &=&\\det \\left( \\begin{matrix}\\left( 1-a\\right)  &\\left( 1-a^{2}\\right)  &\\left[ \\left( 1-a\\right)  \\left( a^{2}+a+1\\right)  +6\\left( a-1\\right)  \\right]  \\\\ \\left( 2-a\\right)  &\\left( 4-a^{2}\\right)  &\\left[ \\left( 2-a\\right)  \\left( a^{2}+2a+4\\right)  +3\\left( a-2\\right)  \\right]  \\\\ \\left( 3-a\\right)  &\\left( 9-a^{2}\\right)  &\\left[ \\left( 3-a\\right)  \\left( a^{2}+3a+9\\right)  +2\\left( a-3\\right)  \\right]  \\end{matrix} \\right)  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.13)$</p>\n",
    "\n",
    "Es claro, conforme el desarrollo anterior, que el determinante se anula cuando $a=1$, $a=2$ o $a=3$. Para $a\\neq 1$, $a\\neq 2$ y $a\\neq 3$, proseguimos con el desarrollo del determinante, con lo cual,\n",
    "\n",
    "$$\\begin{array}{rcl}\\det \\left( \\begin{matrix}1&a&a^{2}&\\left( a^{3}+6\\right)  \\\\ 1&1&1&\\left( 1+6a\\right)  \\\\ 1&2&4&\\left( 8+3a\\right)  \\\\ 1&3&9&\\left( 27+2a\\right)  \\end{matrix} \\right)  &\\overbrace{=}^{\\mathrm{propiedades} } &\\left( 1-a\\right)  \\left( 2-a\\right)  \\left( 3-a\\right)  \\det \\left( \\begin{matrix}1&\\left( 1+a\\right)  &\\left( a^{2}+a-5\\right)  \\\\ 1&\\left( 2+a\\right)  &\\left( a^{2}+2a+1\\right)  \\\\ 1&\\left( 3+a\\right)  &\\left( a^{2}+3a+7\\right)  \\end{matrix} \\right)  \\\\ &\\overbrace{=}^{\\begin{array}{c}F_{21}\\left( -1\\right)  \\\\ F_{31}\\left( -1\\right)  \\end{array} } &\\left( 1-a\\right)  \\left( 2-a\\right)  \\left( 3-a\\right)  \\det \\left( \\begin{matrix}1&\\left( 1+a\\right)  &\\left( a^{2}+a-5\\right)  \\\\ 0&1&\\left( a+6\\right)  \\\\ 0&2&\\left( a+6\\right)  \\end{matrix} \\right)  \\\\ &=&2\\left( 1-a\\right)  \\left( 2-a\\right)  \\left( 3-a\\right)  \\det \\left( \\begin{matrix}1&\\left( 1+a\\right)  &\\left( a^{2}+a-5\\right)  \\\\ 0&1&\\left( a+6\\right)  \\\\ 0&1&\\left( a+6\\right)  \\end{matrix} \\right)  \\\\ &=&0\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.14)$</p>\n",
    "\n",
    "Luego tenemos que $\\det(\\mathbf{A})=0$ para todo $a\\in \\mathbb{R}$. ◼︎"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e704cec",
   "metadata": {},
   "source": [
    "## Diagonalización de matrices.\n",
    "Una vez estudiado el concepto de determinante, vamos a ocuparnos de un problema más general y que consiste en saber cuando, para una transformación lineal del tipo $T:\\mathbb{R}^{n}\\longrightarrow \\mathbb{R}^{n}$, es posible encontrar una base $\\alpha$ con respecto a la cual la matriz asociada $\\mathbf{A}=[T]_{\\alpha}^{\\alpha}$ sea de tipo **diagonal**. De manera equivalente, queremos determinar las condiciones para las cuales una matriz $\\mathbf{A}\\in \\mathbb{R}^{n\\times n}$ puede *descomponerse* de la forma\n",
    "\n",
    "$$\\mathbf{A} =\\mathbf{P} \\mathbf{D} \\mathbf{P}^{-1}$$\n",
    "<p style=\"text-align: right;\">$(3.15)$</p>\n",
    "\n",
    "Donde $\\mathbf{D}\\in \\mathbb{R}^{n\\times n}$ es una matriz diagonal.\n",
    "\n",
    "Este problema de naturaleza puramente algebraica tiene una cantidad significativa de aplicaciones en otras ramas de las matemáticas, como en ecuaciones diferenciales, estadística y, por supuesto, en machine learning. Puntualmente, la diagonalización es un procedimiento esencial en la derivación de la descomposición de matrices en **valores singulares** y que, a su vez, constituye la base del **análisis de componentes principales**, uno de los modelos de aprendizaje no supervisado más utilizados para la reducción de la dimensión de conjuntos de datos con un elevado número de variables, sin perder una cantidad significativa de información. Tal vez el teorema más importante de esta subsección es el que dice que toda matriz simétrica puede representarse mediante la expresión (3.15).\n",
    "\n",
    "### Autovalores y autovectores.\n",
    "Para comenzar con el estudio de la diagonalización de matrices, primero introduciremos algunos conceptos y resultados esenciales.\n",
    "\n",
    "**<font color='blue'>Definición 3.2 – Autovalores y autovectores:</font>** Sea $V$ un $\\mathbb{K}$-espacio vectorial y $T:V\\longrightarrow V$ una transformación lineal. Diremos que $v\\in V$ es un **autovector o vector propio** de $T$ si se cumplen las siguientes condiciones:\n",
    "\n",
    "- **(C1):** $v\\in O_{V}$.\n",
    "- **(C2):** Existe un escalar $\\lambda \\in \\mathbb{K}$ tal que $T(v)=\\lambda v$.\n",
    "\n",
    "El escalar $\\lambda$ se denomina **autovalor o valor propio** asociado al autovector $v$.\n",
    "\n",
    "Equivalentemente, diremos que $v\\in V-\\left\\{ O_{V}\\right\\}$ es un autovector asociado a la matriz $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$ si $v$ es un autovector de la transformación lineal $T:V\\longrightarrow V$ explícitamente definida como $T(v)=\\mathbf{A}v$. Es decir, existe un escalar $\\lambda \\in \\mathbb{K}$ tal que $\\mathbf{A}v=\\lambda v$. De la misma forma, diremos que $\\lambda$ es un autovalor de la matriz $\\mathbf{A}$.\n",
    "\n",
    "**<font color='crimson'>Teorema 3.2:</font>** *Dada una matriz $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$, si $\\lambda \\in \\mathbb{K}$ es un autovalor de $\\mathbf{A}$, entonces las siguientes expresiones son equivalentes:*\n",
    "\n",
    "- **(T1):** $\\exists v\\neq O_{V}\\  |\\  \\mathbf{A} v=\\lambda v$, *donde $V$ es un $\\mathbb{K}$-espacio vectorial y $v$ es un autovector de la matriz $\\mathbf{A}$.*\n",
    "- **(T2):** $\\exists v\\in V$ *tal que $v$ es una solución no trivial del sistema de ecuaciones $\\left( \\mathbf{A} -\\lambda \\mathbf{I}_{n} \\right)  v=O_{V}$.*\n",
    "- **(T3):** $\\ker \\left( \\mathbf{A} -\\lambda \\mathbf{I}_{n} \\right)  \\neq \\left\\{ O_{V}\\right\\}$.\n",
    "- **(T4):** $\\left( \\mathbf{A} -\\lambda \\mathbf{I}_{n} \\right)$ *es una matriz no invertible*.\n",
    "◆\n",
    "\n",
    "Queda claro pues que necesitamos una forma sencilla de determinar qué valores de $\\lambda \\in \\mathbb{K}$ son, en efecto, autovalores. Para ello, es útil reconocer que la condición **(T4)** en el teorema (3.2) puede expresarse como una ecuación en la variable $\\lambda$. Por supuesto, es acá donde cobra sentido el desarrollo que hicimos del concepto de determinante de una matriz $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$, puesto que, como ya verificamos con el teorema (3.1), toda matriz es no singular (invertible) si su determinante es no nulo. Por lo tanto, la condición **(T4)** puede expresarse como $\\det \\left( \\mathbf{A} -\\lambda \\mathbf{I}_{n} \\right)  =0$, donde $\\mathbf{I}_{n}$ es la matriz identidad.\n",
    "\n",
    "Tiene sentido, por lo tanto, la siguiente definición.\n",
    "\n",
    "**<font color='blue'>Definición 3.3 – Polinomio característico:</font>** Sea $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$ una matriz tal que ésta coincide con la representación matricial de una transformación lineal $T:V\\longrightarrow V$ que opera sobre el $\\mathbb{K}$-espacio vectorial $V$. La expresión $P_{T}\\left( \\lambda \\right)  =\\det \\left( \\mathbf{A} -\\lambda \\mathbf{I}_{n} \\right)  \\in \\mathbb{K}_{n} \\left[ \\lambda \\right]$ será llamada **polinomio característico** de la matriz $\\mathbf{A}$ (y, por extensión, de la transformación lineal $T$).\n",
    "\n",
    "De la definición (3.2), se tiene que $v\\in V$ es un autovector de $\\mathbf{A}$ si $v\\neq O_{V}$ y existe un escalar $\\lambda \\in \\mathbb{K}$ tal que $\\mathbf{A}v=\\lambda v$. De esta manera, podemos verificar que $\\lambda$ es un autovalor de la matriz $\\mathbf{A}$ si y sólo si es una solución no nula de la ecuación\n",
    "\n",
    "$$\\left( \\mathbf{A} -\\lambda \\mathbf{I}_{n} \\right)  v=O_{V}$$\n",
    "<p style=\"text-align: right;\">$(3.16)$</p>\n",
    "\n",
    "Así pues, todo escalar $\\lambda \\in \\mathbb{K}$ que satisfaga (3.16) será un autovalor de $\\mathbf{A}$. Notemos además que, conforme la expresión anterior, si $v$ es un autovector, también lo es cualquier otro vector que sea linealmente dependiente con respecto a $v$. Es decir, si 𝑣 es un autovector de la matriz $\\mathbf{A}$, entonces también lo es $\\alpha v;\\forall v\\in \\mathbb{K}$. Más aún, si $v$ es un autovector, cualquier autovalor asociado a $v$ es único, puesto que si $T(v)=\\lambda_{1}v=\\lambda_{2}v$, entonces $(\\lambda_{1}-\\lambda_{2})v=O_{V}$. Como $v\\neq O_{V}$, entonces $\\lambda_{1}-\\lambda_{2}=0$, lo que implica que $\\lambda_{1}=\\lambda_{2}$.\n",
    "\n",
    "**<font color='blue'>Definición 3.4 – Autoespacio:</font>** Sea $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$ una matriz tal que ésta coincide con la representación matricial de una transformación lineal $T:V\\longrightarrow V$, siendo $V$ un $\\mathbb{K}$-espacio vectorial. Para cada autovalor $\\lambda$ de $\\mathbf{A}$ definimos el **autoespacio o espacio propio** de $\\lambda$, denotado como $W_{\\lambda}$, como\n",
    "\n",
    "$$W_{\\lambda }=\\ker \\left( \\mathbf{A} -\\lambda \\mathbf{I}_{n} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.17)$</p>\n",
    "\n",
    "**<font color='blue'>Definición 3.5 – Similitud entre matrices:</font>** Sean $\\mathbf{A} ,\\mathbf{B} \\in \\mathbb{K}^{n\\times n}$ dos matrices no singulares. Diremos que $\\mathbf{A}$ y $\\mathbf{B}$ son **similares** si existe otra matriz $\\mathbf{P}\\in \\mathbb{K}^{n\\times 1}$ tal que\n",
    "\n",
    "$$\\mathbf{A} =\\mathbf{P} \\mathbf{B} \\mathbf{P}^{-1}$$\n",
    "<p style=\"text-align: right;\">$(3.18)$</p>\n",
    "\n",
    "**<font color='crimson'>Teorema 3.3 – Preservación de autovalores en matrices similares:</font>** *Sean $\\mathbf{A} ,\\mathbf{B} \\in \\mathbb{K}^{n\\times n}$ dos matrices similares entre sí. Entonces ambas matrices tienen los mismos autovalores (y, por extensión, el mismo polinomio característico).*\n",
    "\n",
    "Vamos a demostrar el teorema (3.3) a fin de entender completamente este resultado. En efecto, sean $\\mathbf{A} ,\\mathbf{B} \\in \\mathbb{K}^{n\\times n}$, tal que ambas matrices son similares (es decir, $\\mathbf{A} =\\mathbf{P} \\mathbf{B} \\mathbf{P}^{-1}$). Construyendo la expresión $\\mathbf{A} -\\lambda \\mathbf{I}_{n}$, tenemos que\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{A} -\\lambda \\mathbf{I}_{n} &=&\\mathbf{P} \\mathbf{B} \\mathbf{P}^{-1} -\\lambda \\mathbf{P} \\mathbf{P}^{-1} =\\mathbf{P} \\left( \\mathbf{B} -\\lambda \\mathbf{I}_{n} \\right)  \\mathbf{P}^{-1} \\\\ &\\Longrightarrow &\\det \\left( \\mathbf{A} -\\lambda \\mathbf{I}_{n} \\right)  =\\det \\left( \\mathbf{P} \\left( \\mathbf{B} -\\lambda \\mathbf{I}_{n} \\right)  \\mathbf{P}^{-1} \\right)  \\\\ &\\Longrightarrow &\\det \\left( \\mathbf{P} \\right)  \\det \\left( \\mathbf{B} -\\lambda \\mathbf{I}_{n} \\right)  \\det \\left( \\mathbf{P}^{-1} \\right)  \\  \\left( \\mathrm{pero} \\  \\det \\left( \\mathbf{P}^{-1} \\right)  =\\frac{1}{\\det \\left( \\mathbf{P} \\right)  } \\right)  \\\\ &\\Longrightarrow &\\det \\left( \\mathbf{B} -\\lambda \\mathbf{I}_{n} \\right)  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.19)$</p>\n",
    "\n",
    "Así, efectivamente, $\\mathbf{A}$ y $\\mathbf{B}$ tienen el mismo polinomio característico y, por tanto, los mismos autovalores.\n",
    "\n",
    "**Ejemplo 3.6:** Sea $T:\\mathbb{R}^{3}\\longrightarrow \\mathbb{R}^{3}$ una transformación lineal definida como $T(x,y,z)=(3x+2y+z,3y+2z,-z)$. Vamos a determinar los autovalores y autovectores asociados a $T$. \n",
    "\n",
    "En primer lugar, debemos construir la matriz $\\mathbf{A}$ asociada a $T$ considerando la base canónica de vectores en $\\mathbb{R}^{3}$ (que, recordemos, es $\\mathbf{e}(3)=\\left\\{ \\mathbf{e}_{1} ,\\mathbf{e}_{2} ,\\mathbf{e}_{3} \\right\\}  =\\left\\{ \\left( 1,0,0\\right)  ,\\left( 0,1,0\\right)  ,\\left( 0,0,1\\right)  \\right\\}$). Esto resulta sencillo, ya que\n",
    "\n",
    "$$T\\left( x,y,z\\right)  =\\left( 3x+2y+z,3y+2z,-z\\right)  =x\\begin{pmatrix}3\\\\ 0\\\\ 0\\end{pmatrix} +y\\begin{pmatrix}2\\\\ 3\\\\ 0\\end{pmatrix} +z\\begin{pmatrix}1\\\\ 2\\\\ -1\\end{pmatrix} \\Longrightarrow \\mathbf{A}= \\begin{pmatrix}3&2&1\\\\ 0&3&2\\\\ 0&0&-1\\end{pmatrix}$$\n",
    "<p style=\"text-align: right;\">$(3.20)$</p>\n",
    "\n",
    "Ahora debemos resolver la ecuación $P_{\\mathbf{A}}(\\lambda)=0$. En efecto,\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{A} =\\begin{pmatrix}3&2&1\\\\ 0&3&2\\\\ 0&0&-1\\end{pmatrix} &\\Longrightarrow &P_{\\mathbf{A} }\\left( \\lambda \\right)  =\\det \\left( \\begin{matrix}3-\\lambda &2&1\\\\ 0&3-\\lambda &2\\\\ 0&0&-1-\\lambda \\end{matrix} \\right)  =0\\\\ &\\Longrightarrow &P_{\\mathbf{A} }\\left( \\lambda \\right)  =\\left( \\lambda -3\\right)^{3}  \\left( \\lambda +1\\right)  =0\\Longleftrightarrow \\lambda_{1} =3\\wedge \\lambda_{2} =-1\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.21)$</p>\n",
    "\n",
    "Por lo tanto, los autovalores de la matriz $\\mathbf{A}$ (y, por extensión, de $T$) son $\\lambda_{1}=3$ y $\\lambda_{2}=-1$. Ahora determinamos los autoespacios respectivos,\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{u} \\in \\left( \\mathbb{R}^{3} \\right)_{\\lambda }  &\\Longleftrightarrow &\\mathbf{u} \\in \\mathbb{R}^{3} \\wedge T\\left( \\mathbf{u} \\right)  =\\lambda \\mathbf{u} \\\\ &\\Longleftrightarrow &\\mathbf{u} =\\left( x,y,z\\right)  \\wedge T\\left( x,y,z\\right)  =\\lambda \\left( x,y,z\\right)  \\\\ &\\Longleftrightarrow &\\mathbf{u} =\\left( x,y,z\\right)  \\wedge T\\left( x,y,z\\right)  =\\left( \\lambda x,\\lambda y,\\lambda z\\right)  \\\\ &\\Longleftrightarrow &\\mathbf{u} =\\left( x,y,z\\right)  \\wedge \\left( 3x+2y+z,3y+2z,-z\\right)  =\\left( \\lambda x,\\lambda y,\\lambda z\\right)  \\\\ &\\Longleftrightarrow &\\mathbf{u} =\\left( x,y,z\\right)  \\wedge \\begin{cases}\\begin{array}{rcl}3x+2y+z&=&\\lambda x\\\\ 3y+2z&=&\\lambda y\\\\ -z&=&\\lambda z\\end{array} &\\end{cases} \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.22)$</p>\n",
    "\n",
    "Debemos por tanto evaluar el sistema de ecuaciones determinado en (3.22) usando los autovalores determinados previamente. Así tenemos, para $\\lambda_{1}=3$,\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{u} \\in \\left( \\mathbb{R}^{3} \\right)_{\\lambda =3} &\\Longleftrightarrow &\\mathbf{u} =\\left( x,y,z\\right)  \\wedge \\begin{cases}\\begin{array}{rcl}3x+2y+z&=&\\lambda x\\\\ 3y+2z&=&\\lambda y\\\\ -z&=&\\lambda z\\end{array} &\\end{cases} \\\\ &\\Longleftrightarrow &\\mathbf{u} =\\left( x,y,z\\right)  \\wedge z=0,y=0\\\\ &\\Longleftrightarrow &\\mathbf{u} =\\left( x,0,0\\right)  ;x\\in \\mathbb{R} \\\\ &\\Longleftrightarrow &\\mathbf{u} =x\\left( 1,0,0\\right)  \\\\ &\\Longleftrightarrow &\\left( \\mathbb{R}^{3} \\right)_{\\lambda =3}  =\\left< \\left\\{ \\left( 1,0,0\\right)  \\right\\}  \\right>  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.23)$</p>\n",
    "\n",
    "Por otro lado, para $\\lambda_{2}=-1$, tenemos,\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{u} \\in \\left( \\mathbb{R}^{3} \\right)_{\\lambda =-1}  &\\Longleftrightarrow &\\mathbf{u} =\\left( x,y,z\\right)  \\wedge \\begin{cases}\\begin{array}{rcl}3x+2y+z&=&-x\\\\ 3y+2z&=&-y\\\\ -z&=&-z\\end{array} &\\end{cases} \\\\ &\\Longleftrightarrow &\\mathbf{u} =\\left( x,y,z\\right)  \\wedge z=2y,x=0\\\\ &\\Longleftrightarrow &\\mathbf{u} =\\left( 0,y,-2y\\right)  ;y\\in \\mathbb{R} \\\\ &\\Longleftrightarrow &\\mathbf{u} =y\\left( 0,1,-2\\right)  \\\\ &\\Longleftrightarrow &\\left( \\mathbb{R}^{3} \\right)_{\\lambda =-1}  =\\left< \\left\\{ \\left( 0,1,-2\\right)  \\right\\}  \\right>  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.24)$</p>\n",
    "◼︎\n",
    "\n",
    "Sea $T:\\mathbb{K}^{n}\\longrightarrow \\mathbb{K}^{n}$ una transformación lineal y sea $\\mathbf{A}=[T]_{\\alpha}^{\\alpha}$ la matriz asociada a $T$ en la base canónica $\\alpha$. Supongamos que existe una base de autovectores de $\\mathbf{A}$, definida como $\\beta =\\left\\{ \\mathbf{v}_{1},...,\\mathbf{v}_{n}\\right\\}$, siendo $\\beta$ por tanto una base de $\\mathbb{K}^{n}$. Para todo $i\\in \\mathbb{N}$, definimos $\\lambda_{i}\\in \\mathbb{K}$ tal que $\\mathbf{A}\\lambda \\mathbf{v}_{i}=\\lambda_{i} \\mathbf{v}_{i}$. Vemos que la matriz asociada a $T$ en la base $\\beta$, que denominamos como $\\mathbf{D}=[T]_{\\beta}^{\\beta}$, es diagonal, ya que\n",
    "\n",
    "$$\\begin{array}{rcl}\\mathbf{A} \\mathbf{v}_{1} =\\lambda_{1} \\mathbf{v}_{1} &\\Longrightarrow &\\lambda_{1} \\mathbf{v}_{1} =\\lambda_{1} \\mathbf{v}_{1} +0\\mathbf{v}_{2} +\\cdots +0\\mathbf{v}_{n} \\\\ \\mathbf{A} \\mathbf{v}_{2} =\\lambda_{2} \\mathbf{v}_{2} &\\Longrightarrow &\\lambda_{2} \\mathbf{v}_{2} =0\\mathbf{v}_{1} +\\lambda_{2} \\mathbf{v}_{2} +\\cdots +0\\mathbf{v}_{n} \\\\ &\\vdots &\\\\ \\mathbf{A} \\mathbf{v}_{n} =\\lambda_{n} \\mathbf{v}_{n} &\\Longrightarrow &\\lambda_{n} \\mathbf{v}_{n} =0\\mathbf{v}_{1} +0\\mathbf{v}_{2} +\\cdots +\\lambda_{n} \\mathbf{v}_{n} \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.25)$</p>\n",
    "\n",
    "Por lo tanto, podemos escribir\n",
    "\n",
    "$$\\left[ T\\right]^{\\beta }_{\\beta }  =\\left( \\left[ T\\left( \\mathbf{v}_{1} \\right)  \\right]_{\\beta }  ,\\left[ T\\left( \\mathbf{v}_{2} \\right)  \\right]_{\\beta }  ,...,\\left[ T\\left( \\mathbf{v}_{n} \\right)  \\right]_{\\beta }  \\right)  =\\left( \\begin{matrix}\\lambda_{1} &0&\\cdots &0\\\\ 0&\\lambda_{2} &\\cdots &0\\\\ \\vdots &\\vdots &\\ddots &\\vdots \\\\ 0&0&\\cdots &\\lambda_{n} \\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.26)$</p>\n",
    "\n",
    "Así que, efectivamente, la matriz $\\mathbf{D}=[ T]^{\\beta }_{\\beta }$ y, por ende, la matriz $\\mathbf{A}$ puede expresarse mediante la **descomposición propia** (o auto-descomposición) definida como $\\mathbf{A}=\\mathbf{P}\\mathbf{D}\\mathbf{P}^{-1}$. Esto resulta conveniente, ya que algunas ventajas de conocer la matriz diagonal $\\mathbf{D}$ son las siguientes:\n",
    "\n",
    "- **(V1):** $\\rho(\\mathbf{A})=\\rho(\\mathbf{D})=$ número de autovalores no nulos de $\\mathbf{A}$ (y, por extensión, de $\\mathbf{D}$). Recordemos que $\\rho(\\mathbf{A})$ denota el rango de la matriz $\\mathbf{A}$.\n",
    "- **(V2):** $\\det(\\mathbf{A})=\\det(\\mathbf{P}\\mathbf{D}\\mathbf{P}^{-1})=\\det(\\mathbf{P})\\det(\\mathbf{D})\\det(\\mathbf{P}^{-1})=\\det(\\mathbf{D})=\\prod^{n}_{k=1} \\lambda_{k}$.\n",
    "- **(V3):** Si $\\mathbf{A}$ es una matriz no singular (de decir, si $\\det(\\mathbf{A})\\neq 0$), entonces, para cada $\\lambda_{k}\\neq 0$ y $\\mathbf{A}^{-1}=\\mathbf{P}\\mathbf{D^{-1}}\\mathbf{P^{-1}}$, donde\n",
    "\n",
    "$$\\mathbf{D}^{-1} =\\left( \\begin{matrix}\\lambda^{-1}_{1} &0&\\cdots &0\\\\ 0&\\  \\lambda^{-1}_{2} &\\cdots &0\\\\ \\vdots &\\vdots &\\ddots &\\vdots \\\\ 0&0&\\cdots &\\lambda^{-1}_{n} \\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.27)$</p>\n",
    "\n",
    "- **(V4):** $\\mathbf{A}^{m} =\\left( \\mathbf{P} \\mathbf{D} \\mathbf{P}^{-1} \\right)^{m}  =\\left( \\mathbf{P} \\mathbf{D} \\mathbf{P}^{-1} \\right)  \\left( \\mathbf{P} \\mathbf{D} \\mathbf{P}^{-1} \\right)  \\overbrace{\\cdots }^{m\\  \\mathrm{veces} } \\left( \\mathbf{P} \\mathbf{D} \\mathbf{P}^{-1} \\right)$. Así que,\n",
    "\n",
    "$$\\mathbf{A}^{m} =\\mathbf{P} \\left( \\begin{matrix}\\lambda^{-m}_{1} &0&\\cdots &0\\\\ 0&\\lambda^{-m}_{2} &\\cdots &0\\\\ \\vdots &\\vdots &\\ddots &\\vdots \\\\ 0&0&\\cdots &\\lambda^{-m}_{n} \\end{matrix} \\right)  \\mathbf{P}^{-1}$$\n",
    "<p style=\"text-align: right;\">$(3.28)$</p>\n",
    "\n",
    "Como hemos visto, $\\mathbf{A} =\\mathbf{P} \\mathbf{D} \\mathbf{P}^{-1}$, donde $\\mathbf{P}=[\\mathbf{e}]_{\\beta}^{\\alpha}$ (donde $\\mathbf{e}(n)$ es la base canónica de vectores de $\\mathbb{K}^{n}$). Es decir, tenemos que expresar los correspondientes autovectores en términos de la base canónica de $\\mathbb{K}^{n}$. De esta manera, $\\mathbf{P}=(\\mathbf{v}_{1},...,\\mathbf{v}_{n})$. Tiene sentido entonces la siguiente definición.\n",
    "\n",
    "**<font color='blue'>Definición 3.6 – Matriz diagonalizable:</font>** Sea $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$ una matriz no singular. Diremos que $\\mathbf{A}$ es **diagonalizable** si $\\mathbb{K}^{n}$ admite una base de autovectores de $\\mathbf{A}$ (es decir, los autovectores de $\\mathbf{A}$ conforman un sistema de generadores para $\\mathbb{K}^{n}$ y son linealmente independientes).\n",
    "\n",
    "Con esta definición, ya podemos enunciar dos importantes teoremas relativos al proceso de diagonalización.\n",
    "\n",
    "**<font color='crimson'>Teorema 3.4:</font>** *Sea $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$ una matriz no singular. Entonces $\\mathbf{A}$ es diagonalizable si y sólo si $\\mathbf{A}$ es similar a una matriz diagonal.* ◆\n",
    "\n",
    "**<font color='crimson'>Teorema 3.5:</font>** *Sea $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$ una matriz no singular y sea $\\left\\{ \\lambda_{i} \\right\\}^{k}_{i=1}$ un conjunto de autovalores de $\\mathbf{A}$ (todos distintos). Si $\\left\\{ v_{i}\\right\\}^{k}_{i=1}$ representa el conjunto de autovectores de $\\mathbf{A}$ para el cual se tiene que $\\mathbf{A} v_{i}=\\lambda_{i} v_{i}$, entonces $\\left\\{ v_{i}\\right\\}^{k}_{i=1}$ es un conjunto linealmente independiente.* ◆\n",
    "\n",
    "Antes de proesguir, veremos la extensión natural del concepto de suma directa para más de dos subespacios vectoriales. Definamos primero los $k$ subespacios $U_{1},...,U_{k}$ de $V$. Definiremos la **suma** de tales subespacios como\n",
    "\n",
    "$$\\sum^{k}_{i=1} U_{i}=U_{1}+\\cdots +U_{k}\\triangleq \\left\\{ v=\\sum^{k}_{i=1} u_{i}\\  |\\  \\forall i\\in \\left\\{ 1,...,k\\right\\}  ,u_{i}\\in U_{i}\\right\\}$$\n",
    "<p style=\"text-align: right;\">$(3.29)$</p>\n",
    "\n",
    "La suma de subespacios así definida también es, como cabría esperar, un subespacio de $V$. Estamos en condiciones, por tanto, de establecer la siguiente definición.\n",
    "\n",
    "**<font color='blue'>Definición 3.7 – Suma directa múltiple:</font>** Sea $V$ un $\\mathbb{K}$-espacio vectorial y $\\left\\{ U_{i}\\right\\}^{k^{}}_{i=1}$ una colección de $k$ subespacios vectoriales de $V$. Diremos que el subespacio $Z=\\sum^{k}_{i=1} U_{i}$ es la **suma directa** de $\\left\\{ U_{i}\\right\\}^{k^{}}_{i=1}$, lo que denotamos como $Z=\\bigoplus^{k}_{i=1} U_{i}=U_{1}\\oplus \\cdots \\oplus U_{k}$ si, para todo $v\\in Z$, $v$ se escribe de manera única como $v=\\sum^{k}_{i=1} u_{i}$, donde $u_{i}\\in U_{i}$, para $i=1,...,k$. Es decir,\n",
    "\n",
    "$$Z=\\bigoplus^{k}_{i=1} U_{i}=U_{1}\\oplus \\cdots \\oplus U_{k}\\Longleftrightarrow v=\\sum^{k}_{i=1} u_{i}\\  ;\\  u_{i}\\in U_{i},\\forall i\\in \\left\\{ 1,...,k\\right\\}$$\n",
    "<p style=\"text-align: right;\">$(3.30)$</p>\n",
    "\n",
    "La suma directa múltiple de subespacios cumple con las siguientes propiedades:\n",
    "\n",
    "- **(P1):** $Z=\\bigoplus^{k}_{i=1} U_{i}\\Longleftrightarrow \\left( Z=\\sum^{k}_{i=1} U_{i}\\wedge \\forall j\\in \\left\\{ 1,...,k\\right\\}  ,U_{j}\\cap \\left( \\sum^{k}_{\\begin{matrix}i=1\\\\ i\\neq j\\end{matrix} } U_{i}\\right)  =\\left\\{ O_{V}\\right\\}  \\right)$.\n",
    "- **(P2):** Si $Z=\\sum^{k}_{i=1} U_{i}$ y $Z$ es de dimensión finita (lo que suele denotarse como $\\dim(Z)<\\infty$), entonces las siguientes proposiciones son equivalentes:\n",
    "    - $Z=\\bigoplus^{k}_{i=1} U_{i}$.\n",
    "    - $\\left( \\forall i=\\left\\{ 1,...,k\\right\\}  \\right)  \\left( \\forall u_{i}\\in U_{i}-\\left\\{ O_{V}\\right\\}  \\right)  ,\\left\\{ u_{1},...,u_{k}\\right\\}$ es linealmente independiente.\n",
    "    - La yuxtaposición de las bases de las bases de los subespacios $\\left\\{ U_{i}\\right\\}^{k}_{i=1}$ es una base (y no sólo un sistema de generadores) para $Z$.\n",
    "    - $\\dim(Z)=\\sum^{k}_{i=1} \\dim \\left( U_{i}\\right)$.\n",
    "\n",
    "Las propiedades que derivan de la definición (3.7) nos permiten formular el siguiente teorema.\n",
    "\n",
    "**<font color='crimson'>Teorema 3.6:</font>** *Sea $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$ una matriz no singular tal que $\\left\\{ \\lambda_{i} \\right\\}^{k}_{i=1}$ es el conjunto de autovalores de $\\mathbf{A}$ (todos distintos) y $W_{\\lambda_{i}}=\\ker(\\mathbf{A}-\\lambda \\mathbf{I}_{n})$ es el autoespacio asociado al autovalor $\\lambda_{i}$. Si $W=\\sum^{k}_{i=1} W_{\\lambda_{i} }$, entonces tenemos que*\n",
    "\n",
    "$$W=\\bigoplus^{k}_{i=1} W_{\\lambda_{i} }$$\n",
    "<p style=\"text-align: right;\">$(3.31)$</p>\n",
    "\n",
    "*En particular, $\\mathbf{A}$ es diagonalizable si y sólo si*\n",
    "\n",
    "$$\\mathbb{K}^{n} =\\bigoplus^{k}_{i=1} W_{\\lambda_{i} }$$\n",
    "<p style=\"text-align: right;\">$(3.32)$</p>\n",
    "◆\n",
    "\n",
    "**<font color='DodgerBlue'>Corolario 3.1:</font>** *Sea $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$ una matriz no singular tal que $\\left\\{ \\lambda_{i} \\right\\}^{k}_{i=1}$ es el conjunto de autovalores de $\\mathbf{A}$ (todos distintos) y $W_{\\lambda_{i}}=\\ker(\\mathbf{A}-\\lambda \\mathbf{I}_{n})$ es el autoespacio asociado al autovalor $\\lambda_{i}$. Si $W=\\sum^{k}_{i=1} W_{\\lambda_{i} }$, entonces tenemos que*\n",
    "\n",
    "- **(T1):** $W_{\\lambda_{i} }=\\ker \\left( \\mathbf{A} -\\lambda \\mathbf{I}_{n} \\right)$ *es de dimensión 1.*\n",
    "- **(T2):** *Sea $v_{i}\\in W_{\\lambda_{i}}$ con $v_{i}\\neq O_{V}$. Entonces $\\left\\{ v_{1},...,v_{n}\\right\\}$ es una base de autovectores.*\n",
    "◆\n",
    "\n",
    "Este resultado nos entrega una condición suficiente (pero no necesaria) para establecer que una matriz $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$ sea diagonalizable: En síntesis, se tiene que $\\mathbf{A}$ es diagonalizable si $\\mathbf{A}$ tiene $n$ autovalores distintos. Sin embargo, esto no excluyente.\n",
    "\n",
    "**<font color='blue'>Definición 3.8 – Multiplicidad geométrica:</font>** Sea $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$ una matriz no singular y $\\lambda$ un autovalor de $\\mathbf{A}$. Definimos la **multiplicidad geométrica** de $\\lambda$, denotada como $\\gamma_{\\mathbf{A}}(\\lambda)$, como la dimensión del autoespacio $W_{\\lambda }=\\ker \\left( \\mathbf{A} -\\lambda \\mathbf{I}_{n} \\right)$. Es decir, $\\gamma_{\\mathbf{A} } \\left( \\lambda \\right)  =\\dim \\left( W_{\\lambda }\\right)$.\n",
    "\n",
    "**<font color='blue'>Definición 3.9 – Multiplicidad algebraica:</font>** Sea $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$ una matriz no singular y $\\lambda$ un autovalor de $\\mathbf{A}$. Definimos la **multiplicidad algebraica** del autovalor $\\lambda$, denotada como $\\alpha_{\\mathbf{A}}(\\lambda)$, como la máxima potencia de $(x-\\lambda)$ que es divisor del polinomio característico de $\\mathbf{A}$.\n",
    "\n",
    "Con ambas definiciones de multiplicidad ya establecidas, y con los resultados obtenidos previamente, estamos en condiciones de establecer el siguiente teorema, que establece las condiciones necesarias y suficientes para garantizar que una matriz es diagonalizable.\n",
    "\n",
    "**<font color='crimson'>Teorema 3.7 – Criterio de diagonalización de matrices:</font>** *Sea $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$ una matriz no singular y $P_{\\mathbf{A}}(\\lambda)$ su polinomio característico. Se tiene entonces que $\\mathbf{A}$ es diagonalizable si y sólo si $P_{\\mathbf{A}}(\\lambda)$ puede descomponerse en $\\mathbb{K}$ en una serie de factores lineales. Es decir,*\n",
    "\n",
    "$$P_{\\mathbf{A} }\\left( \\lambda \\right)  =c_{\\mathbf{A} }\\left( \\lambda -\\lambda_{1} \\right)^{\\alpha_{\\mathbf{A} } \\left( \\lambda_{1} \\right)  }  \\left( \\lambda -\\lambda_{2} \\right)^{\\alpha_{\\mathbf{A} } \\left( \\lambda_{2} \\right)  }  \\cdots \\left( \\lambda -\\lambda_{k} \\right)^{\\alpha_{\\mathbf{A} } \\left( \\lambda_{k} \\right)  }  =\\prod^{k}_{i=1} c_{\\mathbf{A} }\\left( \\lambda -\\lambda_{i} \\right)^{\\alpha_{\\mathbf{A} } \\left( \\lambda_{i} \\right)  }$$\n",
    "<p style=\"text-align: right;\">$(3.33)$</p>\n",
    "\n",
    "*Además, para cada autovalor $\\lambda$ de $\\mathbf{A}$, se debe tener que $\\gamma_{\\mathbf{A}}(\\lambda)=\\alpha_{\\mathbf{A}}(\\lambda)$.* ◆\n",
    "\n",
    "**Ejemplo 3.7:** Determinaremos los valores de $a$ y $b$ para los cuales la matriz\n",
    "\n",
    "$$\\mathbf{A} =\\left( \\begin{matrix}2a-b&0&2a-2b\\\\ 1&a&2\\\\ -a+b&0&-a+2b\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.34)$</p>\n",
    "\n",
    "es diagonalizable.\n",
    "\n",
    "En efecto, partimos calculando el polinomio característico de $\\mathbf{A}$,\n",
    "\n",
    "$$\\begin{array}{rcl}P_{\\mathbf{A} }\\left( \\lambda \\right)  &=&\\det \\left( \\mathbf{A} -\\lambda \\mathbf{I}_{3} \\right)  \\\\ &=&\\det \\left( \\begin{matrix}2a-b-\\lambda &0&2a-2b\\\\ 1&a-\\lambda &2\\\\ -a+b&0&-a+2b-\\lambda \\end{matrix} \\right)  \\\\ &=&\\left( a-\\lambda \\right)  \\det \\left( \\begin{matrix}2a-b-\\lambda &2a-2b\\\\ -a+b&-a+2b-\\lambda \\end{matrix} \\right)  \\\\ &\\underbrace{=}_{F_{21}\\left( 1\\right)  } &\\left( a-\\lambda \\right)  \\det \\left( \\begin{matrix}2a-b-\\lambda &2a-2b\\\\ a-\\lambda &a-\\lambda \\end{matrix} \\right)  \\\\ &=&\\left( a-\\lambda \\right)^{2}  \\det \\left( \\begin{matrix}2a-b-\\lambda &2a-2b\\\\ 1&1\\end{matrix} \\right)  \\\\ &=&\\left( a-\\lambda \\right)^{2}  \\left( b-\\lambda \\right)  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.36)$</p>\n",
    "\n",
    "Así pues las raíces del polinomio característico $P_{\\mathbf{A} }\\left( \\lambda \\right)$ son $\\lambda=a$ y $\\lambda=b$. Por lo tanto, separamos la solución de este problema en dos casos posibles, en los cuales se puede tener que $a\\neq b$ o $a=b$. Entonces, si $a\\neq b$, los autovalores asociados a $\\mathbf{A}$ son $\\lambda_{1}=a$ y $\\lambda_{2}=b$. La multiplicidad algebraica de $\\lambda_{1}=a$ es 2. Para determinar su multiplicidad geométrica, debemos determinar la dimensión del autoespacio asociado a este autovalor. Así tenemos que,\n",
    "\n",
    "$$W_{a}=\\ker \\left( \\mathbf{A} -a\\mathbf{I}_{3} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.37)$</p>\n",
    "\n",
    "Luego,\n",
    "\n",
    "$$\\begin{array}{lll}\\left( \\mathbf{A} -a\\mathbf{I}_{3} \\right)  \\mathbf{x} =\\left( \\begin{matrix}0\\\\ 0\\\\ 0\\end{matrix} \\right)  &\\Longleftrightarrow &\\mathbf{x} =\\left( \\begin{matrix}x_{1}\\\\ x_{2}\\\\ x_{3}\\end{matrix} \\right)  \\in \\mathbb{R}^{3\\times 1} \\wedge a\\neq b;\\left( \\begin{matrix}a-b&0&2a-2b\\\\ 1&0&2\\\\ -a+b&0&-2a+2b\\end{matrix} \\right)  \\left( \\begin{matrix}x_{1}\\\\ x_{2}\\\\ x_{3}\\end{matrix} \\right)  =\\left( \\begin{matrix}0\\\\ 0\\\\ 0\\end{matrix} \\right)  \\\\ &\\Longleftrightarrow &\\mathbf{x} =\\left( \\begin{matrix}x_{1}\\\\ x_{2}\\\\ x_{3}\\end{matrix} \\right)  \\in \\mathbb{R}^{3\\times 1} \\wedge a\\neq b;\\left( \\begin{matrix}1&0&2\\\\ a-b&0&2a-2b\\\\ -a+b&0&-2a+2b\\end{matrix} \\right)  \\left( \\begin{matrix}x_{1}\\\\ x_{2}\\\\ x_{3}\\end{matrix} \\right)  =\\left( \\begin{matrix}0\\\\ 0\\\\ 0\\end{matrix} \\right)  \\\\ &\\Longleftrightarrow &\\mathbf{x} =\\left( \\begin{matrix}x_{1}\\\\ x_{2}\\\\ x_{3}\\end{matrix} \\right)  \\in \\mathbb{R}^{3\\times 1} \\wedge a\\neq b;\\left( \\begin{matrix}1&0&2\\\\ 0&0&0\\\\ 0&0&0\\end{matrix} \\right)  \\left( \\begin{matrix}x_{1}\\\\ x_{2}\\\\ x_{3}\\end{matrix} \\right)  =\\left( \\begin{matrix}0\\\\ 0\\\\ 0\\end{matrix} \\right)  \\\\ &\\Longrightarrow &\\rho \\left( \\mathbf{A} -a\\mathbf{I}_{3} \\right)  =1\\\\ &\\Longrightarrow &\\dim \\left( W_{a}\\right)  =2\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.38)$</p>\n",
    "\n",
    "Por lo tanto, la multiplicidad geométrica de $\\lambda_{1}=a$ es 2. Para $\\lambda_{2}=b$, la multiplicidad geométrica es 1, ya que su multiplicidad algebraica es también igual a 1. En resumen, si $a\\neq b$, se tiene que\n",
    "\n",
    "$$\\begin{array}{l}\\lambda_{1} =a\\  ;\\  \\gamma_{\\mathbf{A} } \\left( \\lambda_{1} \\right)  =2\\  ;\\  \\alpha_{\\mathbf{A} } \\left( \\lambda_{1} \\right)  =2\\\\ \\lambda_{2} =b\\  ;\\  \\gamma_{\\mathbf{A} } \\left( \\lambda_{2} \\right)  =1\\  ;\\  \\alpha_{\\mathbf{A} } \\left( \\lambda_{2} \\right)  =1\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.39)$</p>\n",
    "\n",
    "Así que, por el teorema (3.7), la matriz $\\mathbf{A}$ es diagonalizable cuando $a\\neq b$.\n",
    "\n",
    "Cuando $a=b$, tenemos un único autovalor 𝜆=𝑎 con multiplicidad algebraica $\\alpha_{\\mathbf{A}}(\\lambda)=3$. Calculamos por tanto su multiplicidad geométrica como sigue\n",
    "\n",
    "$$\\begin{array}{lll}\\left( \\mathbf{A} -a\\mathbf{I}_{3} \\right)  \\mathbf{x} =\\left( \\begin{matrix}0\\\\ 0\\\\ 0\\end{matrix} \\right)  &\\Longleftrightarrow &\\mathbf{x} =\\left( \\begin{matrix}x_{1}\\\\ x_{2}\\\\ x_{3}\\end{matrix} \\right)  \\in \\mathbb{R}^{3} \\wedge \\left( \\begin{matrix}0&0&0\\\\ 1&0&2\\\\ 0&0&0\\end{matrix} \\right)  \\left( \\begin{matrix}x_{1}\\\\ x_{2}\\\\ x_{3}\\end{matrix} \\right)  =\\left( \\begin{matrix}0\\\\ 0\\\\ 0\\end{matrix} \\right)  \\\\ &\\Longrightarrow &\\rho \\left( \\mathbf{A} -a\\mathbf{I}_{3} \\right)  =1\\\\ &\\Longrightarrow &\\dim \\left( W_{a}\\right)  =1\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.40)$</p>\n",
    "\n",
    "Por lo tanto, la multiplicidad geométrica de $\\lambda=a$ es igual a 1. De esta manera, conforme el teorema (3.7), dado que las multiplicidades algebraica y geométrica de $\\lambda=a$ son distintas, deducimos que $\\mathbf{A}$ no es diagonalizable cuando $a=b$. ◼︎\n",
    "\n",
    "**Ejemplo 3.8:** Consideremos la familia de isomorfismos $f_{a,b}:\\mathbb{R}^{3}\\longrightarrow \\mathbb{R}^{3}$, definida como $f(x,y,z)=(z,by,ax)$, donde $a,b\\in \\mathbb{R}$. Vamos a determinar los valores de $a$ y $b$ tales que $f_{a,b}$ es diagonalizable y, en caso de serlo, localizar su forma diagonal.\n",
    "\n",
    "En efecto, por el teorema (3.7), sabemos que, para que $f_{a,b}$ sea diagonalizable, su polinomio característico debe tener raíces no nulas y sus multiplicidades algebraicas y geométricas deben ser iguales. En efecto, primero necesitamos encontrar una matriz asociada a $f_{a,b}$, la cual perfectamente puede ser construida tomando la base canónica de $\\mathbb{R}^{3}$. En ese caso, tenemos que\n",
    "\n",
    "$$f_{a,b}\\left( x,y,z\\right)  =\\left( z,by,ax\\right)  =x\\left( \\begin{matrix}0\\\\ 0\\\\ a\\end{matrix} \\right)  +y\\left( \\begin{matrix}0\\\\ b\\\\ 0\\end{matrix} \\right)  +z\\left( \\begin{matrix}1\\\\ 0\\\\ 0\\end{matrix} \\right)  \\Longrightarrow \\mathbf{A} =\\left( \\begin{matrix}0&0&a\\\\ 0&b&0\\\\ 1&0&0\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.41)$</p>\n",
    "\n",
    "Con esta información, construimos el polinomio característico de $\\mathbf{A}$ como sigue,\n",
    "\n",
    "$$P_{\\mathbf{A} }\\left( \\lambda \\right)  =\\det \\left( \\mathbf{A} -\\lambda \\mathbf{I}_{3} \\right)  =\\det \\left( \\begin{matrix}-\\lambda &0&a\\\\ 0&b-\\lambda &0\\\\ 1&0&-\\lambda \\end{matrix} \\right)  =\\left( \\lambda^{2} -a\\right)  \\left( \\lambda -b\\right)$$\n",
    "<p style=\"text-align: right;\">$(3.42)$</p>\n",
    "\n",
    "Por lo tanto, tenemos que $\\lambda =\\pm a\\vee \\lambda =b$. Distinguimos de este modo cinco casos de interés:\n",
    "\n",
    "<font color='DarkTurquoise'>Caso 1:</font> Si $a>0\\wedge b\\neq \\pm \\sqrt{a}$, entonces $\\mathbf{A}$ es diagonalizable, porque tiene tres autovalores distintos. Su forma diagonal es, por tanto,\n",
    "\n",
    "$$\\mathbf{D} =\\begin{pmatrix}\\sqrt{a} &0&0\\\\ 0&-\\sqrt{a} &0\\\\ 0&0&b\\end{pmatrix}$$\n",
    "<p style=\"text-align: right;\">$(3.43)$</p>\n",
    "\n",
    "<font color='DarkTurquoise'>Caso 2:</font> Si $a=b=0$ entonces $\\lambda =0$ es el único autovalor de $\\mathbf{A}$ con multiplicidad algebraica igual a 3. En este caso, el autoespacio de $\\lambda$ es $W_{\\lambda =0}=\\left\\{ \\left( x,y,0\\right)  ;x,y\\in \\mathbb{R} \\right\\}$. Por lo tanto, $\\dim(W_{\\lambda =0})=2$, siendo entonces la multiplicidad geométrica de $\\lambda$ igual a 2. De esta manera, conforme el teorema (3.7), $\\mathbf{A}$ no es diagonalizable.\n",
    "\n",
    "<font color='DarkTurquoise'>Caso 3:</font> Si $a=0$ y $b\\neq 0$, entonces $\\mathbf{A}$ tiene dos autovalores distintos: $\\lambda_{1}=0$, con multiplicidad algebraica igual a 2, y $\\lambda_{2}=b$, con multiplicidad algebraica igual a 1. Pero $W_{\\lambda_{1} =0}=\\left\\{ \\left( x,0,0\\right)  ;x\\in \\mathbb{R} \\right\\}  \\Longrightarrow \\dim \\left( W_{\\lambda_{1} =0}\\right)  =1$, con lo cual $\\alpha_{\\mathbf{A} } \\left( \\lambda_{1} \\right)  \\neq \\gamma_{\\mathbf{A} } \\left( \\lambda_{1} \\right)$. De esta manera, por el teorema (3.7), deducimos que $\\mathbf{A}$ no es diagonalizable.\n",
    "\n",
    "<font color='DarkTurquoise'>Caso 4:</font> Si $a>0\\wedge b=\\sqrt{a}$, entonces $\\mathbf{A}$ tiene dos autovalores distintos, $\\lambda_{1}=\\sqrt{a}$ (con multiplicidad algebraica $\\gamma_{\\mathbf{A} } \\left( \\lambda_{1} \\right)  =2$) y $\\lambda_{2}=-\\sqrt{a}$ (con multiplicidad algebraica $\\gamma_{\\mathbf{A} } \\left( \\lambda_{2} \\right)  =1$). Luego tenemos que $W_{\\lambda_{1} =\\sqrt{a} }=\\left\\{ \\left( x,y,\\sqrt{a} x\\right)  |\\  x,y\\in \\mathbb{R} \\right\\}$, lo que implica que $W_{\\lambda_{1} =\\sqrt{a} }=2=\\gamma_{\\mathbf{A} } \\left( \\lambda_{1} \\right)$. De esta manera, por el teorema (3.7), deducimos que $\\mathbf{A}$ es diagonalizable. Su forma diagonal es\n",
    "\n",
    "$$\\mathbf{D} =\\left( \\begin{matrix}\\sqrt{a} &0&0\\\\ 0&\\sqrt{a} &0\\\\ 0&0&-\\sqrt{a} \\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.44)$</p>\n",
    "\n",
    "<font color='DarkTurquoise'>Caso 5:</font> Este caso es totálmente análogo al anterior. La matriz $\\mathbf{A}$ es diagonalizable, y su forma diagonal es\n",
    "\n",
    "$$\\mathbf{D} =\\left( \\begin{matrix}\\sqrt{a} &0&0\\\\ 0&-\\sqrt{a} &0\\\\ 0&0&-\\sqrt{a} \\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.45)$</p>\n",
    "◼︎\n",
    "\n",
    "**Ejemplo 3.9:** En <font color='purple'>Numpy</font> es sencillo determinar los autovalores y autovectores asociados a una matriz. Si consideramos el problema abordado en el ejemplo (3.6) y la matriz $\\mathbf{A}$ determinada en la ecuación (3.20), podemos resolver rápidamente la diagonalización por medio de la función `eig()`, dependiente del módulo de álgebra lineal de <font color='purple'>Numpy</font> (`numpy.linalg`). Esta función retorna una tupla con dos arreglos: El primero contiene los autovalores de la matriz de interés, y el segundo los autovectores asociados a cada autovalor (columna a columna):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "744a057b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d357f8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos la matriz A.\n",
    "A = np.array([\n",
    "    [3, 2, 1],\n",
    "    [0, 3, 2],\n",
    "    [0, 0, -1],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cef9b8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos los autovalores y autovectores de la matriz A.\n",
    "lambda_, v = linalg.eig(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7fdbae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.,  3., -1.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mostramos los autovalores en pantalla.\n",
    "lambda_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a50a88de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.    , -1.    ,  0.    ],\n",
       "       [ 0.    ,  0.    , -0.4472],\n",
       "       [ 0.    ,  0.    ,  0.8944]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mostramos los autovectores en pantalla.\n",
    "v.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a64ae1d",
   "metadata": {},
   "source": [
    "Vemos pues que este cálculo fue muy sencillo y bastante rápido. La razón de esto es que <font color='purple'>Numpy</font>, como bien sabemos, es una librería que está optimizada para el tratamiento de arreglos gracias a sus operaciones vectorizadas. Además, el cálculo de los autovalores y autovectores de una matriz en <font color='purple'>Numpy</font> no requiere del cálculo de determinantes (por el efecto del polinomio característico) de manera directa, ya que cuenta con rutinas de **aproximación** extremadamente eficientes que se ejecutan *tras bambalinas*. Por ejemplo, si verificamos los tiempos de ejecución del cálculo de autovalores y autovectores por medio de la función `eig()` sobre matrices cuyos tamaños se duplican cada vez, veremos que los tiempos de ejecución no escalan de manera significativa con respecto a estos tamaños:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd517fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos una semilla aleatoria fija.\n",
    "rng = np.random.default_rng(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3015b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construimos tres matrices, cada una el doble de grande que la anterior, conformadas por\n",
    "# números aleatorios enteros, uniformamente distribuidos entre -9 y 9.\n",
    "A = rng.integers(low=-10, high=10, size=(3, 3))\n",
    "B = rng.integers(low=-10, high=10, size=(6, 6))\n",
    "C = rng.integers(low=-10, high=10, size=(12, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86d8af66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.4 µs ± 2.36 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "36.3 µs ± 2.13 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "52.8 µs ± 433 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit linalg.eig(A)\n",
    "%timeit linalg.eig(B)\n",
    "%timeit linalg.eig(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fa47cf",
   "metadata": {},
   "source": [
    "◼︎ \n",
    "\n",
    "### Espectro de una matriz simétrica.\n",
    "Sea $\\mathbf{A}\\in \\mathbb{R}^{n\\times n}$ una matriz simétrica (es decir, $\\mathbf{A}=\\mathbf{A}^{\\top}$). Entonces podemos escribir\n",
    "\n",
    "$$\\forall \\mathbf{u} ,\\mathbf{v} \\in \\mathbb{R}^{n} :\\left< \\mathbf{A} \\mathbf{u} ,\\mathbf{v} \\right>  =\\left< \\mathbf{u} ,\\mathbf{A} \\mathbf{v} \\right>$$\n",
    "<p style=\"text-align: right;\">$(3.46)$</p>\n",
    "\n",
    "Consideremos además el producto interno canónico de matrices, definido para las matrices $\\mathbf{A}\\wedge \\mathbf{B}\\in \\mathbb{R}^{n\\times n}$ como\n",
    "\n",
    "$$\\left< \\mathbf{A} ,\\mathbf{B} \\right>  =\\mathrm{tr} \\left( \\mathbf{B}^{\\top } \\mathbf{A} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.47)$</p>\n",
    "\n",
    "En efecto, por las propiedades de este producto interno, tenemos que $\\left< \\mathbf{A} \\mathbf{u} ,\\mathbf{v} \\right>  =\\left( \\mathbf{A} \\mathbf{u} \\right)^{\\top }  \\mathbf{v} =\\mathbf{u}^{\\top } \\mathbf{A}^{\\top } \\mathbf{v} =\\mathbf{u}^{\\top } \\mathbf{A} \\mathbf{v} =\\left< \\mathbf{u} ,\\mathbf{A} \\mathbf{v} \\right>$. Probaremos ahora que, si $\\mathbf{A}\\in \\mathbb{R}^{n\\times n}$ y satisface la ecuación (3.46), entonces $\\mathbf{A}$ es simétrica. Podemos deducir aquello a partir del siguiente hecho: Si $\\mathbf{e} \\left( n\\right)  =\\left\\{ \\mathbf{e}_{1} ,...,\\mathbf{e}_{n} \\right\\}$ es la base canónica de $\\mathbb{R}^{n}$, entonces\n",
    "\n",
    "$$\\left< \\mathbf{A} \\mathbf{e}_{j} ,\\mathbf{e}_{i} \\right>  =a_{ji}$$\n",
    "<p style=\"text-align: right;\">$(3.48)$</p>\n",
    "\n",
    "Luego, usando la ecuación (3.46), obtenemos\n",
    "\n",
    "$$a_{ji}\\left< \\mathbf{A} \\mathbf{e}_{j} ,\\mathbf{e}_{i} \\right>  =\\left< \\mathbf{e}_{j} ,\\mathbf{A} \\mathbf{e}_{i} \\right>  =\\left< \\mathbf{A} \\mathbf{e}_{i} ,\\mathbf{e}_{j} \\right>$$\n",
    "\n",
    "Con esto en mente, vamos a construir la siguiente definición.\n",
    "\n",
    "**<font color='blue'>Definición 3.10 – Matriz transpuesta conjugada:</font>** Sea $\\mathbf{A}\\in \\mathbb{C}^{n\\times n}$ una matriz definida explícitamente como $\\mathbf{A} =\\left\\{ a_{jk}\\right\\}  ,a_{jk}\\in \\mathbb{C}$. Definimos la **matriz transpuesta conjugada** de $\\mathbf{A}$, denotada como $\\mathbf{A}^{\\ast }$, a la matriz $\\mathbf{A}^{\\ast } =\\left\\{ \\overline{a}_{jk} \\right\\}$, donde $\\overline{a}_{jk}$ es el número complejo conjugado de $a_{jk}$ (es decir, para $a_{jk}=u_{jk}+iv_{jk}$, donde $u_{jk},v_{jk}\\in \\mathbb{R}^{2}$, se tiene que $\\overline{a}_{jk} =u_{jk}-iv_{jk}$, donde $i=\\sqrt{-1}$ es la unidad imaginaria).\n",
    "\n",
    "Cuando $\\mathbf{A}=\\mathbf{A}^{\\ast}$, decimos que la matriz $\\mathbf{A}$ es **hermítica**. De esta manera, se tiene que toda matriz simétrica cuyos elementos están definidos sobre el cuerpo $\\mathbb{R}$ es, por tanto, hermítica.\n",
    "\n",
    "De forma análoga al caso simétrico, se prueba que\n",
    "\n",
    "$$\\forall u,v\\in \\mathbb{C} :\\left< \\mathbf{A} u,v\\right>  =\\left< u,\\mathbf{A} v\\right>$$\n",
    "<p style=\"text-align: right;\">$(3.49)$</p>\n",
    "\n",
    "si y sólo si $\\mathbb{A}$ es hermítica. Así, en particular, $\\forall \\mathbf{u} ,\\mathbf{v} \\in \\mathbb{C}^{n} ,\\left< \\mathbf{A} \\mathbf{u} ,\\mathbf{v} \\right>  =\\left< \\mathbf{u} ,\\mathbf{A} \\mathbf{v} \\right>$, si $\\mathbb{A}\\in \\mathbb{R}^{n\\times n}$ y $\\mathbb{A}$ es simétrica.\n",
    "\n",
    "Si $\\mathbb{A}\\in \\mathbb{C}^{n\\times n}$, entonces el polinomio característico $P_{\\mathbb{A}}(\\lambda)=\\det(\\mathbb{A}-\\lambda \\mathbb{I}_{n})$ es un polinomio de grado $n$ con coeficientes complejos y, por lo tanto, conforme el teorema fundamental del álgebra, tiene $n$ raíces en $\\mathbb{C}$ (posiblemente repetidas). Denotaremos por\n",
    "\n",
    "$$\\sigma \\left( \\mathbf{A} \\right)  =\\left\\{ \\lambda \\in \\mathbb{C} :\\det \\left( \\mathbf{A} -\\lambda \\mathbf{I}_{n} \\right)  =0\\right\\}$$\n",
    "<p style=\"text-align: right;\">$(3.50)$</p>\n",
    "\n",
    "al conjunto de todas las raíces del polinimo característico $P_{\\mathbb{A}}(\\lambda)$. Llamaremos a este conjunto **espectro** de la matriz $\\mathbf{A}$. Como $\\mathbb{R}^{n\\times n}\\subset \\mathbb{C}^{n\\times n}$, podemos de igual forma definir el espectro de $\\mathbf{A}$ para $\\mathbf{A}\\in \\mathbb{R}^{n\\times n}$. Sin embargo, el espectro de una matriz $\\mathbf{A}$ es un subconjunto de $\\mathbb{C}$ aun cuando $\\mathbf{A}\\in \\mathbb{R}^{n\\times n}$.\n",
    "\n",
    "**Ejemplo 3.10:** Consideremos la matriz $\\mathbf{B}$ definida como\n",
    "\n",
    "$$\\mathbf{B} =\\left( \\begin{matrix}0&1&0\\\\ -1&0&0\\\\ 0&0&1\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.51)$</p>\n",
    "\n",
    "Construimos el polinomio característico de $\\mathbf{B}$ como sigue,\n",
    "\n",
    "$$P_{\\mathbf{B} }\\left( \\lambda \\right)  =\\det \\left( \\mathbf{B} -\\lambda \\mathbf{I}_{3} \\right)  =\\det \\left( \\begin{matrix}-\\lambda &1&0\\\\ -1&-\\lambda &0\\\\ 0&0&1-\\lambda \\end{matrix} \\right)  =\\left( 1-\\lambda \\right)  \\left( \\lambda^{2} +1\\right)$$\n",
    "<p style=\"text-align: right;\">$(3.52)$</p>\n",
    "\n",
    "Las raíces del polinomio característico $P_{\\mathbf{B} }\\left( \\lambda \\right)$ son $\\lambda =1$ y $\\lambda =\\pm i$. Si $\\mathbf{B}$ es considerada como una matriz de elementos reales (es decir, estamos trabajando en $\\mathbb{R}^{n\\times n}$, $\\mathbf{B}$ tiene sólo un autovalor, que es $\\lambda=1$. Si el cuerpo considerado es $\\mathbb{C}$, entonces $\\mathbf{B}$ tiene tres autovalores, que se resumen en el conjunto $\\sigma \\left( \\mathbf{B} \\right)  =\\left\\{ 1,i,-i\\right\\}$. Este conjunto es, por tanto, el espectro de $\\mathbf{B}$.\n",
    "\n",
    "Estamos en condiciones, por lo tanto, de formular una serie de teoremas y corolarios que nos permitirán llegar a uno de los resultados más importantes de esta sección.\n",
    "\n",
    "**<font color='crimson'>Teorema 3.8:</font>** *Sea $\\mathbf{A}\\in \\mathbb{C}^{n\\times n}$ una matriz hermítica. Entonces el espectro de $\\mathbf{A}$ es tal que $\\sigma(\\mathbf{A})\\subseteq \\mathbb{R}$.* ◆\n",
    "\n",
    "**<font color='DodgerBlue'>Corolario 3.2:</font>** *Sea $\\mathbf{A}\\in \\mathbb{R}^{n\\times n}$ una matriz simétrica. Entonces $\\sigma(\\mathbf{A})\\subseteq \\mathbb{R}$.* ◆\n",
    "\n",
    "**<font color='crimson'>Teorema 3.9:</font>** *Sea $\\mathbf{A}\\in \\mathbb{C}^{n\\times n}$ una matriz hermítica, con $\\lambda_{1}$ y $\\lambda_{2}$ autovalores de $\\mathbf{A}$. Si $\\lambda_{1}\\neq \\lambda_{2}$ y $v_{1},v_{2}$ son los autovectores asociados a $\\lambda_{1}$ y $\\lambda_{2}$, respectivamente. Entonces $\\left< v_{1},v_{2}\\right>  =0$ con el producto interno canónico en $\\mathbb{C}$. Es decir, los autovectores $v_{1}$ y $v_{2}$ son ortogonales.* ◆\n",
    "\n",
    "**<font color='DodgerBlue'>Corolario 3.3:</font>** *Sea $\\mathbf{A}\\in \\mathbb{R}^{n\\times n}$ una matriz simétrica. Entonces los autovectores de $\\mathbf{A}$ asociados a autovalores distintos son ortogonales.* ◆\n",
    "\n",
    "**<font color='blue'>Definición 3.11 – Transformación simétrica:</font>** Sean $W$ un subespacio de $\\mathbb{R}$ y $T:W\\longrightarrow W$ una transformación lineal. Diremos que $T$ es una **transformación simétrica** en $W$ si se cumple que\n",
    "\n",
    "$$\\left< T\\left( \\mathbf{u} \\right)  ,\\mathbf{v} \\right>  =\\left< \\mathbf{u} ,T\\left( \\mathbf{v} \\right)  \\right>  ;\\forall \\mathbf{u} ,\\mathbf{v} \\in W$$\n",
    "<p style=\"text-align: right;\">$(3.53)$</p>\n",
    "\n",
    "Observemos que $T:\\mathbb{R}^{n}\\longrightarrow \\mathbb{R}^{n}$ es una transformación simétrica si y sólo si la matriz asociada a $T$ en base canónica de $\\mathbb{R}^{n}$ es simétrica.\n",
    "\n",
    "El siguiente es uno de los resultados más importantes de esta sección.\n",
    "\n",
    "**<font color='crimson'>Teorema 3.10 – Descomposición espectral:</font>** *Sea $W$ un subespacio de $\\mathbb{R}^{n}$, tal que $\\dim(W)\\geq 1$ y $T:W\\longrightarrow W$ una transformación lineal simétrica. Entonces existe una base ortonormal de $W$ compuesta por autovectores de $T$.* ◆\n",
    "\n",
    "**<font color='DodgerBlue'>Corolario 3.4:</font>** *Sea $\\mathbf{A}\\in \\mathbb{R}^{n\\times n}$ una matriz simétrica. Entonces $\\mathbf{A}$ es diagonalizable y, además, $\\mathbf{A}=\\mathbf{P} \\mathbf{D} \\mathbf{P}^{\\top}$, donde $\\mathbf{P}=(v_{1},...,v_{n})$ y $\\alpha =\\left\\{ v_{1},...,v_{n}\\right\\}$ es una base ortonormal de autovectores de $\\mathbf{A}$. Entonces $\\mathbf{D}$ es la matriz diagonal de los autovalores asociados a los autovectores que constituyen la base $\\alpha$.* ◆\n",
    "\n",
    "Del corolario (3.4) podemos establecer que toda matriz $\\mathbf{P}$ que satisface la expresión $\\mathbf{P}^{\\top}\\mathbf{P}=\\mathbf{I}_{n}$ se denomina **ortogonal** o **unitaria**. Entonces, si $\\mathbf{P}$ es una matriz ortogonal, entonces $\\mathbf{P}$ también preserva la norma inducida por el producto interno canónico de $\\mathbb{R}^{n\\times n}$. Es decir, $\\left\\Vert \\mathbf{P} \\mathbf{u} \\right\\Vert  =\\left< \\mathbf{P} \\mathbf{u} ,\\mathbf{P} \\mathbf{u} \\right>  =\\left< \\mathbf{u} ,\\mathbf{u} \\right>  =\\left\\Vert \\mathbf{u} \\right\\Vert^{2}$. Notemos además que no hay unicidad en la base de autovectores y, por lo tanto, una matriz diagonalizable $\\mathbf{A}$ puede descomponerse en la forma $\\mathbf{P} \\mathbf{D} \\mathbf{P}^{-1}$ de muchas maneras. Lo que hemos establecido para el caso de una matriz simétrica, es que se puede tomar una base ortonormal de autovectores y que, para esta base, se tiene la descomposición $\\mathbf{P} \\mathbf{D} \\mathbf{P}^{\\top}$.\n",
    "\n",
    "**Ejemplo 3.11:** Consideremos la matriz $\\mathbf{A}$, definida explícitamente como\n",
    "\n",
    "$$\\mathbf{A} =\\left( \\begin{matrix}2&0&0\\\\ 0&2&0\\\\ 0&0&1\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.54)$</p>\n",
    "\n",
    "Observemos que $P_{\\mathbf{A} }\\left( \\lambda \\right)  =\\det \\left( \\mathbf{A} -\\lambda \\mathbf{I}_{3} \\right)  =\\left( \\begin{matrix}2-\\lambda &0&0\\\\ 0&2-\\lambda &0\\\\ 0&0&1-\\lambda \\end{matrix} \\right)  =\\left( 2-\\lambda \\right)^{2}  \\left( 1-\\lambda \\right)$. Se tienen pues los autovalores $\\lambda_{1}=2$ (con multiplicidad algebraica igual a 2) y $\\lambda_{2}=1$.\n",
    "\n",
    "Calculemos el autoespacio asociado a $\\lambda_{1}=2$,\n",
    "\n",
    "$$W_{\\lambda_{1} =2}=\\ker \\left( \\mathbf{A} -2\\mathbf{I}_{3} \\right)  \\Longrightarrow \\left( x,y,z\\right)  \\in \\mathbb{R}^{3} \\wedge \\left( \\begin{matrix}0&0&0\\\\ 0&0&0\\\\ 0&0&-1\\end{matrix} \\right)  \\left( \\begin{matrix}x\\\\ y\\\\ z\\end{matrix} \\right)  =\\left( \\begin{matrix}0\\\\ 0\\\\ 0\\end{matrix} \\right)  \\Longleftrightarrow W_{\\lambda_{1} =2}=\\left\\{ \\left( x,y,z\\right)  \\in \\mathbb{R}^{3} :z=0\\right\\}$$\n",
    "<p style=\"text-align: right;\">$(3.55)$</p>\n",
    "\n",
    "Determinamos ahora el autoespacio $\\lambda_{2}=1$,\n",
    "\n",
    "$$W_{\\lambda_{2} =1}=\\ker \\left( \\mathbf{A} -\\mathbf{I}_{3} \\right)  \\Longrightarrow \\left( x,y,z\\right)  \\in \\mathbb{R}^{3} \\wedge \\left( \\begin{matrix}1&0&0\\\\ 0&1&0\\\\ 0&0&0\\end{matrix} \\right)  \\left( \\begin{matrix}x\\\\ y\\\\ z\\end{matrix} \\right)  =\\left( \\begin{matrix}0\\\\ 0\\\\ 0\\end{matrix} \\right)  \\Longleftrightarrow W_{\\lambda_{2} =1}=\\left\\{ \\left( x,y,z\\right)  \\in \\mathbb{R}^{3} :x=y=0\\right\\}$$\n",
    "<p style=\"text-align: right;\">$(3.56)$</p>\n",
    "\n",
    "Por lo tanto, una base de autovectores para $\\mathbb{R}^{3}$ es\n",
    "\n",
    "$$\\alpha =\\left\\{ \\left( \\begin{matrix}1\\\\ 0\\\\ 0\\end{matrix} \\right)  ,\\left( \\begin{matrix}1\\\\ 1\\\\ 0\\end{matrix} \\right)  ,\\left( \\begin{matrix}0\\\\ 0\\\\ 1\\end{matrix} \\right)  \\right\\}$$\n",
    "<p style=\"text-align: right;\">$(3.57)$</p>\n",
    "\n",
    "Luego,\n",
    "\n",
    "$$\\mathbf{A} =\\mathbf{P} \\mathbf{D} \\mathbf{P}^{-1} \\wedge \\mathbf{P} =\\left( \\begin{matrix}1&1&0\\\\ 0&1&0\\\\ 0&0&1\\end{matrix} \\right)  ,\\mathbf{P}^{-1} =\\left( \\begin{matrix}1&-1&0\\\\ 0&1&0\\\\ 0&0&1\\end{matrix} \\right)  ,\\mathbf{D} =\\left( \\begin{matrix}2&0&0\\\\ 0&2&0\\\\ 0&0&1\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.58)$</p>\n",
    "\n",
    "Pero $\\mathbf{A}$ también posee una base ortonormal de autovectores,\n",
    "\n",
    "$$\\beta =\\left\\{ \\left( \\begin{matrix}1\\\\ 0\\\\ 0\\end{matrix} \\right)  ,\\left( \\begin{matrix}0\\\\ 1\\\\ 0\\end{matrix} \\right)  ,\\left( \\begin{matrix}0\\\\ 0\\\\ 1\\end{matrix} \\right)  \\right\\}  \\wedge \\mathbf{A} =\\mathbf{P} \\mathbf{D} \\mathbf{P}^{-1} =\\left( \\begin{matrix}1&0&0\\\\ 0&1&0\\\\ 0&0&1\\end{matrix} \\right)  \\left( \\begin{matrix}2&0&0\\\\ 0&2&0\\\\ 0&0&1\\end{matrix} \\right)  \\left( \\begin{matrix}1&0&0\\\\ 0&1&0\\\\ 0&0&1\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.59)$</p>\n",
    "\n",
    "Cabe señalar que, en el caso de que si no hubiese sido sencillo obtener a simple vista una base ortonormal del autoespacio $W_{\\lambda_{1}=2}$, podemos utilizar el método de Gram-Schmidt para obtener la respectiva base ortonormal partiendo desde dicho subespacio. En general, la base ortonormal de autovectores de $\\mathbb{R}^{n}$ (que sabemos que existe, en el caso de que $\\mathbf{A}$ sea simétrica, en el caso del corolario (3.3)) puede obtenerse mediante la aplicación del método de Gram-Schmidt en cada autoespacio. ◼︎"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c09ea3",
   "metadata": {},
   "source": [
    "## Formas.\n",
    "\n",
    "### Formas lineales.\n",
    "La diagonalización de matrices (y de operadores lineales) nos ha permitido formular (y encontrar una forma de resolver) un problema que es conocido en álgebra lineal como descomposición matricial. El problema de descomponer una matriz pasa, en general, por rescribir una matriz $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$ como una multiplicación sucesiva de otras matrices que nos transmiten información valiosa respecto de la matriz original. Lo que vimos en la subsección anterior es un tipo de descomposición basada en matrices diagonales –llamada descomposición propia o autodescomposición– y que es del tipo $\\mathbf{A} =\\mathbf{P} \\mathbf{D} \\mathbf{P}^{-1}$. En adelante, veremos otras descomposiciones interesantes que nos permitirán construir la base de, como cabría esperar, varios tipos de desarrollos propios del campo del aprendizaje automático. En particular, estas descomposiciones nos permitirán expresar objetos geométricos importantes de $\\mathbb{R}^{2}$ y $\\mathbb{R}^{3}$ mediante técnicas de álgebra lineal.\n",
    "\n",
    "Consideremos entonces, una vez más, el problema elemental del álgebra lineal: Dado un $\\mathbb{K}$-espacio vectorial normado $V$ y una base $\\alpha =\\left\\{ v_{1},...,v_{n}\\right\\}$ de $V$, sabemos que todo vector $v\\in V$ puede escribirse como una única combinación lineal de los elementos de $\\alpha$. Es decir,\n",
    "\n",
    "$$v=\\sum^{n}_{k=1} a_{k}v_{k}\\  ;\\  a_{k}\\in \\mathbb{K} \\Longleftrightarrow \\left[ v\\right]_{\\alpha }  =\\left( \\begin{matrix}a_{1}\\\\ \\vdots \\\\ a_{n}\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.60)$</p>\n",
    "\n",
    "Queremos determinar las coordenadas del vector $v$ de forma tal que dicha especificación sea independiente del producto interno que caracteriza a $V$. Para ello, observemos que cada elemento de la base $\\alpha$ puede escribirse como\n",
    "\n",
    "$$\\left[ v_{1}\\right]_{\\alpha }  =\\left( \\begin{matrix}1\\\\ 0\\\\ \\vdots \\\\ 0\\end{matrix} \\right)  ;\\left[ v_{2}\\right]_{\\alpha }  =\\left( \\begin{matrix}0\\\\ 1\\\\ \\vdots \\\\ 0\\end{matrix} \\right)  ;\\  \\cdots \\  ;\\left[ v_{n}\\right]_{\\alpha }  =\\left( \\begin{matrix}0\\\\ 0\\\\ \\vdots \\\\ 1\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.61)$</p>\n",
    "\n",
    "Por lo tanto, podemos imaginar la expresión anterior como unn especie de *lector de coordenadas* para los elementos de la base $\\alpha$, de tal forma que éste lee un 1 en la posición $k$, y un 0 en otra posición. Por otro lado, en el caso general, un vector $v\\in V$ se expresa, en la base $\\alpha$, como\n",
    "\n",
    "$$\\left[ v\\right]_{\\alpha }  =a_{1}\\left( \\begin{matrix}1\\\\ 0\\\\ \\vdots \\\\ 0\\end{matrix} \\right)  +a_{2}\\left( \\begin{matrix}0\\\\ 1\\\\ \\vdots \\\\ 0\\end{matrix} \\right)  +\\  \\cdots \\  +a_{n}\\left( \\begin{matrix}0\\\\ 0\\\\ \\vdots \\\\ 1\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.62)$</p>\n",
    "\n",
    "De lo anterior, observamos que cada $v\\in V$ necesita un total de $n$ lectores, pues dicho lector debe ser, como mínimo, lineal y entregar como valor un escalar. Tiene sentido entonces la siguiente definición.\n",
    "\n",
    "**<font color='blue'>Definición 3.12 – $\\alpha$-lector:</font>** Sea $V$ un $\\mathbb{K}$-espacio vectorial y $\\alpha =\\left\\{ v_{1},...,v_{n}\\right\\}$ una base de $V$. Llamaremos $\\alpha$-lector al conjunto $\\alpha^{\\ast } =\\left\\{ v^{\\ast }_{1},...,v^{\\ast }_{n}\\right\\}$, donde, para cada $k=1,...,n$, se tiene que\n",
    "\n",
    "$$\\begin{array}{lll}v^{\\ast }_{k}&:&V\\longmapsto \\mathbb{K} \\\\ &&v\\longmapsto a_{k}\\end{array} \\Longleftrightarrow \\left[ v\\right]_{\\alpha }  =\\left( \\begin{matrix}a_{1}\\\\ a_{2}\\\\ \\vdots \\\\ a_{n}\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.63)$</p>\n",
    "\n",
    "En particular, un $\\alpha$-lector tiene la siguiente propiedad\n",
    "\n",
    "$$v^{\\ast }_{i}\\left( v_{j}\\right)  =\\begin{cases}1&;\\  \\mathrm{si} \\  i=j\\\\ 0&;\\  \\mathrm{si} \\  i\\neq j\\end{cases}$$\n",
    "<p style=\"text-align: right;\">$(3.64)$</p>\n",
    "\n",
    "Observamos que, para cada $k=1,...,n$, $v_{k}^{\\ast}$ es una transformación lineal del espacio vectorial $V$ en su cuerpo de escalares $\\mathbb{K}$. En símbolos, para cada $k=1,...,n$, $v_{k}^{\\ast}\\in \\mathbb{L}_{\\mathbb{K}}(V,\\mathbb{K})$. En efecto, si $v=\\sum^{n}_{k=1} a_{k}v_{k}\\wedge u=\\sum^{n}_{k=1} b_{k}v_{k}$, entonces se tiene que\n",
    "\n",
    "$$v^{\\ast }_{s}\\left( u+v\\right)  =v^{\\ast }_{s}\\left[ \\sum^{n}_{k=1} \\left( a_{i}+b_{i}\\right)  v_{i}\\right]  =a_{s}+b_{s}=v^{\\ast }_{s}\\left( v\\right)  +v^{\\ast }_{s}\\left( u\\right)$$\n",
    "<p style=\"text-align: right;\">$(3.65)$</p>\n",
    "\n",
    "Análogamente, si $\\lambda \\in \\mathbb{K}$, entonces\n",
    "\n",
    "$$v^{\\ast }_{s}\\left( \\lambda v\\right)  =v^{\\ast }_{s}\\left[\\displaystyle  \\sum^{n}_{k=1} \\left( \\lambda a_{i}\\right)  v_{i}\\right]  =\\lambda a_{s}=\\lambda v^{\\ast }_{s}\\left( v\\right)$$\n",
    "<p style=\"text-align: right;\">$(3.66)$</p>\n",
    "\n",
    "**<font color='crimson'>Teorema 3.11:</font>** $\\alpha^{\\ast }$ *es una base del espacio vectorial $\\mathbb{L}_{\\mathbb{K}}(V,\\mathbb{K})$.* ◆\n",
    "\n",
    "En efecto, primero probaremos que $\\alpha^{\\ast}$ es un sistema de generadores para $\\mathbb{L}_{\\mathbb{K}}(V,\\mathbb{K})$. Sea $T\\in \\mathbb{L}_{\\mathbb{K}}(V,\\mathbb{K})$, entonces\n",
    "\n",
    "$$\\begin{array}{rcl}v=\\sum^{n}_{k=1} a_{k}v_{k}&\\Longrightarrow &T\\left( v\\right)  =\\displaystyle \\sum^{n}_{k=1} a_{k}T\\left( v_{k}\\right)  \\  ;\\  \\left( T\\  \\mathrm{es} \\  \\mathrm{lineal} \\right)  \\\\ a_{k}=v^{\\ast }_{k}\\left( v\\right)  &\\Longrightarrow &T\\left( v\\right)  =\\displaystyle \\sum^{n}_{k=1} v^{\\ast }_{k}\\left( v\\right)  T\\left( v_{k}\\right)  \\  ;\\  \\left( \\mathrm{definicion} \\  \\left( 3.12\\right)  \\right)  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.67)$</p>\n",
    "\n",
    "Así que,\n",
    "\n",
    "$$T\\left( v\\right)  =\\sum^{n}_{k=1} T\\left( v_{k}\\right)  v^{\\ast }_{k}\\left( v\\right)  =\\left[ \\sum^{n}_{k=1} T\\left( v_{k}\\right)  v^{\\ast }_{k}\\right]  \\left( v\\right)$$\n",
    "<p style=\"text-align: right;\">$(3.68)$</p>\n",
    "\n",
    "Aplicando la definición de igualdad de funciones, tenemos que\n",
    "\n",
    "$$T=\\sum^{n}_{k=1} \\underbrace{T\\left( v_{k}\\right)  }_{\\in \\mathbb{K} } v^{\\ast }_{k}$$\n",
    "<p style=\"text-align: right;\">$(3.69)$</p>\n",
    "\n",
    "O equivalentemente,\n",
    "\n",
    "$$T\\in \\left< \\left\\{ v^{\\ast }_{1},v^{\\ast }_{2},...,v^{\\ast }_{n}\\right\\}  \\right>$$\n",
    "<p style=\"text-align: right;\">$(3.70)$</p>\n",
    "\n",
    "Ahora debemos probar que $\\alpha^{\\ast}$ es un conjunto linealmente independiente en $\\mathbb{L}_{\\mathbb{K}}(V,\\mathbb{K})$. En efecto, si suponemos que $\\sum^{n}_{k=1} r_{k}v^{\\ast }_{k}=0$, entonces debemos verificar que, para cada $v\\in V$, la función $v_{k}^{\\ast}$ será nula en $V$. O, en otras palabras, $\\ker \\left( \\sum^{n}_{k=1} r_{k}v^{\\ast }_{k}\\right)  =V$. Observemos que, si esta función se anula en todo el espacio vectorial $V$, en particular, también anula a los elementos $v_{1},...,v_{n}$ de la base $\\alpha$. Así que, para cada $s=1,...,n$, por la definición (3.12), tendremos que\n",
    "\n",
    "$$0=\\sum^{n}_{k=1} r_{k}v^{\\ast }_{k}\\left( v_{s}\\right)  =r_{s}$$\n",
    "<p style=\"text-align: right;\">$(3.71)$</p>\n",
    "\n",
    "En conclusión, $\\alpha^{\\ast}$ es una base del espacio vectorial $\\mathbb{L}_{\\mathbb{K}}(V,\\mathbb{K})$.\n",
    "\n",
    "**<font color='blue'>Definición 3.13 – Espacio dual y base dual:</font>** Sea $V$ un $\\mathbb{K}$-espacio vectorial y $\\alpha =\\left\\{ v_{1},...,v_{n}\\right\\}$ una base de $V$. Diremos que $V^{\\ast}=\\mathbb{L}_{\\mathbb{K}}(V,\\mathbb{K})$ se llamará **espacio dual** de $V$. La base $\\alpha^{\\ast } =\\left\\{ v^{\\ast }_{1},...,v^{\\ast }_{n}\\right\\}$, por consiguiente, se llamará **base dual** de la base $\\alpha$.\n",
    "\n",
    "En conclusión, si $V$ es un $\\mathbb{K}$-espacio vectorial, y $\\alpha =\\left\\{ v_{1},...,v_{n}\\right\\}$ es una base de $V$, entonces\n",
    "\n",
    "$$v=\\sum^{n}_{k=1} v^{\\ast }_{k}\\left( v\\right)  v_{k}\\Longleftrightarrow \\left[ v\\right]_{\\alpha }  =\\left( \\begin{matrix}v^{\\ast }_{1}\\left( v\\right)  \\\\ v^{\\ast }_{2}\\left( v\\right)  \\\\ \\vdots \\\\ v^{\\ast }_{n}\\left( v\\right)  \\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.72)$</p>\n",
    "\n",
    "**Ejemplo 3.12:** Dados tres números reales distintos $r_{1},r_{2}$ y $r_{3}$, podemos definir tres funciones como sigue\n",
    "\n",
    "$$\\begin{array}{ll}T_{i}:&\\mathbb{R}_{2} \\left[ x\\right]  \\longmapsto \\mathbb{R} \\\\ &p\\left( x\\right)  \\longmapsto p\\left( r_{i}\\right)  \\end{array} \\  ;\\  i=1,2,3$$\n",
    "<p style=\"text-align: right;\">$(3.73)$</p>\n",
    "\n",
    "En primer lugar, vamos a demostrar $T_{i}$ es una transformación lineal entre $\\mathbb{R}_{2}[x]$ y $\\mathbb{R}$ (es decir, $T_{i}\\in \\mathbb{L}_{\\mathbb{R}}(\\mathbb{R}_{2}[x],\\mathbb{R})$ para $i=1,2,3$). En efecto, sean $p(x),q(x)\\in \\mathbb{R}_{2}[x]$ y $\\lambda \\in \\mathbb{R}$. En primer lugar, debemos probar que $T_{i}\\left( p\\left( x\\right)  +q\\left( x\\right)  \\right)  =T_{i}\\left( p\\left( x\\right)  \\right)  +T_{i}\\left( q\\left( x\\right)  \\right)$. De este modo tenemos que\n",
    "\n",
    "$$T_{i}\\left( p\\left( x\\right)  +q\\left( x\\right)  \\right)  =p\\left( r_{i}\\right)  +q\\left( r_{i}\\right)  =T_{i}\\left( p\\left( x\\right)  \\right)  +T_{i}\\left( q\\left( x\\right)  \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.74)$</p>\n",
    "\n",
    "Ahora debemos mostrar que $T_{i}\\left( \\lambda p\\left( x\\right)  \\right)  =\\lambda T_{i}\\left( p\\left( x\\right)  \\right)$. En efecto,\n",
    "\n",
    "$$T_{i}\\left( \\lambda p\\left( x\\right)  \\right)  =\\lambda p\\left( r_{i}\\right)  =\\lambda T_{i}\\left( p\\left( x\\right)  \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.75)$</p>\n",
    "\n",
    "Así que, efectivamente, $T_{i}\\in \\mathbb{L}_{\\mathbb{R}}(\\mathbb{R}_{2}[x],\\mathbb{R})=\\left( \\mathbb{R}_{2} \\left[ x\\right]  \\right)^{\\ast }$ para $i=1,2,3$. Ahora probaremos que $\\alpha^{\\ast } =\\left\\{ T_{1},T_{2},T_{3}\\right\\}$ es un conjunto linealmente independiente en $\\left( \\mathbb{R}_{2} \\left[ x\\right]  \\right)^{\\ast }$. De esta manera,\n",
    "\n",
    "$$a_{1}T_{1}+a_{2}T_{2}+a_{3}T_{3}=0\\Longleftrightarrow \\left( a_{1}T_{1}+a_{2}T_{2}+a_{3}T_{3}\\right)  \\left( p\\left( x\\right)  \\right)  =0\\  ;\\  \\forall p\\left( x\\right)  \\in \\mathbb{R}_{2} \\left[ x\\right]$$\n",
    "<p style=\"text-align: right;\">$(3.76)$</p>\n",
    "\n",
    "En particular, tenemos que, usando la base canónica de $\\mathbb{R}_{2}[x]$,\n",
    "\n",
    "$$\\begin{array}{rcl}\\left( a_{1}T_{1}+a_{2}T_{2}+a_{3}T_{3}\\right)  \\left( 1\\right)  &=&0\\\\ \\left( a_{1}T_{1}+a_{2}T_{2}+a_{3}T_{3}\\right)  \\left( x\\right)  &=&0\\\\ \\left( a_{1}T_{1}+a_{2}T_{2}+a_{3}T_{3}\\right)  \\left( x^{2}\\right)  &=&0\\end{array} \\  \\Longrightarrow \\  \\begin{array}{rcl}a_{1}+a_{2}+a_{3}&=&0\\\\ a_{1}r_{1}+a_{2}r_{2}+a_{3}r_{3}&=&0\\\\ a_{1}r^{2}_{1}+a_{2}r^{2}_{2}+a_{3}r^{2}_{3}&=&0\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.77)$</p>\n",
    "\n",
    "Si escribimos este sistema en forma matricial, obtenemos\n",
    "\n",
    "$$\\underbrace{\\left( \\begin{matrix}1&1&1\\\\ r_{1}&r_{2}&r_{3}\\\\ r^{2}_{1}&r^{2}_{2}&r^{2}_{3}\\end{matrix} \\right)  }_{=\\mathbf{A} } \\left( \\begin{matrix}a_{1}\\\\ a_{2}\\\\ a_{3}\\end{matrix} \\right)  =\\left( \\begin{matrix}0\\\\ 0\\\\ 0\\end{matrix} \\right)  \\  \\Longrightarrow \\  \\det \\left( \\mathbf{A} \\right)  =\\left( r_{1}-r_{2}\\right)  \\left( r_{2}-r_{3}\\right)  \\left( r_{3}-r_{1}\\right)$$\n",
    "<p style=\"text-align: right;\">$(3.78)$</p>\n",
    "\n",
    "Sabemos que los números $r_{1},r_{2},r_{3}$ son distintos. Por lo tanto, es claro que $\\det(\\mathbf{A})\\neq 0$ y la solución del sistema (3.78) es trivial; es decir, $a_{1}=a_{2}=a_{3}=0$. Así que, efectivamente, $\\alpha^{\\ast } =\\left\\{ T_{1},T_{2},T_{3}\\right\\}$ es un conjunto linealmente independiente en $\\left( \\mathbb{R}_{2} \\left[ x\\right]  \\right)^{\\ast }$.\n",
    "\n",
    "Probaremos ahora que $\\alpha^{\\ast } =\\left\\{ T_{1},T_{2},T_{3}\\right\\}$ es una base de $\\left( \\mathbb{R}_{2} \\left[ x\\right]  \\right)^{\\ast }$. En efecto, tenemos que $\\dim\\limits_{\\mathbb{R} } \\left( \\mathbb{R}_{2} \\left[ x\\right]  \\right)^{\\ast }  =\\dim\\limits_{\\mathbb{R} } \\left( \\mathbb{R}_{2} \\left[ x\\right]  \\right)  =3$, así que, efectivamente, $\\alpha^{\\ast }$ es una base de $\\left( \\mathbb{R}_{2} \\left[ x\\right]  \\right)^{\\ast }$.\n",
    "\n",
    "Finalmente, determinaremos la correspondiente base $\\alpha$ de $\\mathbb{R}_{2}[x]$. Supongamos entonces que $\\alpha =\\left\\{ p_{1}\\left( x\\right)  ,p_{2}\\left( x\\right)  ,p_{3}\\left( x\\right)  \\right\\}$ es una base, donde\n",
    "\n",
    "$$p_{1}\\left( x\\right)  =c_{01}+c_{11}x+c_{21}x^{2}\\  ;\\  p_{2}\\left( x\\right)  =c_{02}+c_{12}x+c_{22}x^{2}\\  ;\\  p_{3}\\left( x\\right)  =c_{13}+c_{23}x+c_{33}x^{2}$$\n",
    "<p style=\"text-align: right;\">$(3.79)$</p>\n",
    "\n",
    "Entonces, usando la definición (3.13), debemos tener, para $T_{1}$\n",
    "\n",
    "$$\\begin{array}{lll}T_{1}\\left( p_{1}\\left( x\\right)  \\right)  &=&1\\\\ T_{2}\\left( p_{2}\\left( x\\right)  \\right)  &=&0\\\\ T_{3}\\left( p_{3}\\left( x\\right)  \\right)  &=&0\\end{array} \\  \\Longrightarrow \\  \\begin{array}{lll}c_{01}+c_{11}r_{1}+c_{21}r^{2}_{1}&=&1\\\\ c_{02}+c_{12}r_{1}+c_{22}r^{2}_{1}&=&0\\\\ c_{03}+c_{13}r_{1}+c_{23}r^{2}_{1}&=&0\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.80)$</p>\n",
    "\n",
    "Equivalentemente, para $T_{2}$ y $T_{3}$,\n",
    "\n",
    "$$\\begin{array}{lll}T_{1}\\left( p_{1}\\left( x\\right)  \\right)  &=&0\\\\ T_{2}\\left( p_{2}\\left( x\\right)  \\right)  &=&1\\\\ T_{3}\\left( p_{3}\\left( x\\right)  \\right)  &=&0\\end{array} \\  \\Longrightarrow \\  \\begin{array}{lll}c_{01}+c_{11}r_{2}+c_{21}r^{2}_{2}&=&0\\\\ c_{02}+c_{12}r_{2}+c_{22}r^{2}_{2}&=&1\\\\ c_{03}+c_{13}r_{2}+c_{23}r^{2}_{2}&=&0\\end{array} \\  \\wedge \\  \\begin{array}{lll}T_{1}\\left( p_{1}\\left( x\\right)  \\right)  &=&0\\\\ T_{2}\\left( p_{2}\\left( x\\right)  \\right)  &=&0\\\\ T_{3}\\left( p_{3}\\left( x\\right)  \\right)  &=&1\\end{array} \\  \\Longrightarrow \\  \\begin{array}{lll}c_{01}+c_{11}r_{3}+c_{21}r^{2}_{3}&=&0\\\\ c_{02}+c_{12}r_{3}+c_{22}r^{2}_{3}&=&0\\\\ c_{03}+c_{13}r_{3}+c_{23}r^{2}_{3}&=&1\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.81)$</p>\n",
    "\n",
    "Para el polinomio $p_{1}(x)$ tenemos que\n",
    "\n",
    "$$\\begin{array}{lll}c_{01}+c_{11}r_{1}+c_{21}r^{2}_{1}&=&1\\\\ c_{01}+c_{11}r_{2}+c_{21}r^{2}_{2}&=&0\\\\ c_{01}+c_{11}r_{3}+c_{21}r^{2}_{3}&=&0\\end{array} \\  \\Longrightarrow \\  \\begin{array}{lll}c_{11}\\left( r_{1}-r_{2}\\right)  +c_{21}\\left( r^{2}_{1}-r^{2}_{2}\\right)  &=&1\\\\ c_{11}\\left( r_{2}-r_{3}\\right)  +c_{21}\\left( r^{2}_{2}-r^{2}_{3}\\right)  &=&0\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.82)$</p>\n",
    "\n",
    "Entonces,\n",
    "\n",
    "$$\\begin{array}{lll}c_{11}+c_{21}\\left( r_{1}+r_{2}\\right)  &=&\\displaystyle \\frac{1}{r_{1}-r_{2}} \\\\ c_{11}+c_{21}\\left( r_{2}+r_{3}\\right)  &=&0\\end{array} \\  \\Longrightarrow \\  c_{21}=\\frac{1}{\\left( r_{1}-r_{2}\\right)  \\left( r_{2}-r_{3}\\right)  }$$\n",
    "<p style=\"text-align: right;\">$(3.83)$</p>\n",
    "\n",
    "Así que,\n",
    "\n",
    "$$p_{1}\\left( x\\right)  =\\frac{\\left( x-r_{2}\\right)  \\left( x-r_{3}\\right)  }{\\left( r_{1}-r_{2}\\right)  \\left( r_{2}-r_{3}\\right)  }$$\n",
    "<p style=\"text-align: right;\">$(3.84)$</p>\n",
    "\n",
    "De manera análoga, podemos mostrar que\n",
    "\n",
    "$$p_{2}\\left( x\\right)  =\\frac{\\left( x-r_{1}\\right)  \\left( x-r_{3}\\right)  }{\\left( r_{2}-r_{1}\\right)  \\left( r_{2}-r_{3}\\right)  } \\wedge p_{3}\\left( x\\right)  =\\frac{\\left( x-r_{1}\\right)  \\left( x-r_{2}\\right)  }{\\left( r_{3}-r_{1}\\right)  \\left( r_{3}-r_{2}\\right)  }$$\n",
    "<p style=\"text-align: right;\">$(3.85)$</p>\n",
    "◼︎"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63037d91",
   "metadata": {},
   "source": [
    "### Formas bilineales.\n",
    "Sea $V$ un $\\mathbb{R}$-espacio vectorial y supongamos que $V$ está equipado con un producto interno. Sabemos que\n",
    "\n",
    "- El producto interno es una función $\\left( u,v\\right)  \\in V\\times V\\longmapsto \\left< u,v\\right>  \\in \\mathbb{R}$.\n",
    "- Para cada $v\\in V$, la función $\\left<, v\\right>\\in V^{\\ast}$ (el espacio dual de $V$), definimos $u\\in V\\longmapsto \\left<u,v\\right>\\in \\mathbb{R}$. Del mismo modo, para cada $u\\in V$, la función $\\left<u,\\right>\\in V^{\\ast}$, definimos $v\\in V\\longmapsto \\left<u,v\\right>\\in \\mathbb{R}$.\n",
    "- Supongamos que $\\alpha =\\left\\{ v_{1},...,v_{n}\\right\\}$ es una base de $V$. Entonces podemos utilizar la propiedad lineal de ambos vectores (coordenadas) $u$ y $v$ como sigue: Para $u=\\sum_{i=1}^{n} a_{i}v_{i}$ y $v=\\sum_{i=1}^{n} b_{i}v_{i}$, se tiene que\n",
    "\n",
    "$$\\left< u,v\\right>  =\\left< \\sum^{n}_{i=1} a_{i}v_{i},\\sum^{n}_{i=1} b_{i}v_{i}\\right>  =\\sum^{n}_{i=1} \\sum^{n}_{j=1} a_{i}b_{i}\\left< v_{i},v_{j}\\right>$$\n",
    "<p style=\"text-align: right;\">$(3.86)$</p>\n",
    "\n",
    "Equivalentemente, si interpretamos $\\left( \\left< v_{i},v_{j}\\right>  \\right)  \\in \\mathbb{R}^{n\\times n}$, entonces\n",
    "\n",
    "$$\\begin{array}{lll}\\left< u,v\\right>  &=&\\left( \\begin{matrix}a_{1}&\\cdots &a_{n}\\end{matrix} \\right)  \\left( \\left< v_{i},v_{j}\\right>  \\right)  \\left( \\begin{matrix}b_{1}\\\\ \\vdots \\\\ b_{n}\\end{matrix} \\right)  \\\\ &=&\\left[ u\\right]^{\\top }_{\\alpha }  \\underbrace{\\left( \\left< v_{i},v_{j}\\right>  \\right)  }_{\\left[ \\left< \\  ,\\  \\right>  \\right]^{\\alpha }_{\\alpha }  } \\left[ v\\right]_{\\alpha }  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.87)$</p>\n",
    "\n",
    "En particular,\n",
    "\n",
    "- **(C1):** Si $\\alpha$ es una base ortonomal, entonces $\\left< u,v\\right>  =\\left( \\begin{matrix}a_{1}&\\cdots &a_{n}\\end{matrix} \\right)  \\left( \\begin{matrix}b_{1}\\\\ \\vdots \\\\ b_{n}\\end{matrix} \\right)  =\\left[ u\\right]^{\\top }_{\\alpha }  \\left[ v\\right]_{\\alpha }  =\\sum^{n}_{i=1} a_{i}b_{i}$.\n",
    "- **(C2):** Si $u=v$ y $\\alpha$ es una base ortonormal, entonces $\\left< u,u\\right>  =\\sum^{n}_{i=1} a^{2}_{i}=\\left\\Vert u\\right\\Vert^{2}$.\n",
    "- **(C3):** En general, si $\\left[ \\left< \\  ,\\  \\right>  \\right]^{\\alpha }_{\\alpha }$ es diagonalizable y $\\beta$ es la base ortonormal de autovectores de $\\left< \\  ,\\  \\right>$, entonces se tiene que\n",
    "\n",
    "$$\\begin{array}{lll}\\left< u,v\\right>  &=&\\left[ u\\right]^{\\top }_{\\alpha }  \\left[ \\mathbf{I} \\right]^{\\beta }_{\\alpha }  \\underbrace{\\left( \\left< v_{i},v_{j}\\right>  \\right)  }_{=\\left[ \\left< \\  ,\\  \\right>  \\right]^{\\beta }_{\\beta }  } \\left[ \\mathbf{I} \\right]^{\\beta }_{\\alpha }  \\left[ v\\right]_{\\alpha }  \\\\ &=&\\left[ u\\right]^{\\top }_{\\alpha }  \\left( \\left[ \\mathbf{I} \\right]^{\\beta }_{\\alpha }  \\right)^{\\top }  \\underbrace{\\left( \\left< v_{i},v_{j}\\right>  \\right)  }_{=\\left[ \\left< \\  ,\\  \\right>  \\right]^{\\beta }_{\\beta }  } \\left[ \\mathbf{I} \\right]^{\\beta }_{\\alpha }  \\left[ v\\right]_{\\alpha }  \\\\ &=&\\left( \\left[ \\mathbf{I} \\right]^{\\beta }_{\\alpha }  \\left[ u\\right]_{\\alpha }  \\right)^{\\top }  \\underbrace{\\left( \\left< v_{i},v_{j}\\right>  \\right)  }_{=\\left[ \\left< \\  ,\\  \\right>  \\right]^{\\beta }_{\\beta }  } \\left[ \\mathbf{I} \\right]^{\\beta }_{\\alpha }  \\left[ v\\right]_{\\alpha }  \\\\ &=&\\left[ u\\right]^{\\top }_{\\beta }  \\underbrace{\\left( \\left< v_{i},v_{j}\\right>  \\right)  }_{=\\left[ \\left< \\  ,\\  \\right>  \\right]^{\\beta }_{\\beta }  } \\left[ v\\right]_{\\beta }  \\\\ &=&\\left[ u\\right]^{\\top }_{\\beta }  \\mathrm{diag} \\left\\{ \\lambda_{1} ,...,\\lambda_{n} \\right\\}  \\left[ v\\right]_{\\beta }  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.88)$</p>\n",
    "\n",
    "Donde $\\mathrm{diag} \\left\\{ \\lambda_{1} ,...,\\lambda_{n} \\right\\}$ es la matriz diagonal que tiene como elementos no nulos a los autovalores de $\\left< \\  ,\\  \\right>$.\n",
    "\n",
    "- **(C4):** En particular, si $\\beta =\\left\\{ w_{1},...,w_{n}\\right\\}$, entonces\n",
    "\n",
    "$$\\left< u,u\\right>  =\\sum^{n}_{i=1} \\lambda_{i} \\left< u,w_{i}\\right>^{2}$$\n",
    "<p style=\"text-align: right;\">$(3.89)$</p>\n",
    "\n",
    "Tiene sentido, después de este arduo trabajo, la siguiente definición.\n",
    "\n",
    "**<font color='blue'>Definición 3.14 – Forma bilineal:</font>** Sea $V$ un $\\mathbb{K}$-espacio vectorial y $B:V\\times V\\longrightarrow \\mathbb{K}$ una función tal que $B(u,v)\\in \\mathbb{K}$ para cada par de vectores $(u,v)\\in V\\times V$. Diremos que $B$ es una **forma bilineal** si $B$ es lineal en cada coordenada. Es decir, $B$ es lineal para cada $v\\in V, B_{v}\\in V^{\\ast}$, donde $B_{v}(u)=B(u,v)$, y para cada $u\\in V, B_{u}\\in V^{\\ast}$, donde $B_{u}(v)=B(v,u)$.\n",
    "\n",
    "Antes de, como siempre, entrar en ejemplos más prácticos, explicitaremos los beneficios teóricos que se desprenden de esta condición de bilinearidad. En este caso, si $\\alpha =\\left\\{ v_{1},...,v_{n}\\right\\}$ es una base de vectores de $V$, entonces para $u=\\sum^{n}_{i=1} a_{i}v_{i}\\wedge v=\\sum^{n}_{i=1} b_{i}v_{i}$, se tiene que\n",
    "\n",
    "$$\\begin{array}{lll}B\\left( u,v\\right)  &=&B\\left( \\sum^{n}_{i=1} a_{i}v_{i},\\sum^{n}_{j=1} b_{j}v_{j}\\right)  \\\\ &=&B\\left( \\sum^{n}_{i=1} \\sum^{n}_{j=1} a_{i}b_{j}B\\left( v_{i},v_{j}\\right)  \\right)  \\\\ &=&\\left( \\begin{matrix}a_{1}&\\cdots &a_{n}\\end{matrix} \\right)  B\\left( v_{i},v_{j}\\right)  \\left( \\begin{matrix}b_{1}\\\\ \\vdots \\\\ b_{n}\\end{matrix} \\right)  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.90)$</p>\n",
    "\n",
    "Es decir, hemos encontrado una igualdad importante (de hecho, fundamental):\n",
    "\n",
    "$$B\\left( u,v\\right)  =\\left[ u\\right]^{\\top }_{\\alpha }  \\underbrace{B\\left( v_{i},v_{j}\\right)  }_{\\left[ B\\right]^{\\alpha }_{\\alpha }  } \\left[ v\\right]_{\\alpha }$$\n",
    "<p style=\"text-align: right;\">$(3.91)$</p>\n",
    "\n",
    "Lo anterior nos lleva al siguiente resultado.\n",
    "\n",
    "**<font color='crimson'>Teorema 3.12:</font>** *Sea $V$ un $\\mathbb{K}$-espacio vectorial y designemos por $B(V)$ al conjunto de todas las formas bilineales que existen en $V$. Si $\\dim_{\\mathbb{K}}(V)=n$, entonces $B\\left( V\\right)  \\cong \\mathbb{K}^{n\\times n}$. Es decir, estos conjuntos son isomorfos.* ◆\n",
    "\n",
    "**Ejemplo 3.13:** Sea $\\mathbf{A} =\\left( \\begin{matrix}1&2\\\\ 3&4\\end{matrix} \\right)$. Entonces definimos\n",
    "\n",
    "$$\\begin{array}{lll}B\\left( \\left( x_{1},y_{1}\\right)  ,\\left( x_{2},y_{2}\\right)  \\right)  &=&\\left( x_{1},y_{1}\\right)  \\left( \\begin{matrix}1&2\\\\ 3&4\\end{matrix} \\right)  \\left( \\begin{matrix}x_{2}\\\\ y_{2}\\end{matrix} \\right)  \\\\ &=&\\left( x_{1}+3y_{1},2x_{1}+y_{1}\\right)  \\left( \\begin{matrix}x_{2}\\\\ y_{2}\\end{matrix} \\right)  \\\\ &=&x_{1}x_{2}+3x_{2}y_{1}+2x_{1}y_{2}+4y_{1}y_{2}\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.92)$</p>\n",
    "\n",
    "La función $B\\left( \\mathbf{x}_{1} ,\\mathbf{x}_{2} \\right)  =B\\left( \\left( x_{1},y_{1}\\right)  ,\\left( x_{2},y_{2}\\right)  \\right)  =x_{1}x_{2}+3x_{2}y_{1}+2x_{1}y_{2}+4y_{1}y_{2}$ construida de esta manera es, evidentemente, una forma bilineal. ◼︎\n",
    "\n",
    "**<font color='blue'>Definición 3.15 – Matriz de una forma bilineal:</font>** Sea $V$ un $\\mathbb{K}$-espacio vectorial y $\\alpha =\\left\\{ v_{1},...,v_{n}\\right\\}$ una base de vectores de $V$. Sea $B:V\\times V\\longrightarrow \\mathbb{K}$ una forma bilineal sobre $V$. Entonces $\\left[ B\\right]^{\\alpha }_{\\alpha }  =\\left( B\\left( v_{i},v_{j}\\right)  \\right)$ será llamada **matriz de la forma bilineal** $B$ respecto de la base $\\alpha$.\n",
    "\n",
    "Si, para alguna base $\\alpha$, se tiene que $B(v_{i},v_{j})=B(v_{j},v_{i})$ para $i,j=1,...,n$, entonces $[B]_{\\alpha}^{\\alpha}$ es una matriz simétrica y, recíprocamente, si $[B]_{\\alpha}^{\\alpha}$ es simétrica para alguna base $\\alpha$, entonces $B(v_{i},v_{j})=B(v_{j},v_{i})$ para $u,v\\in V$. En tal caso, diremos que $B$ es una **forma bilineal simétrica**.\n",
    "\n",
    "**Ejemplo 3.14:** Sea $\\mathbf{A} =\\left( \\begin{matrix}1&2\\\\ 2&5\\end{matrix} \\right)$. Entonces, en la base canónica de $\\mathbb{R}^{2}$, se tiene que\n",
    "\n",
    "$$\\begin{array}{lll}B_{\\mathbf{A} }\\left( \\left( x_{1},y_{1}\\right)  ,\\left( x_{2},y_{2}\\right)  \\right)  &=&\\left( x_{1},y_{1}\\right)  \\left( \\begin{matrix}1&2\\\\ 2&5\\end{matrix} \\right)  \\left( \\begin{matrix}x_{2}\\\\ y_{2}\\end{matrix} \\right)  \\\\ &=&\\left[ \\left( x_{1},y_{1}\\right)  \\right]^{\\top }_{\\mathbf{e} \\left( 2\\right)  }  \\left[ \\mathbf{A} \\right]^{\\mathbf{e} \\left( 2\\right)  }_{\\mathbf{e} \\left( 2\\right)  }  \\left[ \\left( x_{2},y_{2}\\right)  \\right]_{\\mathbf{e} \\left( 2\\right)  }  \\\\ &=&\\left( x_{1}+2y_{1},2x_{1}+5y_{1}\\right)  \\left( \\begin{matrix}x_{2}\\\\ y_{2}\\end{matrix} \\right)  \\\\ &=&x_{1}x_{2}+2x_{2}y_{1}+2x_{1}y_{2}+5y_{1}y_{2}\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.93)$</p>\n",
    "\n",
    "En particular, si $(x_{1},y_{1})=(x_{2},y_{2})=(x,y)$, se tendrá que\n",
    "\n",
    "$$B_{\\mathbf{A} }\\left( \\left( x,y\\right)  ,\\left( x,y\\right)  \\right)  =x^{2}+4xy+5y^{2}$$\n",
    "<p style=\"text-align: right;\">$(3.94)$</p>\n",
    "◼︎"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c73e40",
   "metadata": {},
   "source": [
    "### Formas cuadráticas.\n",
    "Procedemos ahora a estudiar, por tanto, otro tipo de descomposición. Puntualmente, estudiaremos las **formas cuadráticas** inducidas por una matriz simétrica. Las formas cuadráticas son la base de varios problemas de optimización (que tienen como objetivo encontrar máximos o mínimos relativos de una función definida en algún subespacio determinado). En particular, queremos caracterizar un tipo especial de matrices, llamadas **matrices definidas positivas**, y mostraremos una aplicación sencilla de las formas cuadráticas en la descripción (y transformación) de ciertos dominios de $\\mathbb{R}^{2}$ conocidos como secciones cónicas, y que resultan de la intersección de un cilindro circular recto con un plano con cierta orientación.\n",
    "\n",
    "**<font color='blue'>Definición 3.16 – Forma cuadrática:</font>** Dada la matriz simétrica $\\mathbf{A}\\in \\mathbb{R}^{n\\times n}$, definimos la aplicación\n",
    "\n",
    "$$\\begin{array}{ll}q:&\\mathbb{R}^{n} \\longrightarrow \\mathbb{R} \\\\ &\\mathbf{x} \\longmapsto q\\left( \\mathbf{x} \\right)  =\\mathbf{x}^{\\top } \\mathbf{A} \\mathbf{x} \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.95)$</p>\n",
    "\n",
    "y que es llamada **forma cuadrática** en $\\mathbb{R}^{n}$.\n",
    "\n",
    "Notemos que una forma cuadrática en $\\mathbb{R}^{n}$ es **homogénea de grado 2**. Es decir, $\\forall \\mathbf{x} \\in \\mathbb{R}^{n} ,q\\left( \\lambda \\mathbf{x} \\right)  =\\lambda^{2} q\\left( \\mathbf{x} \\right)$. En efecto, $q\\left( \\lambda \\mathbf{x} \\right)  =\\left( \\lambda \\mathbf{x} \\right)^{\\top }  \\mathbf{A} \\left( \\lambda \\mathbf{x} \\right)  =\\lambda \\mathbf{x}^{\\top } \\mathbf{A} \\lambda \\mathbf{x} =\\lambda^{2} \\mathbf{x}^{\\top } \\mathbf{A} \\mathbf{x} =\\lambda^{2} q\\left( \\mathbf{x} \\right)$.\n",
    "\n",
    "Lo anterior motiva la siguiente definición.\n",
    "\n",
    "**<font color='blue'>Definición 3.17 – Matriz (semi-)definida positiva (negativa):</font>** Sea $\\mathbf{A}\\in \\mathbb{R}^{n\\times n}$ una matriz simétrica. Diremos que:\n",
    "\n",
    "- **(D1):** $\\mathbf{A}$ es **definida positiva** si, para todo $\\mathbf{x}\\in \\mathbb{R}^{n}$, tal que $\\mathbf{x}\\neq \\mathbf{0}$, se tiene que $\\mathbf{x}^{\\top}\\mathbf{A}\\mathbf{x} > 0$.\n",
    "- **(D2):** $\\mathbf{A}$ es **semi-definida positiva** si, para todo $\\mathbf{x}\\in \\mathbb{R}^{n}$, tal que $\\mathbf{x}\\neq \\mathbf{0}$, se tiene que $\\mathbf{x}^{\\top}\\mathbf{A}\\mathbf{x} \\geq 0$.\n",
    "- **(D3):** $\\mathbf{A}$ es **definida negativa** si, para todo $\\mathbf{x}\\in \\mathbb{R}^{n}$, tal que $\\mathbf{x}\\neq \\mathbf{0}$, se tiene que $\\mathbf{x}^{\\top}\\mathbf{A}\\mathbf{x} < 0$.\n",
    "- **(D4):** $\\mathbf{A}$ es **semi-definida negativa** si, para todo $\\mathbf{x}\\in \\mathbb{R}^{n}$, tal que $\\mathbf{x}\\neq \\mathbf{0}$, se tiene que $\\mathbf{x}^{\\top}\\mathbf{A}\\mathbf{x} \\leq 0$.\n",
    "\n",
    "**<font color='crimson'>Teorema 3.13:</font>** *Sea $V$ un $\\mathbb{R}$-espacio vectorial y $q$ una forma cuadrática sobre $V$. Entonces existe ua base ortonormal $\\alpha$ de $V$ tal que*\n",
    "\n",
    "$$q\\left( u\\right)  =\\sum^{n}_{i=1} \\lambda_{i} u^{2}_{i}\\  ;\\  \\left[ u\\right]_{\\alpha }  =\\left( u_{1},...,u_{n}\\right)^{\\top }$$\n",
    "<p style=\"text-align: right;\">$(3.96)$</p>\n",
    "\n",
    "*para $u\\in V$.* ◆\n",
    "\n",
    "**Ejemplo 3.15:** Si definimos la forma cuadrática $q:\\mathbb{R}^{2}\\longrightarrow \\mathbb{R}$ tal que $q(x,y)=x^{2}-2xy-y^{2}$, vamos a intentar expresar $q$ de la forma $q\\left( x,y\\right)  =a_{1}\\left( b_{1}x+c_{1}y\\right)^{2}  +a_{2}\\left( b_{2}x+c_{2}y\\right)^{2}$ (llamada **forma canónica**).\n",
    "\n",
    "Partimos, rápidamente, expresando $q$ en su forma matricial como sigue\n",
    "\n",
    "$$q\\left( x,y\\right)  =x^{2}-2xy-y^{2}=\\underbrace{\\left[ \\left( x,y\\right)  \\right]^{\\intercal }_{\\mathbf{e} \\left( 2\\right)  }  }_{\\mathbf{x}^{\\intercal } } \\overbrace{\\left[ q\\right]^{\\mathbf{e} \\left( 2\\right)  }_{\\mathbf{e} \\left( 2\\right)  }  }^{\\mathbf{A} } \\underbrace{\\left[ \\left( x,y\\right)  \\right]_{\\mathbf{e} \\left( 2\\right)  }  }_{\\mathbf{x} } =\\left( x,y\\right)  \\left( \\begin{matrix}1&-1\\\\ -1&-1\\end{matrix} \\right)  \\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.97)$</p>\n",
    "\n",
    "Observemos que la ecuación (3.97) se escribe en la base canónica de $\\mathbb{R}^{2}$, la que es ortonormal respecto del producto interno usual. Procedemos ahora a diagonalizar la matriz $\\mathbf{A}=\\left[ q\\right]^{\\mathbf{e} \\left( 2\\right)  }_{\\mathbf{e} \\left( 2\\right)  }$. Para ello, construimos el correspondiente polinomio característico $P_{\\mathbf{A}}(\\lambda)$ como sigue,\n",
    "\n",
    "$$\\begin{array}{lll}P_{\\mathbf{A} }\\left( \\lambda \\right)  &=&\\det \\left( \\begin{matrix}1-\\lambda &-1\\\\ -1&-1-\\lambda \\end{matrix} \\right)  \\\\ &=&-\\left( 1-\\lambda \\right)  \\left( 1-\\lambda \\right)  -1\\\\ &=&\\lambda^{2} -1\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.98)$</p>\n",
    "\n",
    "Así que los autovalores de $\\mathbf{A}$ son $\\lambda_{1}=\\sqrt{2}\\wedge \\lambda_{2}=-\\sqrt{2}$. Ahora determinamos los autoespacios respectivos,\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{v} \\in \\left( \\mathbb{R}^{2\\times 1} \\right)_{\\lambda }  &\\Longleftrightarrow &\\mathbf{v} \\in \\mathbb{R}^{2\\times 1} \\wedge \\mathbf{A} \\mathbf{v} =\\lambda \\mathbf{v} \\\\ &\\Longleftrightarrow &\\mathbf{v} =\\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  \\wedge \\left( \\begin{matrix}1&-1\\\\ -1&-1\\end{matrix} \\right)  \\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  =\\lambda \\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  \\\\ &\\Longleftrightarrow &\\mathbf{v} =\\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  \\wedge \\begin{cases}\\begin{array}{lll}x-y&=&\\lambda x\\\\ -x-y&=&\\lambda y\\end{array} &\\end{cases} \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.99)$</p>\n",
    "\n",
    "Tenemos pues dos casos a estudiar. El primero es $\\lambda_{1}=\\sqrt{2}$, en cuyo caso se tiene que\n",
    "\n",
    "$$\\begin{array}{lll}x-y&=&\\sqrt{2} x\\\\ -x-y&=&\\sqrt{2} y\\end{array} \\  \\Longleftrightarrow \\  y=\\left( 1-\\sqrt{2} \\right)  x$$\n",
    "<p style=\"text-align: right;\">$(3.100)$</p>\n",
    "\n",
    "Luego,\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{v} \\in \\left( \\mathbb{R}^{2\\times 1} \\right)_{\\lambda_{1} =\\sqrt{2} }  &\\Longleftrightarrow &\\mathbf{v} \\in \\mathbb{R}^{2\\times 1} \\wedge \\mathbf{v} =\\left( \\begin{matrix}x\\\\ \\left( 1-\\sqrt{2} \\right)  x\\end{matrix} \\right)  \\\\ &\\Longleftrightarrow &\\mathbf{v} \\in \\mathbb{R}^{2\\times 1} \\wedge \\mathbf{v} =x\\left( \\begin{matrix}1\\\\ 1-\\sqrt{2} \\end{matrix} \\right)  ;x\\in \\mathbb{R} \\\\ &\\Longleftrightarrow &\\left( \\mathbb{R}^{2\\times 1} \\right)_{\\lambda_{1} =\\sqrt{2} }  =\\left< \\left\\{ \\left( \\begin{matrix}1\\\\ 1-\\sqrt{2} \\end{matrix} \\right)  \\right\\}  \\right>  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.101)$</p>\n",
    "\n",
    "Por otro lado, para $\\lambda_{2}=-\\sqrt{2}$, tenemos que\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{v} \\in \\left( \\mathbb{R}^{2\\times 1} \\right)_{\\lambda_{2} =-\\sqrt{2} }  &\\Longleftrightarrow &\\mathbf{v} \\in \\mathbb{R}^{2\\times 1} \\wedge \\mathbf{v} =\\left( \\begin{matrix}x\\\\ \\left( 1+\\sqrt{2} \\right)  x\\end{matrix} \\right)  \\\\ &\\Longleftrightarrow &\\mathbf{v} \\in \\mathbb{R}^{2\\times 1} \\wedge \\mathbf{v} =x\\left( \\begin{matrix}1\\\\ 1+\\sqrt{2} \\end{matrix} \\right)  ;x\\in \\mathbb{R} \\\\ &\\Longleftrightarrow &\\left( \\mathbb{R}^{2\\times 1} \\right)_{\\lambda_{2} =-\\sqrt{2} }  =\\left< \\left\\{ \\left( \\begin{matrix}1\\\\ 1+\\sqrt{2} \\end{matrix} \\right)  \\right\\}  \\right>  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.101)$</p>\n",
    "\n",
    "Construiremos ahora una base ortonormal de autovectores a partir de la base $\\beta$ definida como\n",
    "\n",
    "$$\\beta =\\left\\{ \\left( \\begin{matrix}1\\\\ 1-\\sqrt{2} \\end{matrix} \\right)  ,\\left( \\begin{matrix}1\\\\ 1+\\sqrt{2} \\end{matrix} \\right)  \\right\\}$$\n",
    "<p style=\"text-align: right;\">$(3.102)$</p>\n",
    "\n",
    "Notemos que $\\beta$ ya es ortogonal, así que, para construir la base ortonormal, basta con dividir cada vector por la norma respectiva. De este modo,\n",
    "\n",
    "$$\\alpha =\\left\\{ \\left( \\begin{matrix}\\displaystyle \\frac{1}{\\sqrt{4-2\\sqrt{2} } } \\\\ \\displaystyle \\frac{1-\\sqrt{2} }{\\sqrt{4-2\\sqrt{2} } } \\end{matrix} \\right)  ,\\left( \\begin{matrix}\\displaystyle \\frac{1}{\\sqrt{4+2\\sqrt{2} } } \\\\ \\displaystyle \\frac{1+\\sqrt{2} }{\\sqrt{4+2\\sqrt{2} } } \\end{matrix} \\right)  \\right\\}$$\n",
    "<p style=\"text-align: right;\">$(3.103)$</p>\n",
    "\n",
    "Ahora construiremos $[v]_{\\alpha}$. Por tanto,\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{v} &=&\\left< \\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  ,\\left( \\begin{matrix}\\displaystyle \\frac{1}{\\sqrt{4-2\\sqrt{2} } } \\\\ \\displaystyle \\frac{1-\\sqrt{2} }{\\sqrt{4-2\\sqrt{2} } } \\end{matrix} \\right)  \\right>  \\left( \\begin{matrix}\\displaystyle \\frac{1}{\\sqrt{4-2\\sqrt{2} } } \\\\ \\displaystyle \\frac{1-\\sqrt{2} }{\\sqrt{4-2\\sqrt{2} } } \\end{matrix} \\right)  +\\left< \\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  ,\\left( \\begin{matrix}\\displaystyle \\frac{1}{\\sqrt{4+2\\sqrt{2} } } \\\\ \\displaystyle \\frac{1+\\sqrt{2} }{\\sqrt{4+2\\sqrt{2} } } \\end{matrix} \\right)  \\right>  \\left( \\begin{matrix}\\displaystyle \\frac{1}{\\sqrt{4+2\\sqrt{2} } } \\\\ \\displaystyle \\frac{1+\\sqrt{2} }{\\sqrt{4+2\\sqrt{2} } } \\end{matrix} \\right)  \\\\ &=&\\left( \\displaystyle \\frac{x}{\\sqrt{4-2\\sqrt{2} } } +\\displaystyle \\frac{y\\left( 1-\\sqrt{2} \\right)  }{\\sqrt{4-2\\sqrt{2} } } \\right)  \\left( \\begin{matrix}\\displaystyle \\frac{1}{\\sqrt{4-2\\sqrt{2} } } \\\\ \\displaystyle \\frac{1-\\sqrt{2} }{\\sqrt{4-2\\sqrt{2} } } \\end{matrix} \\right)  +\\left( \\displaystyle \\frac{x}{\\sqrt{4+2\\sqrt{2} } } +\\displaystyle \\frac{y\\left( 1+\\sqrt{2} \\right)  }{\\sqrt{4+2\\sqrt{2} } } \\right)  \\left( \\begin{matrix}\\displaystyle \\frac{1}{\\sqrt{4+2\\sqrt{2} } } \\\\ \\displaystyle \\frac{1+\\sqrt{2} }{\\sqrt{4+2\\sqrt{2} } } \\end{matrix} \\right)  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.104)$</p>\n",
    "\n",
    "Así que, finalmente,\n",
    "\n",
    "$$q\\left( x,y\\right)  =\\sqrt{2} \\left( \\frac{x}{\\sqrt{4-2\\sqrt{2} } } +\\frac{y\\left( 1-\\sqrt{2} \\right)  }{\\sqrt{4-2\\sqrt{2} } } \\right)^{2}  -\\sqrt{2} \\left( \\frac{x}{\\sqrt{4+2\\sqrt{2} } } +\\frac{y\\left( 1+\\sqrt{2} \\right)  }{\\sqrt{4+2\\sqrt{2} } } \\right)^{2}$$\n",
    "<p style=\"text-align: right;\">$(3.105)$</p>\n",
    "\n",
    "es la forma canónica buscada. ◼︎"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631d9f15",
   "metadata": {},
   "source": [
    "### Aplicaciones a la geometría analítica.\n",
    "Lo estudiado en relación a las formas y, en particular, las formas cuadráticas (y sus formas normales), nos permiten representar de forma compacta y muy eficiente a cualquier **sección cónica**. De los cursos de Cálculo sabemos que las secciones cónicas son regiones en $\\mathbb{R}^{2}$ que corresponden a la intersección de un cono circular recto con un plano cuya orientación puede ser de tres tipos: Paralelo a la generatriz del cono (**parábola**), paralelo al eje vertical del cono (**hipérbola**) y oblicua respecto de la generatriz y el eje del cono sin pasar por su vértice (**elipse**). La **circunferencia** también se considera una sección cónica, pero es el caso particular de la elipse (cuando sus ejes son exactamente iguales), aunque es posible obtener como resultado de un plano que es perpendicular al eje del cono. La Fig. (3.3) muestra un esquema que permite entender como se generan las secciones cónicas y los planos previamente mencionados.\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"figures/fig_3_3.png\" width=\"300\"></p>\n",
    "<p style=\"text-align: center;\">Fig. (3.3): Un esquema que ilustra cómo se generan las secciones cónicas\n",
    "\n",
    "Recordemos que las secciones cónicas tienen sus correspondientes **ecuaciones** para el caso en el cual su orientación es tal que sus ejes de simetría son paralelos, respectivamente, a los ejes X e Y en el plano $\\mathbb{R}^{2}$. En el caso de la elipse, si ésta tiene su centro en el punto $(x_{0},y_{0})$ y sus semiejes mayor y menor tienen longitudes $a$ y $b$, respectivamente, entonces su ecuación es\n",
    "\n",
    "$$\\frac{\\left( x-x_{0}\\right)^{2}  }{a^{2}} +\\frac{\\left( y-y_{0}\\right)^{2}  }{b^{2}} =1$$\n",
    "<p style=\"text-align: right;\">$(3.106)$</p>\n",
    "\n",
    "La elipse puede verse en la Fig. (3.4a).\n",
    "\n",
    "Una hipérbola de semiejes mayor y menor de longitudes $a$ y $b$, respectivamente, con centro en el punto $(x_{0},y_{0})$, tiene por ecuación\n",
    "\n",
    "$$\\frac{\\left( x-x_{0}\\right)^{2}  }{a^{2}} -\\frac{\\left( y-y_{0}\\right)^{2}  }{b^{2}} =1$$\n",
    "<p style=\"text-align: right;\">$(3.107)$</p>\n",
    "\n",
    "la hipérbola se ilustra en la Fig. (3.4b).\n",
    "\n",
    "Finalmente, una parábola con foco $F$ en el punto $(x_{0},y_{0})$ y con directriz cuya ecuación es $x=x_{0}+p$, tiene por ecuación\n",
    "\n",
    "$$\\left( y-y_{0}\\right)^{2}  =4p\\left( x-x_{0}\\right)$$\n",
    "<p style=\"text-align: right;\">$(3.108)$</p>\n",
    "\n",
    "Igualmente, la parábola se ilustra en la Fig. (3.4c).\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"figures/fig_3_4.png\" width=\"1000\"></p>\n",
    "<p style=\"text-align: center;\">Fig. (3.4): (a) Una elipse centrada en el punto $(x_{0},y_{0})$, con semiejes mayor y menor iguales de longitud $a$ y $b$; (b) Una hipérbola centrada en el punto $(x_{0},y_{0})$, con semiejes mayor y menor iguales de longitud $a$ y $b$; (c) Una parábola con foco en el punto $(x_{0},y_{0})$ y distancia focal igual a $p$\n",
    "    \n",
    "Las ecuaciones de las secciones cónicas son todas casos particulares de una ecuación denominada **ecuación general de segundo grado**, y que puede escribirse como $ax^{2}+bxy+cy^{2}+dx+ey+f=0$, donde todos los coeficientes pertenecen a $\\mathbb{R}$. De esta manera, es posible definir una sección cónica como el conjunto de todos los puntos $(x,y)\\in \\mathbb{R}^{2}$ que satisfacen la ecuación general de segundo grado; es decir, pertenecen al conjunto $C=\\left\\{ \\left( x,y\\right)  \\in \\mathbb{R}^{2} :ax^{2}+bxy+cy^{2}+dx+ey+f=0\\right\\}$. Equivalentemente, podemos reescribir esta ecuación como la **suma de una forma cuadrática y una forma lineal**, ya que\n",
    "\n",
    "$$\\underbrace{ax^{2}+bxy+cy^{2}}_{=q\\left( x,y\\right)  } +\\underbrace{dx+ey}_{=L\\left( x,y\\right)  } +f=0$$\n",
    "<p style=\"text-align: right;\">$(3.109)$</p>\n",
    "\n",
    "Donde $q(x,y)$ es la forma cuadrática y $L(x,y)$ es la forma lineal. En términos matriciales, podemos entonces reescribir la ecuación general de segundo grado como sigue,\n",
    "\n",
    "$$\\left( x,y\\right)  \\left( \\begin{matrix}a&b/2\\\\ b/2&c\\end{matrix} \\right)  \\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  +\\left( d,e\\right)  \\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  +f=0$$\n",
    "<p style=\"text-align: right;\">$(3.110)$</p>\n",
    "\n",
    "Aplicando el teorema (3.13) a la ecuación (3.110), obtenemos\n",
    "\n",
    "$$\\left( x_{1},y_{1}\\right)  \\left( \\begin{matrix}\\lambda_{1} &0\\\\ 0&\\lambda_{2} \\end{matrix} \\right)  \\left( \\begin{matrix}x_{1}\\\\ y_{1}\\end{matrix} \\right)  +\\left( d,e\\right)  \\underbrace{\\left( \\begin{matrix}\\left< \\mathbf{v}_{1} ,\\mathbf{e}_{1} \\right>  &\\left< \\mathbf{v}_{2} ,\\mathbf{e}_{1} \\right>  \\\\ \\left< \\mathbf{v}_{1} ,\\mathbf{e}_{2} \\right>  &\\left< \\mathbf{v}_{2} ,\\mathbf{e}_{2} \\right>  \\end{matrix} \\right)  }_{=\\left[ I\\right]^{\\mathbf{e} \\left( 2\\right)  }_{\\alpha }  } \\left( \\begin{matrix}x_{1}\\\\ y_{1}\\end{matrix} \\right)  +f=0$$\n",
    "<p style=\"text-align: right;\">$(3.111)$</p>\n",
    "\n",
    "Donde $\\lambda_{1}$ y $\\lambda_{2}$ son los autovalores de la forma matricial de $q$, $\\alpha =\\left\\{ \\mathbf{v}_{1} ,\\mathbf{v}_{2} \\right\\}  \\subset \\mathbb{R}^{2}$ una base ortonormal de autovectores y\n",
    "\n",
    "$$\\left( \\begin{matrix}x_{1}\\\\ y_{1}\\end{matrix} \\right)  =\\underbrace{\\left( \\begin{matrix}\\left< \\mathbf{v}_{1} ,\\mathbf{e}_{1} \\right>  &\\left< \\mathbf{v}_{2} ,\\mathbf{e}_{1} \\right>  \\\\ \\left< \\mathbf{v}_{1} ,\\mathbf{e}_{2} \\right>  &\\left< \\mathbf{v}_{2} ,\\mathbf{e}_{2} \\right>  \\end{matrix} \\right)  }_{=\\left[ I\\right]^{\\mathbf{e} \\left( 2\\right)  }_{\\alpha }  } \\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.112)$</p>\n",
    "\n",
    "Así que, después de las transformaciones hechas en la ecuación general de segundo grado, tenemos la **ecuación reducida**\n",
    "\n",
    "$$\\lambda_{1} x^{2}_{1}+\\lambda_{2} y^{2}_{1}+Dx_{1}+Ey_{1}+f=0\\  ;\\  \\left( D,E\\right)  =\\left( d,e\\right)  \\left( \\begin{matrix}\\left< \\mathbf{v}_{1} ,\\mathbf{e}_{1} \\right>  &\\left< \\mathbf{v}_{2} ,\\mathbf{e}_{1} \\right>  \\\\ \\left< \\mathbf{v}_{1} ,\\mathbf{e}_{2} \\right>  &\\left< \\mathbf{v}_{2} ,\\mathbf{e}_{2} \\right>  \\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.113)$</p>\n",
    "\n",
    "Por lo tanto, se desprenden varios casos de interés conforme los valores que toman $\\lambda_{1}$ y $\\lambda_{2}$.\n",
    "\n",
    "**<font color='forestgreen'>Caso 1 – $\\lambda_{1}\\lambda_{2}\\neq 0$:</font>** En este caso, completamos cuadrados en la ecuación (3.113), obteniendo\n",
    "\n",
    "$$\\lambda_{1} \\left( x_{1}+\\frac{D}{2\\lambda_{1} } \\right)^{2}  -\\frac{D^{2}}{4\\lambda_{1} } +\\lambda_{2} \\left( y_{1}+\\frac{D}{2\\lambda_{2} } \\right)^{2}  -\\frac{E}{4\\lambda_{2} } +f=0$$\n",
    "<p style=\"text-align: right;\">$(3.114)$</p>\n",
    "\n",
    "Hagamos el cambio de variables $x_{2}=x_{1}+\\frac{D}{2\\lambda_{1} }$, $y_{2}=y_{1}+\\frac{D}{2\\lambda_{2} }$ y $F=f-\\frac{D^{2}}{4\\lambda_{1} } -\\frac{E^{2}}{4\\lambda_{2} }$. Luego reescribimos la ecuación (3.114) como\n",
    "\n",
    "$$\\lambda_{1} x^{2}_{2}+\\lambda_{2} y^{2}_{2}+F=0$$\n",
    "<p style=\"text-align: right;\">$(3.115)$</p>\n",
    "\n",
    "<font color='forestgreen'>Caso 1.1 – $\\lambda_{1}>0, \\lambda_{2}> 0$:</font> En este caso, tenemos:\n",
    "\n",
    "- $F>0$: Por lo tanto, ningún punto $(x_{2},y_{2})\\in \\mathbb{R}^{2}$ satisface la ecuación (3.115) y, por lo tanto, $C=\\emptyset$.\n",
    "- $F=0$: Entonces, para todo punto $(x,y)\\in \\mathbb{R}^{2}$, se tiene que $C$ representa un **punto** en el sistema $(x_{2},y_{2})$, y que podemos escribir como\n",
    "\n",
    "$$C:\\left( -\\frac{D}{2\\lambda_{1} } ,-\\frac{E}{2\\lambda_{2} } \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.116)$</p>\n",
    "\n",
    "- $F<0$: Entonces los puntos $(x_{2},y_{2})\\in \\mathbb{R}^{2}$ que satisfacen la ecuación (3.115) trazan una **elipse** en el sistema $(x_{2},y_{2})$, cuya ecuación es\n",
    "\n",
    "$$C:\\left\\{ \\left( x_{2},y_{2}\\right)  \\in \\mathbb{R}^{2} :\\frac{x^{2}_{2}}{-\\frac{F}{\\lambda_{1} } } +\\frac{y^{2}_{2}}{-\\frac{E}{\\lambda_{2} } } =1\\right\\}$$\n",
    "<p style=\"text-align: right;\">$(3.117)$</p>\n",
    "\n",
    "<font color='forestgreen'>Caso 1.2 – $\\lambda_{1}>0, \\lambda_{2}< 0$:</font> En este caso, tenemos:\n",
    "\n",
    "- $F=0$: Entonces los puntos $(x_{2},y_{2})\\in \\mathbb{R}^{2}$ que satisfacen la ecuación (3.115) trazan un **par de rectas concurrentes** en dicho sistema, cuya ecuación es\n",
    "\n",
    "$$C:\\left\\{ \\left( x_{2},y_{2}\\right)  \\in \\mathbb{R}^{2} :y_{2}=\\pm \\sqrt{-\\frac{\\lambda_{1} }{\\lambda_{2} } } x_{2}\\right\\}$$\n",
    "<p style=\"text-align: right;\">$(3.118)$</p>\n",
    "\n",
    "- $F\\neq 0$: Entonces los puntos $(x_{2},y_{2})\\in \\mathbb{R}^{2}$ que satisfacen la ecuación (3.115) trazan una **hipérbola** en dicho sistema, cuya ecuación es\n",
    "\n",
    "$$C:\\left\\{ \\left( x_{2},y_{2}\\right)  \\in \\mathbb{R}^{2} :\\frac{x^{2}_{2}}{-\\frac{F}{\\lambda_{1} } } +\\frac{y^{2}_{2}}{-\\frac{E}{\\lambda_{2} } } =1\\right\\}$$\n",
    "<p style=\"text-align: right;\">$(3.119)$</p>\n",
    "\n",
    "En conclusión, para $\\lambda_{1}$ y $\\lambda_{2}$ no nulos, tenemos que\n",
    "\n",
    "$$\\lambda_{1} \\neq 0\\wedge \\lambda_{2} \\neq 0\\Longrightarrow \\begin{cases}\\left( \\mathrm{i} \\right)  &\\lambda_{1} \\lambda_{2} >0\\Longrightarrow C:\\begin{cases}\\mathrm{Vacio} :&\\emptyset \\\\ \\mathrm{Punto} :&\\left( -\\displaystyle \\frac{D}{2\\lambda_{1} } ,-\\displaystyle \\frac{E}{2\\lambda_{2} } \\right)  \\\\ \\mathrm{Elipse} :&\\displaystyle \\frac{x^{2}_{2}}{-F/\\lambda_{1} } +\\displaystyle \\frac{y^{2}_{2}}{-E/\\lambda_{2} } =1\\end{cases} \\\\ \\left( \\mathrm{i} \\mathrm{i} \\right)  &\\lambda_{1} \\lambda_{2} <0\\Longrightarrow C:\\begin{cases}\\mathrm{Par} \\  \\mathrm{de} \\  \\mathrm{rectas} \\  \\mathrm{concurrentes} :&y_{2}=\\pm \\sqrt{-\\frac{\\lambda_{1} }{\\lambda_{2} } } x_{2}\\\\ \\mathrm{Hiperbola} :&\\displaystyle \\frac{x^{2}_{2}}{-F/\\lambda_{1} } +\\frac{y^{2}_{2}}{-E/\\lambda_{2} } =1\\end{cases} \\end{cases}$$\n",
    "<p style=\"text-align: right;\">$(3.120)$</p>\n",
    "\n",
    "**<font color='forestgreen'>Caso 2 – $\\lambda_{1}\\lambda_{2}= 0$:</font>** Nuevamente tenemos dos subcasos distintos.\n",
    "\n",
    "<font color='forestgreen'>Caso 2.1 – $\\lambda_{1}=\\lambda_{2}= 0$:</font> En este caso tenemos que la ecuación (3.115) se reduce a la descripción de una **recta**:\n",
    "\n",
    "$$C:\\left\\{ \\left( x_{1},y_{1}\\right)  \\in \\mathbb{R}^{2} :Dx_{1}+Ey_{1}+f=0\\right\\}$$\n",
    "<p style=\"text-align: right;\">$(3.121)$</p>\n",
    "\n",
    "<font color='forestgreen'>Caso 2.2 – $\\lambda_{1}=0,\\lambda_{2}\\neq 0$:</font> En este caso, hacemos el cambio de variables $y_{2}=y_{1}+\\frac{E}{2\\lambda_{2} }$ y $F=f-\\frac{E^{2}}{4\\lambda_{2} }$. De esta manera, la ecuación (3.115) se reduce a una **parábola** o sus degeneraciones:\n",
    "\n",
    "$$C:\\left\\{ \\left( x_{2},y_{2}\\right)  \\in \\mathbb{R}^{2} :\\lambda_{2} y^{2}_{2}+Dx_{1}+F=0\\right\\}$$\n",
    "<p style=\"text-align: right;\">$(3.122)$</p>\n",
    "\n",
    "En conclusión, para $\\lambda_{1}\\lambda_{2}=0$, tenemos que\n",
    "\n",
    "$$\\lambda_{1} \\lambda_{2} =0\\Longrightarrow C:\\begin{cases}\\mathrm{Vacio} :&\\emptyset \\\\ \\mathrm{Una} \\  \\mathrm{recta} :&Dx_{1}+Ey_{1}+f=0\\\\ \\mathrm{Parabola} \\  \\mathrm{o} \\  \\mathrm{par} \\  \\mathrm{de} \\  \\mathrm{rectas} \\  \\mathrm{paralelas} :&\\lambda_{2} y^{2}_{2}+Dx_{1}+F=0\\end{cases}$$\n",
    "<p style=\"text-align: right;\">$(3.123)$</p>\n",
    "\n",
    "Finalmente, observemos que $\\left[ q\\right]^{\\mathbf{e} \\left( 2\\right)  }_{\\mathbf{e} \\left( 2\\right)  }  =\\left[ I\\right]^{\\mathbf{e} \\left( 2\\right)  }_{\\alpha }  \\left[ q\\right]^{\\alpha }_{\\alpha }  \\left[ I\\right]^{\\alpha }_{\\mathbf{e} \\left( 2\\right)  }  =\\mathbf{x}^{\\top } \\mathbf{A} \\mathbf{x}$, de donde tenemos que $\\det \\left( \\mathbf{A} \\right)  =\\det \\left( \\left[ q\\right]^{\\mathbf{e} \\left( 2\\right)  }_{\\mathbf{e} \\left( 2\\right)  }  \\right)  =\\det \\left( \\left[ q\\right]^{\\alpha }_{\\alpha }  \\right)$. Por lo tanto, podemos escribir el producto de los autovalores $\\lambda_{1}$ y $\\lambda_{2}$ asociados a la ecuación general de segundo grado como $\\lambda_{1}\\lambda_{2}=-(b^{2}-4ac)/4$. Usando el numerador de este producto (llamado **discriminante**), podemos clasificar las secciones cónicas resultantes de la ecuación (3.109) como\n",
    "\n",
    "$$C:ax^{2}+bxy+cy^{2}+dx+ey+f=0\\Longrightarrow \\begin{cases}b^{2}-4ac>0\\Longrightarrow C:&\\begin{cases}\\mathrm{Hiperbola} &\\\\ \\mathrm{Par} \\  \\mathrm{de} \\  \\mathrm{rectas} \\  \\mathrm{concurrentes} &\\end{cases} \\\\ b^{2}-4ac<0\\Longrightarrow C:&\\begin{cases}\\emptyset &\\\\ \\mathrm{Punto} &\\\\ \\mathrm{Elipse} &\\end{cases} \\\\ b^{2}-4ac=0\\Longrightarrow C:&\\begin{cases}\\mathrm{Parabola} &\\\\ \\mathrm{Una} \\  \\mathrm{recta} &\\\\ \\mathrm{Par} \\  \\mathrm{de} \\  \\mathrm{rectas} \\  \\mathrm{concurrentes} &\\\\ \\emptyset &\\end{cases} \\end{cases}$$\n",
    "<p style=\"text-align: right;\">$(3.124)$</p>\n",
    "\n",
    "**Ejemplo 3.16:** Identificaremos y graficaremos la sección cónica que representa el lugar geométrico de todos los puntos $(x,y)\\in \\mathbb{R}^{2}$ que satisfacen la ecuación general\n",
    "\n",
    "$$4x^{2}+4y^{2}-8xy+\\frac{33}{2} \\sqrt{2} x-\\frac{31}{2} \\sqrt{2} y+35=0$$\n",
    "<p style=\"text-align: right;\">$(3.125)$</p>\n",
    "\n",
    "En efecto, si eliminamos los coeficientes racionales, podemos expresar este problema como\n",
    "\n",
    "$$C:\\left\\{ \\left( x,y\\right)  \\in \\mathbb{R}^{2} :8x^{2}+8y^{2}-16xy+33\\sqrt{2} x-31\\sqrt{2} y+70=0\\right\\}$$\n",
    "<p style=\"text-align: right;\">$(3.126)$</p>\n",
    "\n",
    "Identificaremos las alternativas posibles de cónicas involucradas en la solución de la ecuación (3.125) mediante el criterio del discriminante. De esta manera, como $b^{2}-4ac=(-16)^{2}-4\\cdot 8\\cdot 8=256-256=0$, entonces tenemos que $C$ puede ser la descripción de una parábola o de algún caso degenerado (o bien, ningún punto $(x,y)\\in \\mathbb{R}^{2}$ satisface la ecuacuón (3.125)). Para identificar cuál de todas estas alternativas es la que describe a $C$, formulamos la ecuación (3.125) en su forma matricial como\n",
    "\n",
    "$$\\left( x,y\\right)  \\left( \\begin{matrix}8&-8\\\\ -8&8\\end{matrix} \\right)  \\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  +\\left( 33\\sqrt{2} ,-32\\sqrt{2} \\right)  \\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  +70=0$$\n",
    "<p style=\"text-align: right;\">$(3.127)$</p>\n",
    "\n",
    "El primer término matricial (la forma cuadrática) será designado como $\\mathbf{A}=q(x,y)$. El siguiente paso es, por tanto, determinar los autovalores asociados a $\\mathbf{A}$. De esta manera, construimos el polinomio característico asociado a $\\mathbf{A}$ como sigue\n",
    "\n",
    "$$P_{\\mathbf{A} }\\left( \\lambda \\right)  =\\det \\left( \\begin{matrix}\\lambda -8&-8\\\\ -8&\\lambda -8\\end{matrix} \\right)  =\\left( \\lambda -8\\right)^{2}  -64$$\n",
    "<p style=\"text-align: right;\">$(3.128)$</p>\n",
    "\n",
    "Luego, las raíces del polinomio característico $P_{\\mathbf{A} }\\left( \\lambda \\right)$ son $\\lambda_{1}=16$ y $\\lambda_{2}=0$. Por lo tanto, ahora debemos determinar los autoespacios asociados a cada uno de estos autovalores. En el caso general, tenemos que\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{u} \\in \\left( \\mathbb{R}^{2\\times 1} \\right)_{\\lambda }  &\\Longleftrightarrow &\\mathbf{u} \\in \\mathbb{R}^{2\\times 1} \\wedge \\mathbf{A} \\mathbf{u} =\\lambda \\mathbf{u} \\\\ &\\Longleftrightarrow &\\mathbf{u} =\\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  \\wedge \\left( \\begin{matrix}8&-8\\\\ -8&8\\end{matrix} \\right)  \\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  =\\lambda \\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  \\\\ &\\Longleftrightarrow &\\mathbf{u} =\\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  \\wedge \\begin{cases}\\begin{array}{rcl}8x-8y&=&\\lambda x\\\\ -8x+8y&=&\\lambda y\\end{array} &\\end{cases} \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.129)$</p>\n",
    "\n",
    "<font color='forestgreen'>Caso 1 – $\\lambda_{1}=16$:</font> Continuamos el desarrollo de la ecuación (3.129):\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{u} \\in \\left( \\mathbb{R}^{2\\times 1} \\right)_{\\lambda =16}  &\\Longleftrightarrow &\\mathbf{u} =\\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  \\wedge \\begin{cases}\\begin{array}{rcl}8x-8y&=&16x\\\\ -8x+8y&=&16y\\end{array} &\\end{cases} \\\\ &\\Longrightarrow &\\mathbf{u} =\\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  \\wedge -x=y\\\\ &\\Longrightarrow &\\mathbf{u} =\\left( \\begin{matrix}x\\\\ -x\\end{matrix} \\right)  =x\\left( \\begin{matrix}1\\\\ -1\\end{matrix} \\right)  \\wedge x\\in \\mathbb{R} \\\\ &\\Longrightarrow &\\left( \\mathbb{R}^{2\\times 1} \\right)_{\\lambda =16}  =\\left< \\left\\{ \\left( \\begin{matrix}1\\\\ -1\\end{matrix} \\right)  \\right\\}  \\right>  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.130)$</p>\n",
    "\n",
    "<font color='forestgreen'>Caso 2 – $\\lambda_{2}=0$:</font> Continuamos nuevamente el desarrollo de la ecuación (3.129):\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{u} \\in \\left( \\mathbb{R}^{2\\times 1} \\right)_{\\lambda =0}  &\\Longleftrightarrow &\\mathbf{u} =\\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  \\wedge \\begin{cases}\\begin{array}{rcl}8x-8y&=&0x\\\\ -8x+8y&=&0y\\end{array} &\\end{cases} \\\\ &\\Longrightarrow &\\mathbf{u} =\\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  \\wedge x=y\\\\ &\\Longrightarrow &\\mathbf{u} =\\left( \\begin{matrix}x\\\\ x\\end{matrix} \\right)  =x\\left( \\begin{matrix}1\\\\ 1\\end{matrix} \\right)  \\wedge x\\in \\mathbb{R} \\\\ &\\Longrightarrow &\\left( \\mathbb{R}^{2\\times 1} \\right)_{\\lambda =0}  =\\left< \\left\\{ \\left( \\begin{matrix}1\\\\ 1\\end{matrix} \\right)  \\right\\}  \\right>  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.131)$</p>\n",
    "\n",
    "Definimos la base ortonormal de autovectores como\n",
    "\n",
    "$$\\alpha =\\left\\{ \\frac{\\left( \\begin{matrix}1\\\\ 1\\end{matrix} \\right)  }{\\left\\Vert \\left( \\begin{matrix}1\\\\ 1\\end{matrix} \\right)  \\right\\Vert  } ,\\frac{\\left( \\begin{matrix}1\\\\ -1\\end{matrix} \\right)  }{\\left\\Vert \\left( \\begin{matrix}1\\\\ -1\\end{matrix} \\right)  \\right\\Vert  } \\right\\}  =\\left\\{ \\frac{1}{\\sqrt{2} } \\left( \\begin{matrix}1\\\\ 1\\end{matrix} \\right)  ,\\frac{1}{\\sqrt{2} } \\left( \\begin{matrix}1\\\\ -1\\end{matrix} \\right)  \\right\\}  =\\left\\{ \\left( \\begin{array}{rr}\\displaystyle \\frac{\\sqrt{2} }{2} \\\\ \\displaystyle \\frac{\\sqrt{2} }{2} \\end{array} \\right)  ,\\left( \\begin{array}{rr}\\displaystyle \\frac{\\sqrt{2} }{2} \\\\ -\\displaystyle \\frac{\\sqrt{2} }{2} \\end{array} \\right)  \\right\\}$$\n",
    "<p style=\"text-align: right;\">$(3.132)$</p>\n",
    "\n",
    "Con esta base en consideración, llevamos la ecuación (3.127) a su forma reducida. Para ello, primero expresamos la matriz de la forma cuadrática $\\mathbf{A}$ en su forma diagonal $q(x,y)=\\mathbf{P}\\mathbf{D}\\mathbf{P}^{-1}=\\left[ I\\right]^{\\mathbf{e} \\left( 2\\right)  }_{\\alpha }  \\left[ q\\right]^{\\alpha }_{\\alpha }  \\left[ I\\right]^{\\alpha }_{\\mathbf{e} \\left( 2\\right)  }$, donde $\\mathbf{D}$ es la matriz diagonal cuyos elementos no nulos son los autovalores de $\\mathbf{A}$. Es decir,\n",
    "\n",
    "$$\\mathbf{D}=\\left[ q\\right]^{\\alpha }_{\\alpha }  =\\left( \\begin{matrix}0&0\\\\ 0&16\\end{matrix} \\right)  $$\n",
    "<p style=\"text-align: right;\">$(3.133)$</p>\n",
    "\n",
    "Por otro lado, la matriz $\\mathbf{P}$ se construye a partir de la base ortonormal de autovectores previamente construida. Es decir,\n",
    "\n",
    "$$\\mathbf{P}=\\left[ I\\right]^{\\mathbf{e} \\left( 2\\right)  }_{\\alpha }  =\\left( \\begin{array}{rr}\\displaystyle \\frac{\\sqrt{2} }{2} &\\displaystyle \\frac{\\sqrt{2} }{2} \\\\ \\displaystyle \\frac{\\sqrt{2} }{2} &-\\displaystyle \\frac{\\sqrt{2} }{2} \\end{array} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.134)$</p>\n",
    "\n",
    "Donde $\\left[ I\\right]^{\\mathbf{e} \\left( 2\\right)  }_{\\alpha }  =\\left[ I\\right]^{\\alpha }_{\\mathbf{e} \\left( 2\\right)  }$. Así que la matriz $\\mathbf{A}$ de la forma cuadrática puede escribirse como\n",
    "\n",
    "$$\\left( \\begin{matrix}8&-8\\\\ -8&8\\end{matrix} \\right)  =\\left( \\begin{array}{rr}\\displaystyle \\frac{\\sqrt{2} }{2} &\\displaystyle \\frac{\\sqrt{2} }{2} \\\\ \\displaystyle \\frac{\\sqrt{2} }{2} &-\\displaystyle \\frac{\\sqrt{2} }{2} \\end{array} \\right)  \\left( \\begin{matrix}0&0\\\\ 0&16\\end{matrix} \\right)  \\left( \\begin{array}{rr}\\displaystyle \\frac{\\sqrt{2} }{2} &\\displaystyle \\frac{\\sqrt{2} }{2} \\\\ \\displaystyle \\frac{\\sqrt{2} }{2} &-\\displaystyle \\frac{\\sqrt{2} }{2} \\end{array} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.135)$</p>\n",
    "\n",
    "Por lo tanto, la cónica $C$ ahora toma la forma\n",
    "\n",
    "$$\\begin{array}{rcl}C&:&\\left( x,y\\right)  \\left( \\begin{matrix}8&-8\\\\ -8&8\\end{matrix} \\right)  \\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  +\\left( 33\\sqrt{2} ,-31\\sqrt{2} \\right)  \\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  +70=0\\\\ \\Longrightarrow C&:&\\left( x,y\\right)  \\left( \\begin{array}{rr}\\displaystyle \\frac{\\sqrt{2} }{2} &\\displaystyle \\frac{\\sqrt{2} }{2} \\\\ \\displaystyle \\frac{\\sqrt{2} }{2} &-\\displaystyle \\frac{\\sqrt{2} }{2} \\end{array} \\right)  \\left( \\begin{matrix}0&0\\\\ 0&16\\end{matrix} \\right)  \\left( \\begin{array}{rr}\\displaystyle \\frac{\\sqrt{2} }{2} &\\displaystyle \\frac{\\sqrt{2} }{2} \\\\ \\displaystyle \\frac{\\sqrt{2} }{2} &-\\displaystyle \\frac{\\sqrt{2} }{2} \\end{array} \\right)  \\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  +\\left( 33\\sqrt{2} ,-31\\sqrt{2} \\right)  \\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  +70=0\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.136)$</p>\n",
    "\n",
    "Notemos que, para el caso de la forma lineal, podemos escribir\n",
    "\n",
    "$$\\left( \\begin{array}{rr}\\displaystyle \\frac{\\sqrt{2} }{2} &\\displaystyle \\frac{\\sqrt{2} }{2} \\\\ \\displaystyle \\frac{\\sqrt{2} }{2} &-\\displaystyle \\frac{\\sqrt{2} }{2} \\end{array} \\right)  \\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  =\\left( \\begin{array}{r}\\displaystyle \\frac{\\sqrt{2} }{2} \\left( x+y\\right)  \\\\ \\displaystyle \\frac{\\sqrt{2} }{2} \\left( x-y\\right)  \\end{array} \\right)  \\Longleftrightarrow \\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  =\\left( \\begin{array}{rr}\\displaystyle \\frac{\\sqrt{2} }{2} &\\displaystyle \\frac{\\sqrt{2} }{2} \\\\ \\displaystyle \\frac{\\sqrt{2} }{2} &-\\displaystyle \\frac{\\sqrt{2} }{2} \\end{array} \\right)  \\left( \\begin{array}{r}\\displaystyle \\frac{\\sqrt{2} }{2} \\left( x+y\\right)  \\\\ \\displaystyle \\frac{\\sqrt{2} }{2} \\left( x-y\\right)  \\end{array} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.137)$</p>\n",
    "\n",
    "Aplicando propiedades de la transposición de matrices y los resultados previos, vemos que la Ec. (3.125) se transforma en\n",
    "\n",
    "$$\\left( \\left( \\begin{array}{rr}\\displaystyle \\frac{\\sqrt{2} }{2} &\\displaystyle \\frac{\\sqrt{2} }{2} \\\\ \\displaystyle \\frac{\\sqrt{2} }{2} &-\\displaystyle \\frac{\\sqrt{2} }{2} \\end{array} \\right)  \\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  \\right)^{\\top }  \\left( \\begin{matrix}0&0\\\\ 0&16\\end{matrix} \\right)  \\left( \\begin{array}{rr}\\displaystyle \\frac{\\sqrt{2} }{2} &\\displaystyle \\frac{\\sqrt{2} }{2} \\\\ \\displaystyle \\frac{\\sqrt{2} }{2} &-\\displaystyle \\frac{\\sqrt{2} }{2} \\end{array} \\right)  \\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  +\\left( 33\\sqrt{2} ,-31\\sqrt{2} \\right)  \\left( \\begin{array}{rr}\\displaystyle \\frac{\\sqrt{2} }{2} &\\displaystyle \\frac{\\sqrt{2} }{2} \\\\ \\displaystyle \\frac{\\sqrt{2} }{2} &-\\displaystyle \\frac{\\sqrt{2} }{2} \\end{array} \\right)  \\left( \\begin{array}{r}\\displaystyle \\frac{\\sqrt{2} }{2} \\left( x+y\\right)  \\\\ \\displaystyle \\frac{\\sqrt{2} }{2} \\left( x-y\\right)  \\end{array} \\right)  +70=0$$\n",
    "<p style=\"text-align: right;\">$(3.138)$</p>\n",
    "\n",
    "Equivalentemente, desarrollando esta expresión, obtenemos\n",
    "\n",
    "$$\\left( \\displaystyle \\frac{\\sqrt{2} }{2} \\left( x+y\\right)  ,\\displaystyle \\frac{\\sqrt{2} }{2} \\left( x-y\\right)  \\right)  \\left( \\begin{matrix}0&0\\\\ 0&16\\end{matrix} \\right)  \\left( \\begin{array}{r}\\displaystyle \\frac{\\sqrt{2} }{2} \\left( x+y\\right)  \\\\ \\displaystyle \\frac{\\sqrt{2} }{2} \\left( x-y\\right)  \\end{array} \\right)  +\\left( 2,64\\right)  \\left( \\begin{array}{r}\\displaystyle \\frac{\\sqrt{2} }{2} \\left( x+y\\right)  \\\\ \\displaystyle \\frac{\\sqrt{2} }{2} \\left( x-y\\right)  \\end{array} \\right)  +70=0$$\n",
    "<p style=\"text-align: right;\">$(3.139)$</p>\n",
    "\n",
    "Multiplicando las matrices involucradas en la descomposición por formas anterior, transformamos la ecuación (3.139) en\n",
    "\n",
    "$$16\\left( \\frac{\\sqrt{2} }{2} \\left( x-y\\right)  \\right)^{2}  +\\frac{2\\sqrt{2} }{2} \\left( x+y\\right)  +\\frac{64\\sqrt{2} }{2} \\left( x-y\\right)  +70=0$$\n",
    "<p style=\"text-align: right;\">$(3.140)$</p>\n",
    "\n",
    "Ahora generamos los cambios de variables respectivos. Notemos que el efecto principal de estos cambios, conforme lo visto en la [clase 1.2](https://github.com/rquezadac/udd_data_science_lectures/blob/main/PARTE%20I%20-%20Fundamentos%20matem%C3%A1ticos%20elementales/clase_1_2.ipynb) (y que hace patente la existencia del término $bxy$ en la ecuación general de segundo grado (3.109)), es la **rotación** de los ejes del plano $\\mathbb{R}^{2}$ en una determinada dirección, donde las formas cuadráticas asociadas nos entregan la información relativa a dicha rotación. Los cambios de variables asociados a este ejercicio (o a cualquier otro caso general) tienen por objetivo expresar la cónica en un nuevo sistema de coordenadas donde no existe tal rotación, a fin de poder construir su correspondiente gráfico. Pongamos entonces $x_{1}=\\frac{\\sqrt{2} }{2} \\left( x+y\\right)  \\wedge y_{1}=\\frac{\\sqrt{2} }{2} \\left( x-y\\right)$. Reemplazando en la ecuación (3.140) obtenemos\n",
    "\n",
    "$$16y^{2}_{1}+2x_{1}+64y_{1}+70=0\\  \\Longleftrightarrow \\  8y^{2}_{1}+x_{1}+32y_{1}+35=0$$\n",
    "<p style=\"text-align: right;\">$(3.141)$</p>\n",
    "\n",
    "Completando cuadrados, obtenemos\n",
    "\n",
    "$$\\begin{array}{lll}8y^{2}_{1}+x_{1}+32y_{1}+35=0&\\Longleftrightarrow &8\\left( y^{2}_{1}+4y_{1}\\right)  +x_{1}+35=0\\\\ &\\Longleftrightarrow &8\\left( y^{2}_{1}+4y_{1}+2^{2}-2^{2}\\right)  +x_{1}+35=0\\\\ &\\Longleftrightarrow &8\\left[ \\left( y_{1}+2\\right)^{2}  -4\\right]  +x_{1}-35=0\\\\ &\\Longleftrightarrow &8\\left( y_{1}+2\\right)^{2}  -32+x_{1}+35\\\\ &\\Longleftrightarrow &8\\left( y_{1}+2\\right)^{2}  =-x_{1}-3\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.142)$</p>\n",
    "\n",
    "La ecuación de la cónica (3.125) en el sistema $(x_{1},y_{1})$ es finalmente\n",
    "\n",
    "$$\\left( y_{1}-\\left( -2\\right)  \\right)^{2}  =-\\frac{1}{8} \\left( x_{1}-\\left( -3\\right)  \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.143)$</p>\n",
    "\n",
    "Esta ecuación representa una **parábola** con foco en el punto $(x_{1},y_{1})=(-3,-2)$ y distancia focal $p=\\frac{1}{8}$. Dicha parábola está rotada en -45º con respecto al origen del sistema original $(x,y)$ (ya que $\\cos(-45º)=\\sqrt{2}/2$). De esta manera, volviendo a las variables originales, tenemos que\n",
    "\n",
    "$$\\left( x_{1},y_{1}\\right)  =\\left( \\frac{\\sqrt{2} }{2} \\left( x+y\\right)  ,\\frac{\\sqrt{2} }{2} \\left( x-y\\right)  \\right)  \\  \\Longleftrightarrow \\  \\begin{array}{lll}x_{1}+y_{1}&=&\\sqrt{2} x\\\\ x_{1}-y_{1}&=&\\sqrt{2} y\\end{array} \\  \\Longrightarrow \\  \\begin{array}{l}x=\\frac{x_{1}+y_{1}}{\\sqrt{2} } \\\\ y=\\frac{x_{1}-y_{1}}{\\sqrt{2} } \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.144)$</p>\n",
    "\n",
    "Por lo que, en el sistema original, el foco está en el punto $F=\\left( \\frac{1}{\\sqrt{2} } \\left( -3-2\\right)  ,\\frac{1}{\\sqrt{2} } \\left( -3-\\left( -2\\right)  \\right)  \\right)  =\\left( -\\frac{5}{\\sqrt{2} } ,-\\frac{1}{\\sqrt{2} } \\right)$. La parábola en cuestión se ilustra en la Fig. (3.5).\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"figures/fig_3_5.png\" width=\"450\"></p>\n",
    "<p style=\"text-align: center;\">Fig. (3.5): Gráfico de la parábola descrita por la ecuación (3.125)\n",
    "◼︎\n",
    "\n",
    "**Ejemplo 3.17:** Consideremos la ecuación\n",
    "    \n",
    "$$5x^{2}+5y^{2}+2axy+8\\sqrt{2} x=0$$\n",
    "<p style=\"text-align: right;\">$(3.145)$</p>\n",
    "\n",
    "donde $a\\in \\mathbb{R}$ es un parámetro. Vamos a encontrar todos los valores de $a$ para los cuales la ecuación (3.145) describe una circunferencia, una elipse, una parábola y una hipérbola. Además, bosquejaremos el gráfico de la cónica resultante para $a=3$.\n",
    "\n",
    "En efecto, los términos cuadráticos de la ecuación (3.145) pueden expresarse conforme la forma cuadrática\n",
    "\n",
    "$$q\\left( x,y\\right)  =5x^{2}+5y^{2}+2axy=\\left( x,y\\right)  \\left( \\begin{matrix}5&a\\\\ a&5\\end{matrix} \\right)  \\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.146)$</p>\n",
    "\n",
    "Definimos la matriz asociada a $q$ como\n",
    "\n",
    "$$\\left[ q\\right]^{\\mathbf{e} \\left( 2\\right)  }_{\\mathbf{e} \\left( 2\\right)  }  =\\mathbf{A} =\\left( \\begin{matrix}5&a\\\\ a&5\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.147)$</p>\n",
    "\n",
    "Construimos el polinomio característico de $\\mathbf{A}$ como sigue,\n",
    "\n",
    "$$P_{\\mathbf{A} }\\left( \\lambda \\right)  =\\det \\left( \\mathbf{A} -\\lambda \\mathbf{I}_{2} \\right)  =\\det \\left( \\begin{matrix}5-\\lambda &a\\\\ a&5-\\lambda \\end{matrix} \\right)  =\\left( 5-\\lambda \\right)^{2}  -a^{2}=\\left( 5-\\lambda +a\\right)  \\left( 5-\\lambda -a\\right)$$\n",
    "<p style=\"text-align: right;\">$(3.148)$</p>\n",
    "\n",
    "Luego los autovalores de $\\mathbf{A}$ que se corresponden con las raíces del polinomio caracteristico $P_{\\mathbf{A} }\\left( \\lambda \\right)$ son $\\lambda_{1}=5+a$ y $\\lambda_{2}=5-a$. Notemos que, para $a=0$, la matriz $\\mathbf{A}$ es diagonal y el desarrollo de la cónica es sencillo, ya que al reemplazar en la ecuación (3.145), obtenemos\n",
    "\n",
    "$$x^{2}+y^{2}+\\frac{8\\sqrt{2} }{5} x=0$$\n",
    "<p style=\"text-align: right;\">$(3.149)$</p>\n",
    "\n",
    "Completando cuadrados,\n",
    "\n",
    "$$\\left( x+\\frac{4}{5} \\sqrt{2} \\right)^{2}  +y^{2}=\\frac{32}{25}$$\n",
    "<p style=\"text-align: right;\">$(3.150)$</p>\n",
    "\n",
    "Por lo tanto, para $a=0$, la ecuación (3.145) representa una circunferencia con centro en el punto $\\left( 0,\\frac{4\\sqrt{2} }{5} \\right)$ y radio $r=\\frac{4\\sqrt{2} }{5}$.\n",
    "\n",
    "Prosigamos ahora con el desarrollo para $a\\neq 0$. Comenzaremos pues con la determinación de los autoespacios correspondientes a los autovalores de $\\mathbf{A}$. Luego tenemos,\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{u} \\in \\left( \\mathbb{R}^{2\\times 1} \\right)_{\\lambda }  &\\Longleftrightarrow &\\mathbf{u} \\in \\mathbb{R}^{2\\times 1} \\wedge \\mathbf{A} \\mathbf{u} =\\lambda \\mathbf{u} \\\\ &\\Longleftrightarrow &\\mathbf{u} =\\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  \\wedge \\left( \\begin{matrix}5&a\\\\ a&5\\end{matrix} \\right)  \\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  =\\lambda \\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  \\\\ &\\Longleftrightarrow &\\mathbf{u} =\\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  \\wedge \\begin{cases}\\begin{array}{lll}5x+ay&=&\\lambda x\\\\ ax+5y&=&\\lambda y\\end{array} &\\end{cases} \\end{array} $$\n",
    "<p style=\"text-align: right;\">$(3.151)$</p>\n",
    "\n",
    "<font color='forestgreen'>Caso 1 – $\\lambda_{1}=5-a$:</font> Continuamos con el desarrollo anterior,\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{u} \\in \\left( \\mathbb{R}^{2\\times 1} \\right)_{\\lambda =5-a}  &\\Longleftrightarrow &\\mathbf{u} =\\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  \\wedge \\begin{cases}\\begin{array}{lll}5x+ay&=&\\left( 5-a\\right)  x\\\\ ax+5y&=&\\left( 5-a\\right)  y\\end{array} &\\end{cases} \\\\ &\\Longrightarrow &\\mathbf{u} =\\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  \\wedge x=-y\\\\ &\\Longrightarrow &\\mathbf{u} =\\left( \\begin{matrix}x\\\\ -x\\end{matrix} \\right)  =x\\left( \\begin{matrix}1\\\\ -1\\end{matrix} \\right)  \\wedge x\\in \\mathbb{R} \\\\ &\\Longrightarrow &\\left( \\mathbb{R}^{2\\times 1} \\right)_{\\lambda =5-a}  =\\left< \\left\\{ \\left( \\begin{matrix}1\\\\ -1\\end{matrix} \\right)  \\right\\}  \\right>  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.152)$</p>\n",
    "\n",
    "<font color='forestgreen'>Caso 2 – $\\lambda_{1}=5+a$:</font> Tenemos,\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{u} \\in \\left( \\mathbb{R}^{2\\times 1} \\right)_{\\lambda =5+a}  &\\Longleftrightarrow &\\mathbf{u} =\\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  \\wedge \\begin{cases}\\begin{array}{lll}5x+ay&=&\\left( 5+a\\right)  x\\\\ ax+5y&=&\\left( 5+a\\right)  y\\end{array} &\\end{cases} \\\\ &\\Longrightarrow &\\mathbf{u} =\\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  \\wedge x=y\\\\ &\\Longrightarrow &\\mathbf{u} =\\left( \\begin{matrix}x\\\\ x\\end{matrix} \\right)  =x\\left( \\begin{matrix}1\\\\ 1\\end{matrix} \\right)  \\wedge x\\in \\mathbb{R} \\\\ &\\Longrightarrow &\\left( \\mathbb{R}^{2\\times 1} \\right)_{\\lambda =5+a}  =\\left< \\left\\{ \\left( \\begin{matrix}1\\\\ 1\\end{matrix} \\right)  \\right\\}  \\right>  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.153)$</p>\n",
    "\n",
    "De esta manera, tenemos la siguiente base ortonormal de autovectores,\n",
    "\n",
    "$$\\alpha =\\left\\{ \\frac{\\left( \\begin{matrix}1\\\\ 1\\end{matrix} \\right)  }{\\left\\Vert \\left( \\begin{matrix}1\\\\ 1\\end{matrix} \\right)  \\right\\Vert  } ,\\frac{\\left( \\begin{matrix}1\\\\ -1\\end{matrix} \\right)  }{\\left\\Vert \\left( \\begin{matrix}1\\\\ -1\\end{matrix} \\right)  \\right\\Vert  } \\right\\}  =\\left\\{ \\frac{1}{\\sqrt{2} } \\left( \\begin{matrix}1\\\\ 1\\end{matrix} \\right)  ,\\frac{1}{\\sqrt{2} } \\left( \\begin{matrix}1\\\\ -1\\end{matrix} \\right)  \\right\\}  =\\left\\{ \\left( \\begin{array}{r}\\displaystyle \\frac{\\sqrt{2} }{2} \\\\ \\displaystyle \\frac{\\sqrt{2} }{2} \\end{array} \\right)  ,\\left( \\begin{array}{r}\\displaystyle \\frac{\\sqrt{2} }{2} \\\\ -\\displaystyle \\frac{\\sqrt{2} }{2} \\end{array} \\right)  \\right\\}$$\n",
    "<p style=\"text-align: right;\">$(3.154)$</p>\n",
    "\n",
    "Con esta base en consideración, llevamos la cónica $C$ a su forma reducida. Para ello, procedemos a diagonalizar la matriz de la forma quadrática $\\mathbf{A}$ como sigue,\n",
    "\n",
    "$$\\left[ q\\right]^{\\mathbf{e} \\left( 2\\right)  }_{\\mathbf{e} \\left( 2\\right)  }  =\\mathbf{A} =\\mathbf{P} \\mathbf{D} \\mathbf{P}^{-1} =\\left[ I\\right]^{\\mathbf{e} \\left( 2\\right)  }_{\\alpha }  \\left[ q\\right]^{\\alpha }_{a}  \\left[ I\\right]^{\\alpha }_{\\mathbf{e} \\left( 2\\right)  }$$\n",
    "<p style=\"text-align: right;\">$(3.155)$</p>\n",
    "\n",
    "Donde,\n",
    "\n",
    "$$\\mathbf{P} =\\left[ I\\right]^{\\mathbf{e} \\left( 2\\right)  }_{\\alpha }  =\\left( \\begin{array}{rr}\\displaystyle \\frac{\\sqrt{2} }{2} &\\displaystyle \\frac{\\sqrt{2} }{2} \\\\ \\displaystyle \\frac{\\sqrt{2} }{2} &-\\displaystyle \\frac{\\sqrt{2} }{2} \\end{array} \\right)  \\wedge \\mathbf{A} =\\left[ q\\right]^{\\alpha }_{\\alpha }  =\\left( \\begin{matrix}5+a&0\\\\ 0&5-a\\end{matrix} \\right)  $$\n",
    "<p style=\"text-align: right;\">$(3.156)$</p>\n",
    "\n",
    "Así que,\n",
    "\n",
    "$$\\left( \\begin{matrix}5&a\\\\ a&5\\end{matrix} \\right)  =\\left( \\begin{array}{rr}\\displaystyle \\frac{\\sqrt{2} }{2} &\\displaystyle \\frac{\\sqrt{2} }{2} \\\\ \\displaystyle \\frac{\\sqrt{2} }{2} &-\\displaystyle \\frac{\\sqrt{2} }{2} \\end{array} \\right)  \\left( \\begin{matrix}5+a&0\\\\ 0&5-a\\end{matrix} \\right)  \\left( \\begin{array}{rr}\\displaystyle \\frac{\\sqrt{2} }{2} &\\displaystyle \\frac{\\sqrt{2} }{2} \\\\ \\displaystyle \\frac{\\sqrt{2} }{2} &-\\displaystyle \\frac{\\sqrt{2} }{2} \\end{array} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.157)$</p>\n",
    "\n",
    "Reemplazando en la ecuación (3.145),\n",
    "\n",
    "$$\\begin{array}{ll}&\\left( x,y\\right)  \\left( \\begin{matrix}5&a\\\\ a&5\\end{matrix} \\right)  \\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  +8\\sqrt{2} x=0\\\\ \\Longrightarrow &\\left( x,y\\right)  \\left( \\begin{array}{rr}\\displaystyle \\frac{\\sqrt{2} }{2} &\\displaystyle \\frac{\\sqrt{2} }{2} \\\\ \\displaystyle \\frac{\\sqrt{2} }{2} &-\\displaystyle \\frac{\\sqrt{2} }{2} \\end{array} \\right)  \\left( \\begin{matrix}5+a&0\\\\ 0&5-a\\end{matrix} \\right)  \\left( \\begin{array}{rr}\\displaystyle \\frac{\\sqrt{2} }{2} &\\displaystyle \\frac{\\sqrt{2} }{2} \\\\ \\displaystyle \\frac{\\sqrt{2} }{2} &-\\displaystyle \\frac{\\sqrt{2} }{2} \\end{array} \\right)  \\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  +8\\sqrt{2} x=0\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.158)$</p>\n",
    "\n",
    "Hacemos ahora el cambio de variables\n",
    "\n",
    "$$\\left( \\begin{matrix}u\\\\ v\\end{matrix} \\right)  =\\left[ I\\right]^{{}\\alpha }_{\\mathbf{e} \\left( 2\\right)  }  \\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.159)$</p>\n",
    "\n",
    "Luego tenemos,\n",
    "\n",
    "$$\\left( x,y\\right)  \\mathbf{A} \\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  =\\left( x,y\\right)  \\left[ I\\right]^{\\mathbf{e} \\left( 2\\right)  }_{\\alpha }  \\left[ q\\right]^{\\alpha }_{\\alpha }  \\left[ I\\right]^{\\alpha }_{\\mathbf{e} \\left( 2\\right)  }  \\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  =\\left( \\left[ I\\right]^{\\mathbf{e} \\left( 2\\right)  }_{\\alpha }  \\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  \\right)^{\\top }  \\left[ q\\right]^{\\alpha }_{\\alpha }  \\left[ I\\right]^{\\alpha }_{\\mathbf{e} \\left( 2\\right)  }  \\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  =\\left( \\begin{matrix}u\\\\ v\\end{matrix} \\right)^{\\top }  \\left[ q\\right]^{\\alpha }_{\\alpha }  \\left( \\begin{matrix}u\\\\ v\\end{matrix} \\right)  =\\left( u,v\\right)  \\left[ q\\right]^{\\alpha }_{\\alpha }  \\left( \\begin{matrix}u\\\\ v\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.160)$</p>\n",
    "\n",
    "Así que,\n",
    "\n",
    "$$\\left( u,v\\right)  \\left[ q\\right]^{\\alpha }_{\\alpha }  \\left( \\begin{matrix}u\\\\ v\\end{matrix} \\right)  =\\left( u,v\\right)  \\left( \\begin{matrix}5+a&0\\\\ 0&5-a\\end{matrix} \\right)  \\left( \\begin{matrix}u\\\\ v\\end{matrix} \\right)  =\\left( u\\left( 5+a\\right)  ,v\\left( 5-a\\right)  \\right)  =u^{2}\\left( 5+a\\right)  +v^{2}\\left( 5-a\\right)$$\n",
    "<p style=\"text-align: right;\">$(3.161)$</p>\n",
    "\n",
    "Por lo tanto, en el sistema $(u,v)$, los términos cuadráticos de la ecuación (3.145) se transforman en $q(u,v)=u^{2}\\left( 5+a\\right)  +v^{2}\\left( 5-a\\right)$. Para transformar el resto de la ecuación, notamos que\n",
    "\n",
    "$$\\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  =\\left[ I\\right]^{\\mathbf{e} \\left( 2\\right)  }_{\\alpha }  \\left( \\begin{matrix}u\\\\ v\\end{matrix} \\right)  =\\left( \\begin{array}{rr}\\displaystyle \\frac{\\sqrt{2} }{2} &\\displaystyle \\frac{\\sqrt{2} }{2} \\\\ \\displaystyle \\frac{\\sqrt{2} }{2} &-\\displaystyle \\frac{\\sqrt{2} }{2} \\end{array} \\right)  \\left( \\begin{matrix}u\\\\ v\\end{matrix} \\right)  =\\displaystyle \\frac{\\sqrt{2} }{2} \\left( \\begin{matrix}1&1\\\\ 1&-1\\end{matrix} \\right)  \\left( \\begin{matrix}u\\\\ v\\end{matrix} \\right)  =\\displaystyle \\frac{\\sqrt{2} }{2} \\left( \\begin{matrix}u+v\\\\ u-v\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.162)$</p>\n",
    "\n",
    "Así que $x=\\frac{\\sqrt{2} }{2} \\left( u+v\\right)  \\wedge y=\\frac{\\sqrt{2} }{2} \\left( u-v\\right)$. Reemplazando en el término de primer grado de la ecuación (3.145), obtenemos\n",
    "\n",
    "$$8\\sqrt{2} x=8\\sqrt{2} \\frac{\\sqrt{2} }{2} \\left( u+v\\right)  =8\\left( u+v\\right)$$\n",
    "<p style=\"text-align: right;\">$(3.163)$</p>\n",
    "\n",
    "Por lo tanto, en el sistema $(u,v)$, la cónica se expresa como\n",
    "\n",
    "$$\\left( 5+a\\right)  u^{2}+\\left( 5-a\\right)  v^{2}+8u+8v=0$$\n",
    "<p style=\"text-align: right;\">$(3.164)$</p>\n",
    "\n",
    "Tenemos entonces varios sub-casos que debemos considerar para cada valor que puede tomar el parámetro $a$ (recordemos que ya revisamos lo que ocurre cuando $a=0$).\n",
    "\n",
    "**(1)** Si $a=5$, la ecuación (3.164) se reduce a\n",
    "\n",
    "$$10u^{2}+8u+8v=0$$\n",
    "<p style=\"text-align: right;\">$(3.165)$</p>\n",
    "\n",
    "Completando cuadrados,\n",
    "\n",
    "$$\\begin{array}{lll}10u^{2}+8u+8v=0&\\Longleftrightarrow &u^{2}+\\displaystyle \\frac{4}{5} u+\\displaystyle \\frac{4}{5} v=0\\\\ &\\Longleftrightarrow &u^{2}+\\frac{4}{5} u+\\left( \\displaystyle \\frac{2}{5} \\right)^{2}  +\\displaystyle \\frac{4}{5} v-\\left( \\displaystyle \\frac{2}{5} \\right)^{2}  =0\\\\ &\\Longleftrightarrow &\\left( u+\\displaystyle \\frac{2}{5} \\right)^{2}  =-\\displaystyle \\frac{4}{5} \\left( v-\\displaystyle \\frac{2}{5} \\right)  \\\\ &\\Longleftrightarrow &\\left( u-\\left( -\\displaystyle \\frac{2}{5} \\right)  \\right)^{2}  =-\\displaystyle \\frac{4}{5} \\left( v-\\frac{2}{5} \\right)  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.166)$</p>\n",
    "\n",
    "Luego, para $a=5$, la ecuación (3.164) describe, en el sistema $(u,v)$, una parábola con foco en $F=(-\\frac{2}{5},\\frac{2}{5})$.\n",
    "\n",
    "**(2)** Si $a=-5$, la ecuación (3.164) se reduce a\n",
    "\n",
    "$$-10u^{2}+8u+8v=0$$\n",
    "<p style=\"text-align: right;\">$(3.168)$</p>\n",
    "\n",
    "De manera análoga al caso $a=5$, completando cuadrados, obtenemos que\n",
    "\n",
    "$$\\left( v-\\frac{2}{5} \\right)^{2}  =\\frac{4}{5} \\left( u-\\left( -\\frac{2}{5} \\right)  \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.169)$</p>\n",
    "\n",
    "Luego, para $a=-5$, la ecuación (3.164) describe, en el sistema $(u,v)$, una parábola con foco en $F=(\\frac{2}{5},-\\frac{2}{5})$, con orientación opuesta al caso $a=5$.\n",
    "\n",
    "**(3)** Si $-5<a<5$, entonces $5+a>0$ y $5-a>0$. Por lo tanto, desarrollamos la ecuación (3.164) completando cuadrados como sigue,\n",
    "\n",
    "$$\\begin{array}{lll}\\left( 5+a\\right)  u^{2}+\\left( 5-a\\right)  v^{2}+8u+8v=0&\\Longleftrightarrow &\\left( 5+a\\right)  \\left( u^{2}+\\displaystyle \\frac{8}{5+a} u\\right)  +\\left( 5-a\\right)  \\left( v^{2}+\\displaystyle \\frac{8}{5-a} v\\right)  =0\\\\ &\\Longleftrightarrow &\\left( 5+a\\right)  \\left( u+\\displaystyle \\frac{4}{5+a} \\right)^{2}  +\\left( 5-a\\right)  \\left( v+\\displaystyle \\frac{4}{5-a} \\right)^{2}  =16\\left( \\displaystyle \\frac{1}{5+a} +\\displaystyle \\frac{1}{5-a} \\right)  \\\\ &\\Longleftrightarrow &\\underbrace{\\left( 5+a\\right)  }_{>0} \\left( u+\\displaystyle \\frac{4}{5+a} \\right)^{2}  +\\underbrace{\\left( 5-a\\right)  }_{>0} \\left( v+\\displaystyle \\frac{4}{5-a} \\right)^{2}  =\\underbrace{\\displaystyle \\frac{160}{25-a^{2}} }_{>0} \\\\ &\\Longleftrightarrow &\\displaystyle \\frac{\\left( 5+a\\right)^{2}  \\left( 5-a\\right)  }{160} \\left( u+\\displaystyle \\frac{4}{5+a} \\right)^{2}  +\\displaystyle \\frac{\\left( 5+a\\right)^{2}  \\left( 5-a\\right)  }{160} \\left( v+\\displaystyle \\frac{4}{5-a} \\right)^{2}  =1\\\\ &\\Longleftrightarrow &\\displaystyle \\frac{\\left( u+\\frac{4}{5+a} \\right)^{2}  }{\\frac{160}{\\left( 5+a\\right)^{2}  \\left( 5-a\\right)  } } +\\frac{\\left( v+\\frac{4}{5-a} \\right)^{2}  }{\\frac{160}{\\left( 5-a\\right)^{2}  \\left( 5+a\\right)  } } =1\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.170)$</p>\n",
    "\n",
    "Por lo tanto, para $-5<a<5$, en el sistema $(u,v)$, la ecuación (3.164) describe una elipse con centro en $P=\\left( -\\frac{4}{5+a} ,-\\frac{4}{5-a} \\right)$ y semiejes $\\ell_{1} =\\frac{4\\sqrt{10} }{\\left( 5+a\\right)  \\sqrt{5-a} } \\wedge \\ell_{2} =\\frac{4\\sqrt{10} }{\\left( 5-a\\right)  \\sqrt{5+a} }$.\n",
    "\n",
    "**(4)** Si $a<-5$, entonces uno de los autovalores de la matriz $\\mathbf{A}$ asociada a la forma cuadrática $q(x,y)$ es positivo, mientras que el otro es negativo. Por lo tanto, la única diferencia con el caso (3) es que los sumandos involucrados en la ecuación (3.170) tienen signos opuestos. Por lo tanto, podemos escribir la ecuación de la cónica, en el sistema $(u,v)$, como\n",
    "\n",
    "$$\\frac{\\left( u+\\frac{4}{5+a} \\right)^{2}  }{\\frac{160}{\\left( 5+a\\right)^{2}  \\left( 5-a\\right)  } } -\\frac{\\left( v+\\frac{4}{5-a} \\right)^{2}  }{\\frac{160}{\\left( 5-a\\right)^{2}  \\left( 5+a\\right)  } } =1$$\n",
    "<p style=\"text-align: right;\">$(3.171)$</p>\n",
    "\n",
    "De este modo, para $a<-5$, la cónica resulta ser una hipérbola con centro en $P=\\left( -\\frac{4}{5+a} ,-\\frac{4}{5-a} \\right)$ y semiejes $\\ell_{1} =\\frac{4\\sqrt{10} }{\\left( 5+a\\right)  \\sqrt{5-a} } \\wedge \\ell_{2} =\\frac{4\\sqrt{10} }{\\left( 5-a\\right)  \\sqrt{5+a} }$.\n",
    "\n",
    "Para $a=3$, sabemos, por la ecuación (3.170), que la cónica es una elipse. Reemplazando en dicha ecuación $a=3$, obtenemos\n",
    "\n",
    "$$\\frac{\\left( u+\\frac{1}{2} \\right)^{2}  }{160/\\left( 64\\cdot 2\\right)  } +\\frac{\\left( v+2\\right)^{2}  }{160/\\left( 4\\cdot 8\\right)  } =1\\  \\Longleftrightarrow \\  \\frac{\\left( u+\\frac{1}{2} \\right)^{2}  }{5/4} +\\frac{\\left( v+2\\right)^{2}  }{5} =1$$\n",
    "<p style=\"text-align: right;\">$(3.172)$</p>\n",
    "\n",
    "La ecuación anterior describe, en el sistema $(u,v)$, una elipse con centro en el punto $P=\\left( -\\frac{1}{2} ,-2\\right)$, con semiejes $\\ell_{1} =\\frac{\\sqrt{5} }{2} \\wedge \\ell_{2} =\\sqrt{5}$. La elipse se encuentra rotada 45º con respecto al origen del sistema original $(x,y)$. Por lo tanto, en dicho sistema, el centro de la elipse se localiza en el punto $P=\\left( \\sqrt{2} \\left( -\\frac{1}{2} -2\\right)  ,\\sqrt{2} \\left( -\\frac{1}{2} +2\\right)  \\right)  =\\left( -\\frac{5}{2} \\sqrt{2} ,\\frac{3}{2} \\sqrt{2} \\right)$. El bosquejo de esta elipse se observa en la Fig. (3.6). ◼︎\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"figures/fig_3_6.png\" width=\"450\"></p>\n",
    "<p style=\"text-align: center;\">Fig. (3.6): Gráfico de la elipse descrita por la ecuación (3.172)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0828a320",
   "metadata": {},
   "source": [
    "## Descomposición de Cholesky.\n",
    "Como ya hemos revisado en las subsecciones anteriores, existen varias formas de descomponer una matriz. Puntualmente, la diagonalización y la descomposición por formas nos han permitido reformular matrices que cumplen con ciertas condiciones en términos de productos de tres matrices.\n",
    "\n",
    "En esta subsección, vamos a estudiar una nueva descomposición conocida como **factorización de Cholesky**, la que nos permite descomponer una matriz $\\mathbf{A}$ que cumple igualmente algunas condiciones en el producto $\\mathbf{A}=\\mathbf{L}\\mathbf{L}^{\\top}$, donde $\\mathbf{L}$ es una matriz triangular inferior cuya diagonal contiene números positivos. Sin embargo, para ello, es necesario que las matrices que queremos descomponer sean **definidas positivas**. Este concepto lo revisamos previamente, pero lo refinaremos a fin de considerar matrices definidas sobre cualquier cuerpo ($\\mathbb{R}$ o $\\mathbb{C}$).\n",
    "\n",
    "**<font color='blue'>Definición 3.18 – Matriz definida positiva (otra vez...):</font>** Sea $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$ una matriz hermítica. Diremos que $\\mathbf{A}$ es una **matriz definida positiva** si cumple con una (y, por tanto, con todas) de las siguientes formulaciones equivalentes:\n",
    "\n",
    "- **(F1):** Para todos los vectores no nulos $\\mathbf{u}\\in \\mathbb{K}^{n}$, se tiene que $\\mathbf{u}^{\\ast}\\mathbf{A}\\mathbf{u}$, donde $\\mathbf{u}^{\\ast}$ es la matriz transpuesta conjugada de $\\mathbf{u}$.\n",
    "- **(F2):** Todos los autovalores de $\\mathbf{A}$ son positivos.\n",
    "- **(F3):** La función $\\left< \\mathbf{u} ,\\mathbf{v} \\right>  =\\mathbf{v}^{\\ast } \\mathbf{A} \\mathbf{u}$ define un producto interno en $\\mathbb{K}^{n}$.\n",
    "\n",
    "Observamos, de las condiciones anteriores, que si $\\mathbf{A}$ es definida positiva, entonces $\\det(\\mathbf{A})>0$ y su inversa $\\mathbf{A}^{-1}$ es también definida positiva.\n",
    "\n",
    "Estamos pues en condiciones de formular el siguiente teorema.\n",
    "\n",
    "**<font color='crimson'>Teorema 3.14 – Descomposición de Cholesky:</font>** *Sea $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$ una matriz simétrica y definida positiva. Entonces $\\mathbf{A}$ puede ser expresada como $\\mathbf{A}=\\mathbf{L}\\mathbf{L}^{\\top}$, donde $\\mathbf{L}$ es una matriz triangular inferior con elementos positivos en su diagonal principal. Es decir,*\n",
    "\n",
    "$$\\mathbf{A} =\\left( \\begin{matrix}a_{11}&a_{12}&\\cdots &a_{1n}\\\\ a_{21}&a_{22}&\\cdots &a_{2n}\\\\ \\vdots &\\vdots &\\ddots &\\vdots \\\\ a_{n1}&a_{n2}&\\cdots &a_{nn}\\end{matrix} \\right)  =\\left( \\begin{matrix}l_{11}&0&\\cdots &0\\\\ l_{21}&l_{22}&\\cdots &0\\\\ \\vdots &\\vdots &\\ddots &\\vdots \\\\ l_{n1}&l_{n2}&\\cdots &l_{nn}\\end{matrix} \\right)  \\left( \\begin{matrix}l_{11}&l_{21}&\\cdots &l_{n1}\\\\ 0&l_{22}&\\cdots &l_{n2}\\\\ \\vdots &\\vdots &\\ddots &\\vdots \\\\ 0&0&\\cdots &l_{nn}\\end{matrix} \\right)  =\\mathbf{L} \\mathbf{L}^{\\top }$$\n",
    "<p style=\"text-align: right;\">$(3.173)$</p>\n",
    "\n",
    "*La matriz $\\mathbf{L}$ se denomina **factor de Cholesky** de $\\mathbf{A}$, y es único.* ◆\n",
    "\n",
    "**Ejemplo 3.18:** Consideremos una matriz simétrica y definida positiva $\\mathbf{A}\\in \\mathbb{R}^{3\\times 3}$, definida como $\\mathbf{A} =\\left\\{ a_{ij}\\right\\}$, tal que\n",
    "\n",
    "$$\\mathbf{A} =\\left( \\begin{matrix}a_{11}&a_{12}&a_{13}\\\\ a_{21}&a_{22}&a_{23}\\\\ a_{31}&a_{32}&a_{33}\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.174)$</p>\n",
    "\n",
    "Vamos a buscar una fórmula para determinar explícitamente los elementos del factor de Cholesky $\\mathbf{L}$ de $\\mathbf{A}$. En efecto, $\\mathbf{A}=\\mathbf{L}\\mathbf{L}^{\\top}$, donde\n",
    "\n",
    "$$\\mathbf{L} \\mathbf{L}^{\\top } =\\left( \\begin{matrix}l_{11}&0&0\\\\ l_{21}&l_{22}&0\\\\ l_{31}&l_{32}&l_{33}\\end{matrix} \\right)  \\left( \\begin{matrix}l_{11}&l_{21}&l_{32}\\\\ 0&l_{22}&l_{32}\\\\ 0&0&l_{33}\\end{matrix} \\right)  =\\left( \\begin{matrix}l^{2}_{11}&l_{21}l_{11}&l_{31}l_{11}\\\\ l_{21}l_{31}&l^{2}_{21}+l^{2}_{22}&l_{31}l_{21}+l_{32}l_{22}\\\\ l_{31}l_{11}&l_{31}l_{21}+l_{32}l_{22}&l^{2}_{31}+l^{2}_{32}+l^{2}_{33}\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.175)$</p>\n",
    "\n",
    "Comparando el lado derecho de la ecuación (3.174) con el lado derecho de la ecuación (3.175), podemos observar que existe un sencillo patrón en los elementos de la diagonal principal de $\\mathbf{L}$. En efecto,\n",
    "\n",
    "$$l_{11}=\\sqrt{a_{11}} \\  ;\\  l_{22}=\\sqrt{a_{22}-l^{2}_{21}} \\  ;\\  l_{33}=\\sqrt{a_{33}-\\left( l^{2}_{31}+l^{2}_{32}\\right)  }$$\n",
    "<p style=\"text-align: right;\">$(3.176)$</p>\n",
    "\n",
    "Similarmente, para los elementos ubicados debajo de la diagonal principal de $\\mathbf{L}$ ($l_{ij}$, donde $i>j$), también se observa un patrón,\n",
    "\n",
    "$$l_{21}=\\frac{1}{l_{11}} a_{21}\\  ;\\  l_{31}=\\frac{1}{l_{11}} a_{31}\\  ;\\  l_{32}=\\frac{1}{l_{22}} \\left( a_{32}-l_{31}l_{21}\\right)$$\n",
    "<p style=\"text-align: right;\">$(3.177)$</p>\n",
    "\n",
    "De esta manera, en términos más generales, para una matriz $\\mathbf{A}\\in \\mathbb{R}^{n\\times n}$, podemos probar (por inducción) que:\n",
    "\n",
    "- Para los elementos de la diagonal principal: $l_{ii}=a_{ii}-\\sum^{i-1}_{k=1} l^{2}_{ik}$.\n",
    "- Para el resto de los elementos: $l_{ij}=\\frac{1}{l_{jj}} \\left( a_{ij}-\\sum^{j-1}_{k=1} l_{ik}l_{jk}\\right)  ;\\forall i>j$.\n",
    "◼︎\n",
    "\n",
    "**Ejemplo 3.19:** En <font color='purple'>Numpy</font> es posible obtener una descomposición de Cholesky fácilmente por medio de la función `cholesky()`, cuya dependencia es igualmente el módulo `numpy.linalg`. Esta función simplemente requiere, como argumento, un arreglo bidimensional que represente a una matriz definida positiva. Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e361f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos nuestra matriz A.\n",
    "A = np.array([\n",
    "    [2, 2, 1],\n",
    "    [2, 3, 0],\n",
    "    [1, 0, 2],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48b5291d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 2, 1],\n",
       "       [2, 3, 0],\n",
       "       [1, 0, 2]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mostramos en pantalla esta matriz.\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c210d1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos su factor de Cholesky.\n",
    "L = linalg.cholesky(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51bf0431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.41421356,  0.        ,  0.        ],\n",
       "       [ 1.41421356,  1.        ,  0.        ],\n",
       "       [ 0.70710678, -1.        ,  0.70710678]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mostramos la matriz L en pantalla.\n",
    "L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e5a765",
   "metadata": {},
   "source": [
    "Es posible verificar que la descompisición obtenida efectivamente genera la matriz original `A`, realizando la operación $\\mathbf{L}\\mathbf{L}^{\\top}$. En efecto,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85bda3ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verificamos que la descomposición efectivamente genera la matriz A.\n",
    "np.allclose(A, L @ L.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa103af3",
   "metadata": {},
   "source": [
    "Por supuesto, la factorización de Cholesky exige que las matrices a descomponer sean definidas positivas. En <font color='purple'>Numpy</font> esta no es una excepción, y se levantará un error de tipo `LinAlgError` si esta condición no se cumple al intentar implementar una decomposición de Cholesky:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e816fb25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3,  2,  1,  1],\n",
       "       [-2,  5, -5, -2],\n",
       "       [-5, -2,  5, -2],\n",
       "       [ 4, -1,  2, -1]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definimos una matriz arbitraria.\n",
    "B = rng.integers(low=-6, high=6, size=(4, 4))\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6bfc5d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix is not positive definite\n"
     ]
    }
   ],
   "source": [
    "# Si intentamos calcular su descomposición de Cholesky, se levantará un error, ya que no es\n",
    "# definida positiva.\n",
    "try:\n",
    "    L = linalg.cholesky(B)\n",
    "except linalg.LinAlgError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1b4a83",
   "metadata": {},
   "source": [
    "◼︎\n",
    "\n",
    "La descomposición de Cholesky es una herramienta importante en el cálculo numérico subyacente a los algoritmos de aprendizaje. Las matrices simétricas y definidas positivas, con frecuencia, requieren de intensivas manipulaciones para llegar a valores de interés en muchos problemas estadísticos. Por ejemplo, la *matriz de covarianza* asociada a un *vector de variables aleatorias distribuidas normalmente* cumple con estas condiciones. La descomposición de Cholesky de esta matriz de covarianza nos permite generar muestreos desde una distribución *Gaussiana*. También nos permite construir transformaciones lineales de variables aleatorias, lo que se explota enormemente al calcular gradientes en modelos estocásticos de gran profundidad, tales como los *auto-enconders* de tipo variacionales (Jimenez Rezende et al., 2014; Kingma & Weilling., 2014). \n",
    "\n",
    "La descomposición de Cholesky también nos permite calcular determinantes de manera eficiente. Dada la factorización $\\mathbf{A}=\\mathbf{L}\\mathbf{L}^{\\top}$, sabemos que $\\det(\\mathbf{A})=\\det(\\mathbf{L}) \\det(\\mathbf{L}^{\\top})=\\det(\\mathbf{L})^{2}$. Ya que $\\mathbf{L}$ es una matriz triangular, el determinante de $\\mathbf{L}$ es simplemente el producto de los elementos de la diagonal principal. Por esta razón, muchos paquetes computacionales (y, en varios casos, librerías de Python) utilizan la descomposición de Cholesky para facilitar cálculos que involucran matrices (siempre que sean definidas positivas)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61825a1",
   "metadata": {},
   "source": [
    "## Descomposición QR.\n",
    "Otro tipo de descomposición muy común en álgebra lineal y, puntualmente, en machine learning (sobretodo por el aumento en la eficiencia de ciertos cálculos matriciales), corresponde a la **descomposición QR**. Tal descomposición es válida para cualquier matriz $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$, y es tal que $\\mathbf{A}=\\mathbf{Q}\\mathbf{R}$, donde $\\mathbf{Q}$ es una matriz ortogonal o unitaria (recordemos que toda matriz $\\mathbf{Q}\\in \\mathbb{K}^{n\\times n}$ que satisface la expresión $\\mathbf{Q}^{\\top}\\mathbf{Q}=\\mathbf{I}_{n}$ se denomina ortogonal o unitaria) y $\\mathbf{R}$ es una matriz triangular superior. Si $\\mathbf{A}$ es, además, no singular (y, por tanto, invertible), entonces la descomposición QR es única siempre que los elementos de la diagonal principal de $\\mathbf{R}$ sean todos positivos.\n",
    "\n",
    "La construcción de la factorización QR puede hacerse de varias formas, siendo la más usual el uso de proyecciones ortogonales y del proceso de ortogonalización de Gram-Schmidt. Para ello, consideremos este último proceso (que fue desarollado en el teorema (2.2)) aplicado a las columnas de una matriz no singular, digamos $\\mathbf{A}=(\\mathbf{a}_{1},...,\\mathbf{a}_{n})$, donde $\\mathbf{a}_{j} =\\left\\{ a_{ij}\\right\\}$ para $i=1,...,n$, y definimos el producto interno usual $\\left< \\mathbf{v} ,\\mathbf{w} \\right>  =\\mathbf{v}^{\\ast } \\mathbf{w}$. Definimos además la proyección ortogonal\n",
    "\n",
    "$$\\pi_{\\mathbf{u} } \\left( \\mathbf{a} \\right)  =\\frac{\\left< \\mathbf{u} ,\\mathbf{a} \\right>  }{\\left< \\mathbf{u} ,\\mathbf{u} \\right>  } \\mathbf{u} =\\frac{\\left< \\mathbf{u} ,\\mathbf{a} \\right>  }{\\left\\Vert \\mathbf{u} \\right\\Vert^{2}  } \\mathbf{u}$$\n",
    "<p style=\"text-align: right;\">$(3.178)$</p>\n",
    "\n",
    "Entonces, usando el método de Gram-Schmidt, obtenemos\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{u}_{1} &=&\\mathbf{a}_{1} \\  ;\\  \\mathbf{e}_{1} =\\frac{\\mathbf{u}_{1} }{\\left\\Vert \\mathbf{u}_{1} \\right\\Vert^{2}  } \\\\ \\mathbf{u}_{2} &=&\\mathbf{a}_{2} -\\pi_{\\mathbf{u}_{1} } \\left( \\mathbf{a}_{2} \\right)  \\  ;\\  \\mathbf{e}_{2} =\\frac{\\mathbf{u}_{2} }{\\left\\Vert \\mathbf{u}_{2} \\right\\Vert^{2}  } \\\\ \\mathbf{u}_{3} &=&\\mathbf{a}_{3} -\\pi_{\\mathbf{u}_{1} } \\left( \\mathbf{a}_{3} \\right)  -\\pi_{\\mathbf{u}_{2} } \\left( \\mathbf{a}_{3} \\right)  \\  ;\\  \\mathbf{e}_{3} =\\frac{\\mathbf{u}_{3} }{\\left\\Vert \\mathbf{u}_{3} \\right\\Vert^{2}  } \\\\ \\vdots &\\vdots &\\vdots \\\\ \\mathbf{u}_{k} &=&\\mathbf{a}_{k} -\\sum^{k-1}_{j=1} \\pi_{\\mathbf{u}_{j} } \\left( \\mathbf{a}_{k} \\right)  \\  ;\\  \\mathbf{e}_{k} =\\frac{\\mathbf{u}_{k} }{\\left\\Vert \\mathbf{u}_{k} \\right\\Vert^{2}  } \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.179)$</p>\n",
    "\n",
    "Podemos expresar las columnas $\\mathbf{a}_{k}$ sobre nuestra nueva base ortonormal, recién calculada, como\n",
    "\n",
    "$$\\mathbf{R} =\\left( \\begin{array}{rrrrr}\\left< \\mathbf{e}_{1} ,\\mathbf{a}_{1} \\right>  &\\left< \\mathbf{e}_{1} ,\\mathbf{a}_{2} \\right>  &\\left< \\mathbf{e}_{1} ,\\mathbf{a}_{3} \\right>  &\\cdots &\\left< \\mathbf{e}_{1} ,\\mathbf{a}_{n} \\right>  \\\\ 0&\\left< \\mathbf{e}_{2} ,\\mathbf{a}_{2} \\right>  &\\left< \\mathbf{e}_{2} ,\\mathbf{a}_{3} \\right>  &\\cdots &\\left< \\mathbf{e}_{2} ,\\mathbf{a}_{n} \\right>  \\\\ 0&0&\\left< \\mathbf{e}_{3} ,\\mathbf{a}_{3} \\right>  &\\cdots &\\left< \\mathbf{e}_{3} ,\\mathbf{a}_{n} \\right>  \\\\ \\vdots &\\vdots &\\vdots &\\ddots &\\vdots \\\\ 0&0&0&\\cdots &\\left< \\mathbf{e}_{n} ,\\mathbf{a}_{n} \\right>  \\end{array} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.180)$</p>\n",
    "\n",
    "Donde $\\left< \\mathbf{e}_{i} ,\\mathbf{a}_{k} \\right>  =\\left\\Vert \\mathbf{u}_{i} \\right\\Vert$. Podemos escribir el desarrollo (3.180) en forma matricial como $\\mathbf{A}=\\mathbf{Q}\\mathbf{R}$, donde $\\mathbf{Q}=(\\mathbf{e}_{1},...,\\mathbf{e}_{n})$ y\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{a}_{1} &=&\\left< \\mathbf{e}_{1} ,\\mathbf{a}_{1} \\right>  \\mathbf{e}_{1} \\\\ \\mathbf{a}_{2} &=&\\left< \\mathbf{e}_{1} ,\\mathbf{a}_{2} \\right>  \\mathbf{e}_{1} +\\left< \\mathbf{e}_{2} ,\\mathbf{a}_{2} \\right>  \\mathbf{e}_{2} \\\\ \\mathbf{a}_{3} &=&\\left< \\mathbf{e}_{1} ,\\mathbf{a}_{3} \\right>  \\mathbf{e}_{1} +\\left< \\mathbf{e}_{2} ,\\mathbf{a}_{2} \\right>  \\mathbf{e}_{2} +\\left< \\mathbf{e}_{3} ,\\mathbf{a}_{3} \\right>  \\mathbf{e}_{3} \\\\ \\vdots &\\vdots &\\vdots \\\\ \\mathbf{a}_{k} &=&\\sum^{k}_{j=1} \\left< \\mathbf{e}_{j} ,\\mathbf{a}_{k} \\right>  \\mathbf{e}_{j} \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.181)$</p>\n",
    "\n",
    "**Ejemplo 3.20:** Consideremos la matriz\n",
    "\n",
    "$$\\mathbf{A} =\\left( \\begin{matrix}1&1&0\\\\ 1&0&1\\\\ 0&1&1\\end{matrix} \\right)  \\in \\mathbb{R}^{3\\times 3}$$\n",
    "<p style=\"text-align: right;\">$(3.182)$</p>\n",
    "\n",
    "Notemos que podemos reescribir la matriz $\\mathbf{A}$ con un arreglo de vectores columna del tipo $\\mathbf{A}=(\\mathbf{a}_{1},\\mathbf{a}_{2},\\mathbf{a}_{3})$, donde\n",
    "\n",
    "$$\\mathbf{a}_{1} =\\left( \\begin{matrix}1\\\\ 1\\\\ 0\\end{matrix} \\right)  \\  ;\\  \\mathbf{a}_{2} =\\left( \\begin{matrix}1\\\\ 0\\\\ 1\\end{matrix} \\right)  \\  ;\\  \\mathbf{a}_{3} =\\left( \\begin{matrix}0\\\\ 1\\\\ 1\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.183)$</p>\n",
    "\n",
    "Dado que estos vectores son linealmente independientes en $\\mathbb{R}^{3}$, podemos construir un conjunto ortonormal de los mismos mediante el proceso de Gram-Schmidt. De esta manera tenemos que\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{u}_{1} &=&\\left( \\begin{matrix}1\\\\ 1\\\\ 0\\end{matrix} \\right)  \\\\ \\mathbf{e}_{1} &=&\\displaystyle \\frac{\\left( \\begin{matrix}1\\\\ 1\\\\ 0\\end{matrix} \\right)  }{\\left\\Vert \\left( \\begin{matrix}1\\\\ 1\\\\ 0\\end{matrix} \\right)  \\right\\Vert  } =\\displaystyle \\frac{1}{\\sqrt{2} } \\left( \\begin{matrix}1\\\\ 1\\\\ 0\\end{matrix} \\right)  =\\left( \\begin{array}{r}\\displaystyle \\frac{1}{\\sqrt{2} } \\\\ \\displaystyle \\frac{1}{\\sqrt{2} } \\\\ 0\\end{array} \\right)  \\\\ \\mathbf{u}_{2} &=&\\left( \\begin{matrix}1\\\\ 0\\\\ 1\\end{matrix} \\right)  -\\displaystyle \\frac{1}{\\sqrt{2} } \\left( \\begin{array}{r}\\displaystyle \\frac{1}{\\sqrt{2} } \\\\ \\displaystyle \\frac{1}{\\sqrt{2} } \\\\ 0\\end{array} \\right)  =\\left( \\begin{array}{r}\\displaystyle \\frac{1}{2} \\\\ -\\displaystyle \\frac{1}{2} \\\\ 1\\end{array} \\right)  \\\\ \\mathbf{e}_{2} &=&\\displaystyle \\frac{\\left( \\begin{array}{r}\\displaystyle \\frac{1}{2} \\\\ -\\displaystyle \\frac{1}{2} \\\\ 1\\end{array} \\right)  }{\\left\\Vert \\left( \\begin{array}{r}\\displaystyle \\frac{1}{2} \\\\ -\\displaystyle \\frac{1}{2} \\\\ 1\\end{array} \\right)  \\right\\Vert  } =\\displaystyle \\frac{1}{\\sqrt{3/2} } \\left( \\begin{array}{r}\\displaystyle \\frac{1}{2} \\\\ -\\displaystyle \\frac{1}{2} \\\\ 1\\end{array} \\right)  =\\left( \\begin{array}{r}\\displaystyle \\frac{1}{\\sqrt{6} } \\\\ -\\displaystyle \\frac{1}{\\sqrt{6} } \\\\ \\displaystyle \\frac{2}{\\sqrt{6} } \\end{array} \\right)  \\\\ \\mathbf{u}_{3} &=&\\left( \\begin{matrix}1\\\\ 0\\\\ 1\\end{matrix} \\right)  -\\displaystyle \\frac{1}{\\sqrt{2} } \\left( \\begin{array}{r}\\displaystyle \\frac{1}{\\sqrt{2} } \\\\ \\displaystyle \\frac{1}{\\sqrt{2} } \\\\ 0\\end{array} \\right)  -\\displaystyle \\frac{1}{\\sqrt{6} } \\left( \\begin{array}{r}\\displaystyle \\frac{1}{\\sqrt{6} } \\\\ -\\displaystyle \\frac{1}{\\sqrt{6} } \\\\ \\displaystyle \\frac{2}{\\sqrt{6} } \\end{array} \\right)  =\\left( \\begin{array}{r}-\\displaystyle \\frac{1}{\\sqrt{3} } \\\\ \\displaystyle \\frac{1}{\\sqrt{3} } \\\\ \\displaystyle \\frac{1}{\\sqrt{3} } \\end{array} \\right)  \\\\ \\mathbf{e}_{3} &=&\\displaystyle \\frac{\\left( \\begin{array}{r}-\\displaystyle \\frac{1}{\\sqrt{3} } \\\\ \\displaystyle \\frac{1}{\\sqrt{3} } \\\\ \\displaystyle \\frac{1}{\\sqrt{3} } \\end{array} \\right)  }{\\left\\Vert \\left( \\begin{array}{r}-\\displaystyle \\frac{1}{\\sqrt{3} } \\\\ \\displaystyle \\frac{1}{\\sqrt{3} } \\\\ \\displaystyle \\frac{1}{\\sqrt{3} } \\end{array} \\right)  \\right\\Vert  } =\\left( \\begin{array}{r}-\\displaystyle \\frac{1}{\\sqrt{3} } \\\\ \\displaystyle \\frac{1}{\\sqrt{3} } \\\\ \\displaystyle \\frac{1}{\\sqrt{3} } \\end{array} \\right)  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.184)$</p>\n",
    "\n",
    "De esta manera,\n",
    "\n",
    "$$\\mathbf{Q} =\\left( \\mathbf{e}_{1} ,\\mathbf{e}_{2} ,\\mathbf{e}_{2} \\right)  =\\left( \\begin{array}{rrr}\\frac{1}{\\sqrt{2} } &\\frac{1}{\\sqrt{6} } &-\\frac{1}{\\sqrt{3} } \\\\ \\frac{1}{\\sqrt{2} } &-\\frac{1}{\\sqrt{6} } &\\frac{1}{\\sqrt{3} } \\\\ 0&\\frac{2}{\\sqrt{6} } &\\frac{1}{\\sqrt{3} } \\end{array} \\right)  \\wedge \\mathbf{R} =\\left( \\begin{matrix}\\left< \\mathbf{u}_{1} ,\\mathbf{e}_{1} \\right>  &\\left< \\mathbf{u}_{2} ,\\mathbf{e}_{1} \\right>  &\\left< \\mathbf{u}_{3} ,\\mathbf{e}_{1} \\right>  \\\\ 0&\\left< \\mathbf{u}_{2} ,\\mathbf{e}_{2} \\right>  &\\left< \\mathbf{u}_{3} ,\\mathbf{e}_{2} \\right>  \\\\ 0&0&\\left< \\mathbf{u}_{3} ,\\mathbf{e}_{3} \\right>  \\end{matrix} \\right)  =\\left( \\begin{array}{rrr}\\frac{2}{\\sqrt{2} } &\\frac{1}{\\sqrt{2} } &\\frac{1}{\\sqrt{2} } \\\\ 0&\\frac{3}{\\sqrt{6} } &\\frac{1}{\\sqrt{6} } \\\\ 0&0&\\frac{2}{\\sqrt{3} } \\end{array} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.185)$</p>\n",
    "◼︎\n",
    "\n",
    "**Ejemplo 3.21:** En <font color='purple'>Numpy</font> existe igualmente una implementación de la descomposición QR mediante el uso de la función `qr()`, también dependiente del módulo `numpy.linalg`. Sólo se requiere como entrada la matriz de interés:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d71c0ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos una matriz arbitraria de 4x4.\n",
    "A = rng.integers(low=-5, high=5, size=(4, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec40e0c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3,  2,  4, -3],\n",
       "       [ 2, -3,  2,  2],\n",
       "       [-1,  2, -3, -5],\n",
       "       [-5, -1,  4, -4]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mostramos nuestra matriz en pantalla.\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a8c3e032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos la descomposición QR de la matriz A.\n",
    "Q, R = linalg.qr(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "535fc5f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.48038446,  0.32771179,  0.7764457 , -0.24283292],\n",
       "       [ 0.32025631, -0.63614643,  0.26310283, -0.65079223],\n",
       "       [-0.16012815,  0.44337478, -0.51100417, -0.71878544],\n",
       "       [-0.80064077, -0.5397606 , -0.25842545,  0.02913995]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mostramos en pantalla la matriz Q.\n",
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee904bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.244998  , -1.44115338, -4.00320385,  6.08486984],\n",
       "       [ 0.        ,  3.99037303, -3.45061243, -2.31325973],\n",
       "       [ 0.        ,  0.        ,  4.13129917,  1.78559124],\n",
       "       [ 0.        ,  0.        ,  0.        ,  2.90428173]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mostramos en pantalla la matriz R.\n",
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "270d1bd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comprobamos que la descomposición así definida efectivamente genera la matriz A.\n",
    "np.allclose(A, Q @ R)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a195aeb7",
   "metadata": {},
   "source": [
    "◼︎\n",
    "\n",
    "## Descomposición en valores singulares.\n",
    "Procedemos ahora a revisar el último tópico relativo a la descomposición matricial con uno de los resultados más importantes del álgebra lineal, conocido como **descomposición en valores singulares** (comúnmente llamada SVD, del inglés *singular value decomposition*). Corresponde a una generalización de la descomposición por autovalores y autovectores (diagonalización) y permite factorizar cualquier matriz $\\mathbf{A}\\in \\mathbb{K}^{m\\times n}$, incluso aunque $\\mathbf{A}$ no sea una matriz cuadrada ($m\\neq n$).\n",
    "\n",
    "Específicamente, la descomposición SVD de una matriz $\\mathbf{A}\\in \\mathbb{K}^{m\\times n}$ es una factorización de la forma $\\mathbf{A}=\\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^{\\ast}$, donde $\\mathbf{U}$ es una matriz ortogonal de $m\\times m$ (es decir, $\\mathbf{U}^{\\ast}=\\mathbf{U}^{-1}$) con elementos no negativos en su diagonal, $\\mathbf{V} $ es una matriz ortogonal de $n\\times n$ y $\\mathbf{V}^{\\ast}$ es la matriz transpuesta conjugada de $\\mathbf{V}$. Si $\\mathbf{A}$ tiene solo elementos en $\\mathbb{R}$, la descomposición SVD se reduce a una del tipo $\\mathbf{A}=\\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^{\\top}$.\n",
    "\n",
    "Los elementos diagonales de $\\Sigma$, que denotamos como $\\sigma_{i} =\\left\\{ \\psi_{ij} \\right\\}$, son determinadas unívocamente por la matriz $\\mathbf{A}$ y son llamados **valores singulares** de $\\mathbf{A}$. El número de valores singulares no nulos es igual al rango de la matriz $\\mathbf{A}$. Las columnas de $\\mathbf{U}$ y las columnas de $\\mathbf{V}$ son llamadas **vectores singulares por la izquierda (siniestrales)** y **vectores singulares por la derecha (dextrales)**, respectivamente, de $\\mathbf{A}$. Entre ambos constituyen dos conjuntos de bases ortonormales $\\left\\{ \\mathbf{u}_{1} ,...,\\mathbf{u}_{m} \\right\\}  \\wedge \\left\\{ \\mathbf{v}_{1} ,...,\\mathbf{v}_{m} \\right\\}$, lo que permite que la descomposición SVD pueda escribirse de forma explícita como\n",
    "\n",
    "$$\\mathbf{A} =\\sum^{r}_{i=1} \\sigma_{i} \\mathbf{u}_{i} \\mathbf{v}^{\\ast }_{i} \\  ;\\  r\\leq \\rho \\left( \\mathbf{A} \\right)  =\\min \\left\\{ m,n\\right\\}$$\n",
    "<p style=\"text-align: right;\">$(3.186)$</p>\n",
    "\n",
    "donde $\\rho(\\mathbf{A})$ es el rango de $\\mathbf{A}$.\n",
    "\n",
    "La descomposición SVD no es única. Sin embargo, siempre es posible escoger una tal que los valores singulares $\\sigma_{i}$ se presenten en un orden decreciente. En este caso, la matriz $\\mathbf{\\Sigma}$ sí es única y es determinada completamente por $\\mathbf{A}$ (pero no $\\mathbf{U}$ ni $\\mathbf{V}$).\n",
    "\n",
    "Antes de comenzar a estudiar en detalle esta descomposición, vamos a definirla por medio de un teorema (aunque, para efectos prácticos, trabajaremos con matrices cuyos elementos son números reales).\n",
    "\n",
    "**<font color='crimson'>Teorema 3.15 – Descomposición en valores singulares:</font>** *Sea $\\mathbf{A}\\in \\mathbb{R}^{m\\times n}$ una matriz rectangular de rango $\\rho(\\mathbf{A})\\in [0, \\min \\left\\{ m,n\\right\\}]$. La descomposición en valores singulares de $\\mathbf{A}$ tiene la forma (esquemática)*\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"figures/svd_illustration.png\" width=\"450\"></p>\n",
    "<p style=\"text-align: right;\">$(3.187)$</p>\n",
    "\n",
    "*Donde $\\mathbf{U}\\in \\mathbb{R}^{m\\times m}$ es una matriz ortogonal con columnas $\\mathbf{u}_{i}, i=1,...,m$, y $\\mathbf{V}\\in \\mathbb{R}^{n\\times n}$ es una matriz ortogonal con columnas $\\mathbf{v}_{j}, j=1,...,n$. La matriz de valores singulares $\\mathbf{\\Sigma } =\\left\\{ \\psi_{ij} \\right\\}  \\in \\mathbb{R}^{m\\times n}$ tiene las mismas dimensiones que $\\mathbf{A}$, y es tal que $psi_{ij}=0$ para $i\\neq j$.* ◆\n",
    "\n",
    "Como comentamos en un principio, la matriz de valores singulares $\\mathbf{\\Sigma}$, bajo ciertas condiciones, es única, pero requiere cierto nivel de atención. Observemos que $\\mathbf{\\Sigma}$ tiene la misma dimensión que $\\mathbf{A}$. Esto significa que $\\mathbf{\\Sigma}$ contiene una submatriz diagonal con los valores singulares de $\\mathbf{A}$ con un relleno de ceros en las filas o columnas siguientes (lo que, en computación científica, se conoce como **zero padding**). Específicamente, si $m>n$, entonces la matriz $\\mathbf{\\Sigma}$ tiene una estructura diagonal hasta la fila $n$ y luego consiste de vectores fila del tipo $\\mathbf{0}^{\\top}$ hasta la fila $m$. Es decir,\n",
    "\n",
    "$$\\mathbf{\\Sigma } =\\left( \\begin{matrix}\\sigma_{1} &0&\\cdots &0\\\\ 0&\\sigma_{2} &\\cdots &0\\\\ \\vdots &\\vdots &\\ddots &\\vdots \\\\ 0&0&\\cdots &\\sigma_{n} \\\\ \\vdots &\\vdots &\\ddots &\\vdots \\\\ 0&0&\\cdots &0\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.188)$</p>\n",
    "\n",
    "Por otro lado, si $m<n$, entonces la matriz $\\mathbf{\\Sigma}$ tiene una estructura diagonal hasta la columna $m$ y nula hasta la posición $n$. Es decir,\n",
    "\n",
    "$$\\mathbf{\\Sigma } =\\left( \\begin{matrix}\\sigma_{1} &0&\\cdots &0&\\cdots &0\\\\ 0&\\sigma_{2} &\\cdots &0&\\cdots &0\\\\ \\vdots &\\vdots &\\ddots &\\vdots &\\ddots &\\vdots \\\\ 0&0&\\cdots &\\sigma_{m} &\\cdots &0\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.189)$</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b03a2b71",
   "metadata": {},
   "source": [
    "### Una primera explicación geométrica.\n",
    "La descomposición SVD nos ofrece una interpretación geométrica intuitiva y muy interesante que nos permite describir este procedimiento antes de entrar en el detalle matemático del mismo. En lo que sigue, discutiremos esta descomposición en base a una secuencia de transformaciones lineales aplicadas sobre ciertas bases del conjunto de matrices de interés. Luego, por medio de un ejemplo, aplicaremos estas transformaciones a un conjunto de vectores en ℝ^2, lo que nos permitirá observar el efecto de cada una, en términos geométricos, de manera más clara.\n",
    "\n",
    "La descomposición SVD de una matriz puede ser interpretada como una que involucra a tres transformaciones lineales asociadas a las correspondientes matrices, digamos del tipo $T:\\mathbb{R}^{n}\\longrightarrow \\mathbb{R}^{n}$, como se muestra en la Fig. (3.7). En términos muy generales, la descomposición SVD realiza un cambio de base mediante el uso de la matriz $\\mathbf{V}^{\\ast}$, seguido de un escalamiento y aumento (o reducción) de su dimensión mediante el uso de la matriz de valores singulares $\\mathbf{\\Sigma}$. Finalmente, se realiza un segundo cambio de base por medio de la matriz $\\mathbf{U}$. Por supuesto, esta secuencia de operaciones guarda mucho más detalles matemáticos que revisaremos más adelante. Pero antes, refinaremos esta interpretación geométrica poniéndole nombre a nuestras operaciones.\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"figures/fig_3_7.png\" width=\"600\"></p>\n",
    "<p style=\"text-align: center;\">Fig. (3.7): Ilustración de la descomposición de valores singulares $\\mathbf{A}=\\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^{\\ast}$ de una matriz de $2\\times 2$ con elementos en $\\mathbb{R}$\n",
    "\n",
    "Asumamos que disponemos de una matriz de cambio de base asociada a una transformación lineal $T:\\mathbb{R}^{n}\\longrightarrow \\mathbb{R}^{m}$ con respecto a las bases canónicas $\\mathbf{e}(n)$ y $\\mathbf{e}(m)$, respectivamente. Además, consideremos dos bases más, digamos $\\alpha$ y $\\beta$, para $\\mathbb{R}^{n}$ y $\\mathbb{R}^{m}$, respectivamente. Entonces:\n",
    "    \n",
    "- **(P1):** La matriz $\\mathbf{V}$ realiza un cambio de base en el dominio $\\mathbb{R}^{n}$ desde la base $\\alpha$ (representada por los vectores rojos $\\mathbf{v}_{1}$ y $\\mathbf{v}_{2}$ en la Fig. (3.7), panel superior izquierdo) a la base canónica $\\mathbf{e}(n)$. Por lo tanto, podemos escribir $\\mathbf{V}^{\\ast}=[I]_{\\alpha}^{\\mathbf{e}(n)}$, lo que implica que el resultado de esta operación es el alineamiento de estos vectores con los ejes del sistema $\\mathbb{R}^{m}$ (lo que se observa en el panel inferior izquierdo de la Fig. (3.7)).\n",
    "    \n",
    "- **(P2):** Habiendo cambiado la base de nuestras coordenadas a la referencia canónica de $\\mathbb{R}^{m}$, la matriz $\\mathbf{\\Sigma}$ escala las nuevas coordenadas con respecto a los valores singulares $\\sigma_{i}$ (y añade o quita dimensiones); es decir, $\\mathbf{\\Sigma}$ es la matriz de cambio de base de $T$ con respecto a $\\alpha$ y $\\beta$ (es decir, $\\mathbf{\\Sigma}=[I]_{\\alpha}^{\\beta}$), representada por los vectores rojos ubicados en el sistema $(\\mathbf{e}_{1},\\mathbf{e}_{2})$, los que además son modificados en su longitud. En sistema completo $(\\mathbf{e}_{1},\\mathbf{e}_{2})$ ahora se encuentra inmerso en otro de mayor dimensión, como se observa en el panel inferior derecho de la Fig. (3.7).\n",
    "    \n",
    "- **(P3):** La matriz $\\mathbf{U}$ realiza un cambio de base en codominio $\\mathbb{R}^{m}$ desde la base $\\beta$ a la base canónica $\\mathbf{e}(m)$ (es decir, $\\mathbf{U}=[I]_{\\beta}^{\\mathbf{e}(m)}$), y que se representa por medio de una rotación de los vectores rojos en el gráfico del panel superior derecho de la Fig. (3.8) fuera del sistema $(\\mathbf{e}_{1},\\mathbf{e}_{2})$.\n",
    "    \n",
    "La descomposición SVD representa un cambio de base en ambos, dominio y codominio de la transformación lineal $T$. Esto es distinto a lo que ocurre en la descomposición por autovalores y autovectores (diagonalización), porque ésta opera siempre en el mismo espacio vectorial, donde se aplica siempre un cambio de base que luego se deshace mediante un cambio de base inverso. Lo que hace especial a la descomposición SVD, es que estas dos bases distintas están enlazadas simultáneamente a la matriz de valores singulares $\\mathbf{\\Sigma}$.\n",
    "    \n",
    "**Ejemplo 3.22:** Consideremos una transformación lineal que aplica una grilla cuadrada de vectores $\\mathcal{X}\\in \\mathbb{R}^{2}$ que caben en una caja de tamaño $2\\times 2$ centrada en el origen. Usando la base canónica de $\\mathbb{R}^{2}$, operamos sobre estos vectores usando las siguientes identidades:\n",
    "    \n",
    "$$\\mathbf{A} =\\left( \\begin{matrix}1&-0.8\\\\ 0&1\\\\ 1&0\\end{matrix} \\right)  =\\mathbf{U} \\mathbf{\\Sigma } \\mathbf{V}^{\\top } =\\left( \\begin{matrix}-0.79&0&-0.62\\\\ 0.38&-0.78&-0.49\\\\ -0.48&-0.62&0.62\\end{matrix} \\right)  \\left( \\begin{matrix}1.62&0\\\\ 0&1\\\\ 0&0\\end{matrix} \\right)  \\left( \\begin{matrix}-0.78&0.62\\\\ -0.62&-0.78\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.190)$</p>\n",
    "\n",
    "Partimos con un conjunto de vectores $\\mathcal{X}$ que son representados, en el panel superior izquierdo de la Fig. (3.8), como puntos coloreados (desde magenta a azul) arreglados en una grilla. Luego aplicamos $\\mathbf{V}^{\\top}\\in \\mathbb{R}^{2\\times 2}$, lo que genera una rotación de $\\mathcal{X}$, lo que se muestra en el panel inferior izquierdo de la Fig. (3.8). Ahora transformamos estos vectores por medio de la matriz de valores singulares $\\mathbf{\\Sigma}$ desde el dominio $\\mathbb{R}^{2}$ al codominio $\\mathbb{R}^{3}$, como se observa en el panel inferior derecho de la Fig. (3.8). Notemos que todos los vectores $\\mathcal{X}$ se ubican en el plano $(x_{1},x_{2})$. La tercera coordenada es siempre igual a cero, y los vectores en el plano $(x_{1},x_{2})$ son escalados por los valores singulares presentes en la diagonal de $\\mathbf{\\Sigma}$.\n",
    "\n",
    "La transformación de los vectores $\\mathcal{X}$ por medio de $\\mathbf{A}$ al codominio $\\mathbb{R}^{3}$ se iguala a la transformación de $\\mathcal{X}$ por medio de la descomposición $\\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^{\\top}$, donde $\\mathbf{U}$ aplica una rotación en el codominio $\\mathbb{R}^{3}$, de manera tal que los vectores sobre los cuales operamos mediante esta transformación ya no están restringidos al sistema $(x_{1},x_{2})$, aunque siguen residiendo en un plano, como se observa en el panel superior derecho de la Fig. (3.8).\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"figures/fig_3_8.png\" width=\"800\"></p>\n",
    "<p style=\"text-align: center;\">Fig. (3.8): Esquema geométrico que interpreta la aplicación de la descomposición de valores singulares en el ejemplo (3.22)\n",
    "\n",
    "◼︎"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "759e5e2d",
   "metadata": {},
   "source": [
    "### Construcción de la descomposición SVD.\n",
    "A continuación, discutiremos las **condiciones de existencia** de la descomposición en valores singulares y mostraremos cómo calcularla en detalle. Esta descomposición comparte ciertas similitudes con la **diagonalización de matrices hermíticas**, ya que\n",
    "\n",
    "$$\\mathbf{S} =\\mathbf{S}^{\\top } =\\mathbf{P} \\mathbf{D} \\mathbf{P}^{-1}$$\n",
    "<p style=\"text-align: right;\">$(3.191)$</p>\n",
    "\n",
    "Donde $\\mathbf{S}\\in \\mathbb{R}^{n\\times n}$ es una matriz hermítica. La correspondiente descomposición en valores singulares es\n",
    "\n",
    "$$\\mathbf{S} =\\mathbf{U} \\mathbf{\\Sigma } \\mathbf{V}^{\\ast }$$\n",
    "<p style=\"text-align: right;\">$(3.192)$</p>\n",
    "\n",
    "Si ponemos $\\mathbf{U}=\\mathbf{P}=\\mathbf{V}$ y $\\mathbf{D}=\\mathbf{\\Sigma}$, podemos concluir que la descomposición en valores singulares de matrices hermíticas es equivalente a su descomposición diagonal.\n",
    "\n",
    "A continuación, discutiremos las condiciones bajo las cuales el Teorema 3.15 se cumple y cómo construimos la descomposición SVD. El cálculo de esta descomposición para una matriz $\\mathbf{A}\\in \\mathbb{R}^{m\\times n}$ es equivalente a encontrar dos conjuntos de bases ortonormales $\\alpha =\\left\\{ \\mathbf{u}_{1} ,...,\\mathbf{u}_{m} \\right\\}  \\wedge \\beta =\\left\\{ \\mathbf{v}_{1} ,...,\\mathbf{v}_{n} \\right\\}$ relativos al dominio $\\mathbb{R}^{m}$ y al codominio $\\mathbb{R}^{n}$, respectivamente. A partir de estas bases, construimos las matrices $\\mathbf{U}$ y $\\mathbf{V}$. Para ello, será necesario que establezcamos primero un teorema esencial en la construcción de la descomposición SVD.\n",
    "\n",
    "**<font color='crimson'>Teorema 3.16:</font>** *Sea $\\mathbf{A}\\in \\mathbb{R}^{m\\times n}$ una matriz no singular. Entonces siempre podremos definir una matriz simétrica y semi-definida positiva $\\mathbf{S}\\in \\mathbb{R}^{n\\times n}$, a partir de $\\mathbf{A}$, como*\n",
    "\n",
    "$$\\mathbf{S} :=\\mathbf{A}^{\\top } \\mathbf{A}$$\n",
    "<p style=\"text-align: right;\">$(3.193)$</p>\n",
    "◆\n",
    "\n",
    "Nuestro plan es comenzar construyendo el conjunto ortonormal de **vectores singulares por la derecha** $\\beta =\\left\\{ \\mathbf{v}_{1} ,...,\\mathbf{v}_{n} \\right\\}  \\in \\mathbb{R}^{n}$. Luego construiremos el conjunto de **vectores singulares por la izquierda** $\\alpha =\\left\\{ \\mathbf{u}_{1} ,...,\\mathbf{u}_{m} \\right\\}  \\in \\mathbb{R}^{m}$. A continuación, enlazaremos ambos conjuntos con el requerimiento de **la ortogonalidad de $\\beta$ se preserve conforme la transformación de $\\mathbf{A}$**. Esto último es importante porque sabemos de antemano que las imágenes $\\mathbf{A}\\mathbf{v}_{i}$ conforman un conjunto ortogonal de vectores. Luego normalizaremos estas imágenes por medio de factores de escalamiento, los que, por supuesto, resultarán ser los **valores singulares** de $\\mathbf{A}$.\n",
    "\n",
    "Partimos entonces construyendo los vectores singulares por la derecha. El teorema (3.10) nos dice que los autovectores de una matriz simétrica conforman una base ortonormal, lo que implica, por supuesto, que dicha matriz es diagonalizable. Además, del teorema (3.16), sabemos que siempre podemos construir una matriz simétrica y semi-definida positiva $\\mathbf{A}^{\\top}\\mathbf{A}\\in \\mathbb{R}^{n\\times n}$ a partir de cualquier matriz rectangular. De esta manera, podemos diagonalizar $\\mathbf{A}^{\\top}\\mathbf{A}$ y obtener\n",
    "\n",
    "$$\\mathbf{A}^{\\top } \\mathbf{A} =\\mathbf{P} \\mathbf{D} \\mathbf{P}^{\\top } =\\mathbf{P} \\left( \\begin{matrix}\\lambda_{1} &\\cdots &0\\\\ \\vdots &\\ddots &\\vdots \\\\ 0&\\cdots &\\lambda_{n} \\end{matrix} \\right)  \\mathbf{P}^{\\top }$$\n",
    "<p style=\"text-align: right;\">$(3.194)$</p>\n",
    "\n",
    "Donde $\\mathbf{P}$ es una matriz ortogonal que está compuesta por una base ortonormal de autovectores. Los escalares $\\lambda_{1}, ..., \\lambda_{n}$ son los autovalores de $\\mathbf{A}^{\\top}\\mathbf{A}$. Asumamos entonces que la descomposición en valores singulares de $\\mathbf{A}$ existe y apliquemos el teorema (3.15) a la ecuación (3.194). De esta manera, obtenemos\n",
    "\n",
    "$$\\mathbf{A}^{\\top } \\mathbf{A} =\\left( \\mathbf{U} \\mathbf{\\Sigma } \\mathbf{V}^{\\top } \\right)^{\\top }  \\left( \\mathbf{U} \\mathbf{\\Sigma } \\mathbf{V}^{\\top } \\right)  =\\mathbf{V} \\mathbf{\\Sigma }^{\\top } \\mathbf{U}^{\\top } \\mathbf{U} \\mathbf{\\Sigma } \\mathbf{V}^{\\top }$$\n",
    "<p style=\"text-align: right;\">$(3.195)$</p>\n",
    "\n",
    "Como $\\mathbf{U}$ y $\\mathbf{V}$ son matrices ortogonales, tenemos que $\\mathbf{U}^{\\top}\\mathbf{U}=\\mathbf{I}_{m}$. Por lo tanto,\n",
    "\n",
    "$$\\mathbf{A}^{\\top } \\mathbf{A} =\\mathbf{V} \\mathbf{\\Sigma }^{\\top } \\mathbf{\\Sigma } \\mathbf{V}^{\\top } =\\mathbf{V} \\left( \\begin{matrix}\\sigma^{2}_{1} &\\cdots &0\\\\ \\vdots &\\ddots &\\vdots \\\\ 0&\\cdots &\\sigma^{2}_{n} \\end{matrix} \\right)  \\mathbf{V}^{\\top }$$\n",
    "<p style=\"text-align: right;\">$(3.196)$</p>\n",
    "\n",
    "Si comparamos las ecuaciones (3.196) y (3.194), podemos darnos cuenta que\n",
    "\n",
    "$$\\mathbf{V}^{\\top } =\\mathbf{P}^{\\top } \\wedge \\sigma^{2}_{i} =\\lambda_{i}$$\n",
    "<p style=\"text-align: right;\">$(3.196)$</p>\n",
    "\n",
    "Por lo tanto, los autovectores de $\\mathbf{A}^{\\top}\\mathbf{A}$ que componen $\\mathbf{P}$ son los vectores singulares derechos $\\mathbf{V}$ de $\\mathbf{A}$, mientras que los autovalores de $\\mathbf{A}^{\\top}\\mathbf{A}$ son iguales al cuadrado de los valores singulares de $\\mathbf{A}$ (y que se corresponden con los elementos no nulos de la matriz $\\mathbf{\\Sigma}$. Para obtener los vectores singulares por la izquierda que componen $\\mathbf{U}$, seguimos un procedimiento similar. Partimos calculando la descomposición en valores singulares de la matriz simétrica $\\mathbf{A}\\mathbf{A}^{\\top}\\in \\mathbb{R}^{m\\times m}$. En este caso, obtenemos\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{A} \\mathbf{A}^{\\top } &=&\\left( \\mathbf{U} \\mathbf{\\Sigma } \\mathbf{V}^{\\top } \\right)  \\left( \\mathbf{U} \\mathbf{\\Sigma } \\mathbf{V}^{\\top } \\right)^{\\top }  \\\\ &=&\\mathbf{U} \\mathbf{\\Sigma } \\mathbf{V}^{\\top } \\mathbf{V} \\mathbf{\\Sigma }^{\\top } \\mathbf{U}^{\\top } \\\\ &=&\\mathbf{U} \\left( \\begin{matrix}\\sigma^{2}_{1} &\\cdots &0\\\\ \\vdots &\\ddots &\\vdots \\\\ 0&\\cdots &\\sigma^{2}_{n} \\end{matrix} \\right)  \\mathbf{U}^{\\top } \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.197)$</p>\n",
    "\n",
    "El teorema (3.10) nos dice que $\\mathbf{A}\\mathbf{A}^{\\top} = \\mathbf{S}\\mathbf{D}\\mathbf{S}^{\\top}$ es diagonalizable y que podemos encontrar una base ortonormal de autovectores de $\\mathbf{A}\\mathbf{A}^{\\top}$, los que están contenidos en $\\mathbf{S}$. Los autovectores ortonormales de $\\mathbf{A}\\mathbf{A}^{\\top}$ son los vectores singulares por la izquierda que constituyen $\\mathbf{U}$ y conforman una base ortonormal en el codominio de la descomposició SVD.\n",
    "\n",
    "Lo anterior deja abierta la pregunta relativa a la estructura de la matriz $\\mathbf{\\Sigma}$. Dado que $\\mathbf{A}\\mathbf{A}^{\\top}$ y $\\mathbf{A}^{\\top}\\mathbf{A}$ tienen los mismos autovalores no nulos, las entradas no nulas de las matrices $\\mathbf{\\Sigma}$ en ambas descomposiciones SVD (para $\\mathbf{A}\\mathbf{A}^{\\top}$ y $\\mathbf{A}\\mathbf{A}^{\\top}$ deben ser las mismas.\n",
    "\n",
    "El último paso es unir todo lo que hemos hecho hasta ahora. Tenemos un conjunto ortonormal de vectores singulares por la derecha en la matriz $\\mathbf{V}$ y, para finalizar la construcción de la factorización SVD, debemos unirlos a los vectores singulares por la izquierda que componen la matriz $\\mathbf{U}$. Para lograr esto, usamos el hecho de que las imágenes de los vectores $\\mathbf{v}_{1},...,\\mathbf{v}_{n}$ bajo $\\mathbf{A}$ tienen que ser, también, ortogonales. En ese caso, necesitamos que el producto interno entre $\\mathbf{A}\\mathbf{v}_{i}$ y $\\mathbf{A}\\mathbf{v}_{j}$ sea nulo para $i\\neq j$. De este modo, tenemos que\n",
    "\n",
    "$$\\left( \\mathbf{A} \\mathbf{v}_{i} \\right)^{\\top }  \\left( \\mathbf{A} \\mathbf{v}_{j} \\right)  =\\mathbf{v}^{\\top }_{i} \\left( \\mathbf{A}^{\\top } \\mathbf{A} \\right)  \\mathbf{v}_{j} =\\mathbf{v}^{\\top }_{i} \\left( \\lambda_{j} \\mathbf{v}_{j} \\right)  =\\lambda_{j} \\mathbf{v}^{\\top }_{i} \\mathbf{v}_{j} =0$$\n",
    "<p style=\"text-align: right;\">$(3.198)$</p>\n",
    "\n",
    "Para el caso $m\\geq r$, se tiene que $\\left\\{ \\mathbf{A} \\mathbf{v}_{1} ,...,\\mathbf{A} \\mathbf{v}_{r} \\right\\}$ es una base para un subespacio $r$-dimensional de $\\mathbb{R}^{m}$.\n",
    "\n",
    "Para completar la construcción de la descomposición SVD, necesitamos vectores singulares por la izquierda que sean ortonormales. Por lo tanto, normalizamos las imágenes de los vectores singulares por la derecha $\\mathbf{A} \\mathbf{v}_{i}$, obteniendo\n",
    "\n",
    "$$\\mathbf{u}_{i} :=\\frac{\\mathbf{A} \\mathbf{v}_{i} }{\\left\\Vert \\mathbf{A} \\mathbf{v}_{i} \\right\\Vert  } =\\frac{1}{\\sqrt{\\lambda_{i} } } \\mathbf{A} \\mathbf{v}_{i} =\\frac{1}{\\sigma_{i} } \\mathbf{A} \\mathbf{v}_{i}$$\n",
    "<p style=\"text-align: right;\">$(3.199)$</p>\n",
    "\n",
    "Por lo tanto, los autovectores de $\\mathbf{A}^{\\top}\\mathbf{A}$, que ya sabemos que son los vectores singulares por la derecha $\\mathbf{v}_{i}$, y sus imágenes normalizadas bajo $\\mathbf{A}$, los vectores singulares por la izquierda $\\mathbf{u}_{i}$, conforman dos bases ortonormales consistentes que están conectadas por la matriz de valores singulares $\\mathbf{\\Sigma}$.\n",
    "\n",
    "Vamos a reorganizar la ecuación (3.199) para obtener la **ecuación de valores singulares**\n",
    "\n",
    "$$\\mathbf{A} \\mathbf{v}_{i} =\\sigma_{i} \\mathbf{u}_{i} \\  ;\\  i=1,...,r$$\n",
    "<p style=\"text-align: right;\">$(3.200)$</p>\n",
    "\n",
    "Para $n<m$, la ecuación (3.200) se cumple únicamente para $i\\leq n$, pero no nos dice nada acerca de los vectores $\\mathbf{u}_{i}$ para $i>n$. Sin embargo, por construcción, sabemos que dichos vectores son ortonormales. Por el contrario, para $m<n$, la ecuación (3.200) se cumple únicamente para $i\\geq m$. Para $i>m$, tenemos que $\\mathbf{A}\\mathbf{v}_{i}=0$ y aún sabemos que los vectores $\\mathbf{v}_{i}$ conforman un conjunto ortonormal. Esto significa que la descomposición SVD también nos provee de una base ortornomal para $\\ker(\\mathbf{A})$ (es decir, el conjunto de vectores $\\mathbf{x}$ tales que $\\mathbf{A}\\mathbf{x}=\\mathbf{0}$).\n",
    "\n",
    "Uniendo los vectores $\\mathbf{v}_{i}$ como columnas de $\\mathbf{V}$ y los vectores $\\mathbf{u}_{i}$ como columnas de $\\mathbf{U}$, obtenemos\n",
    "\n",
    "$$\\mathbf{A} \\mathbf{v} =\\mathbf{U} \\mathbf{\\Sigma } $$\n",
    "<p style=\"text-align: right;\">$(3.201)$</p>\n",
    "\n",
    "Donde $\\mathbf{\\Sigma }$ tiene la misma dimensión que $\\mathbf{A}$ y una estructura diagonal para las filas 1 a $r$. Por lo tanto, multiplicando esta expresión por la derecha por $\\mathbf{V}^{\\top}$, obtenemos $\\mathbf{A}=\\mathbf{U}\\mathbf{\\Sigma }\\mathbf{V}^{\\top}$, que resulta ser la descomposición SVD de $\\mathbf{A}$.\n",
    "\n",
    "**Ejemplo 3.23:** Vamos a determinar la descomposición en valores singulares de la matriz $\\mathbf{A}$ definida como\n",
    "\n",
    "$$\\mathbf{A} =\\left( \\begin{matrix}4&11&14\\\\ 8&7&-2\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.202)$</p>\n",
    "\n",
    "Siguiendo el proceso de construcción de la descomposición SVD visto previamente, sabemos que el primer paso es determinar los autovalores del producto $\\mathbf{A}^{\\top}\\mathbf{A}$. En este caso, como\n",
    "\n",
    "$$\\mathbf{A}^{\\top } \\mathbf{A} =\\left( \\begin{matrix}80&100&40\\\\ 100&170&140\\\\ 40&140&200\\end{matrix} \\right)  $$\n",
    "<p style=\"text-align: right;\">$(3.203)$</p>\n",
    "\n",
    "y se tiene que el polinomio característico de esta matriz es $P_{\\mathbf{A}^{\\top } \\mathbf{A} }\\left( \\lambda \\right)  =\\det \\left( \\mathbf{A}^{\\top } \\mathbf{A} -\\lambda \\mathbf{I}_{3} \\right)  =\\lambda \\left( \\lambda -360\\right)  \\left( \\lambda -90\\right)$, lo que implica que sus autovalores son $\\lambda_{1} =360,\\lambda_{2} =90$ y $\\lambda_{3}=0$. Por lo tanto, los valores singulares de $\\mathbf{A}$ son $\\sigma_{1} =6\\sqrt{10} ,\\sigma_{2} =3\\sqrt{10}$ y $\\sigma_{3}=0$. Buscaremos ahora una base ortonormal de autovectores para $\\mathbf{A}^{\\top } \\mathbf{A}$. Para ello, determinamos los autoespacios respectivos como sigue,\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{u} \\in \\left( \\mathbb{R}^{3\\times 1} \\right)_{\\lambda }  &\\Longleftrightarrow &\\mathbf{u} \\in \\mathbb{R}^{3\\times 1} \\wedge \\left( \\mathbf{A}^{\\top } \\mathbf{A} \\right)  \\mathbf{u} =\\lambda \\mathbf{u} \\\\ &\\Longleftrightarrow &\\mathbf{u} =\\left( \\begin{matrix}x\\\\ y\\\\ z\\end{matrix} \\right)  \\wedge \\left( \\begin{matrix}80&100&40\\\\ 100&170&140\\\\ 40&140&200\\end{matrix} \\right)  \\left( \\begin{matrix}x\\\\ y\\\\ z\\end{matrix} \\right)  =\\left( \\begin{matrix}\\lambda x\\\\ \\lambda y\\\\ \\lambda z\\end{matrix} \\right)  \\\\ &\\Longleftrightarrow &\\mathbf{u} =\\left( \\begin{matrix}x\\\\ y\\\\ z\\end{matrix} \\right)  \\wedge \\begin{cases}\\begin{array}{rcl}80x+100y+40z&=&\\lambda x\\\\ 100x+170y+140z&=&\\lambda y\\\\ 40x+140y+200z&=&\\lambda z\\end{array} &\\end{cases} \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.204)$</p>\n",
    "\n",
    "Para $\\lambda_{1}=360$:\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{u} \\in \\left( \\mathbb{R}^{3\\times 1} \\right)_{\\lambda =360}  &\\Longleftrightarrow &\\mathbf{u} =\\left( \\begin{matrix}x\\\\ y\\\\ z\\end{matrix} \\right)  \\wedge \\begin{cases}\\begin{array}{rcl}80x+100y+40z&=&360x\\\\ 100x+170y+140z&=&360y\\\\ 40x+140y+200z&=&360z\\end{array} &\\end{cases} \\\\ &\\Longrightarrow &\\mathbf{u} =\\left( \\begin{matrix}x\\\\ y\\\\ z\\end{matrix} \\right)  \\wedge y=2x\\wedge z=2x\\\\ &\\Longrightarrow &\\mathbf{u} =\\left( \\begin{matrix}x\\\\ 2x\\\\ 2x\\end{matrix} \\right)  =x\\left( \\begin{matrix}1\\\\ 2\\\\ 2\\end{matrix} \\right)  \\\\ &\\Longrightarrow &\\left( \\mathbb{R}^{3\\times 1} \\right)_{\\lambda =360}  =\\left< \\left\\{ \\left( \\begin{matrix}1\\\\ 2\\\\ 2\\end{matrix} \\right)  \\right\\}  \\right>  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.205)$</p>\n",
    "\n",
    "Para $\\lambda_{2}=90$:\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{u} \\in \\left( \\mathbb{R}^{3\\times 1} \\right)_{\\lambda =90}  &\\Longleftrightarrow &\\mathbf{u} =\\left( \\begin{matrix}x\\\\ y\\\\ z\\end{matrix} \\right)  \\wedge \\begin{cases}\\begin{array}{rcl}80x+100y+40z&=&90x\\\\ 100x+170y+140z&=&90y\\\\ 40x+140y+200z&=&90z\\end{array} &\\end{cases} \\\\ &\\Longrightarrow &\\mathbf{u} =\\left( \\begin{matrix}x\\\\ y\\\\ z\\end{matrix} \\right)  \\wedge 2z=-2x\\wedge 2z=-y\\\\ &\\Longrightarrow &\\mathbf{u} =\\left( \\begin{matrix}-2x\\\\ -x\\\\ 2x\\end{matrix} \\right)  =x\\left( \\begin{matrix}-2\\\\ -1\\\\ 2\\end{matrix} \\right)  \\\\ &\\Longrightarrow &\\left( \\mathbb{R}^{3\\times 1} \\right)_{\\lambda =90}  =\\left< \\left\\{ \\left( \\begin{matrix}-2\\\\ -1\\\\ 2\\end{matrix} \\right)  \\right\\}  \\right>  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.206)$</p>\n",
    "\n",
    "Finalmente, para $\\lambda_{3}=0$\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{u} \\in \\left( \\mathbb{R}^{3\\times 1} \\right)_{\\lambda =0}  &\\Longleftrightarrow &\\mathbf{u} =\\left( \\begin{matrix}x\\\\ y\\\\ z\\end{matrix} \\right)  \\wedge \\begin{cases}\\begin{array}{rcl}80x+100y+40z&=&0x\\\\ 100x+170y+140z&=&0y\\\\ 40x+140y+200z&=&0z\\end{array} &\\end{cases} \\\\ &\\Longrightarrow &\\mathbf{u} =\\left( \\begin{matrix}x\\\\ y\\\\ z\\end{matrix} \\right)  \\wedge z=2x\\wedge z=-2y\\\\ &\\Longrightarrow &\\mathbf{u} =\\left( \\begin{matrix}2x\\\\ -2x\\\\ x\\end{matrix} \\right)  =x\\left( \\begin{matrix}2\\\\ -2\\\\ 1\\end{matrix} \\right)  \\\\ &\\Longrightarrow &\\left( \\mathbb{R}^{3\\times 1} \\right)_{\\lambda =90}  =\\left< \\left\\{ \\left( \\begin{matrix}2\\\\ -2\\\\ 1\\end{matrix} \\right)  \\right\\}  \\right>  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.207)$</p>\n",
    "\n",
    "Por lo tanto, una base ortonormal de autovectores para $\\mathbf{A}^{\\top}\\mathbf{A}$ es\n",
    "\n",
    "$$\\alpha =\\left\\{ \\frac{\\left( \\begin{matrix}1\\\\ 2\\\\ 2\\end{matrix} \\right)  }{\\left\\Vert \\left( \\begin{matrix}1\\\\ 2\\\\ 2\\end{matrix} \\right)  \\right\\Vert  } ,\\frac{\\left( \\begin{matrix}-2\\\\ -1\\\\ 2\\end{matrix} \\right)  }{\\left\\Vert \\left( \\begin{matrix}-2\\\\ -1\\\\ 2\\end{matrix} \\right)  \\right\\Vert  } ,\\frac{\\left( \\begin{matrix}2\\\\ -2\\\\ 1\\end{matrix} \\right)  }{\\left\\Vert \\left( \\begin{matrix}2\\\\ -2\\\\ 1\\end{matrix} \\right)  \\right\\Vert  } \\right\\}  =\\left\\{ \\frac{1}{\\sqrt{9} } \\left( \\begin{matrix}1\\\\ 2\\\\ 2\\end{matrix} \\right)  ,\\frac{1}{\\sqrt{9} } \\left( \\begin{matrix}-2\\\\ -1\\\\ 2\\end{matrix} \\right)  ,\\frac{1}{\\sqrt{9} } \\left( \\begin{matrix}2\\\\ -2\\\\ 1\\end{matrix} \\right)  \\right\\}  =\\left\\{ \\frac{1}{3} \\left( \\begin{matrix}1\\\\ 2\\\\ 2\\end{matrix} \\right)  ,\\frac{1}{3} \\left( \\begin{matrix}-2\\\\ -1\\\\ 2\\end{matrix} \\right)  ,\\frac{1}{3} \\left( \\begin{matrix}2\\\\ -2\\\\ 1\\end{matrix} \\right)  \\right\\}  $$\n",
    "<p style=\"text-align: right;\">$(3.208)$</p>\n",
    "\n",
    "Calculamos ahora $\\mathbf{A}\\mathbf{v}_{i}$ para $i=1,2,3$ como sigue,\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{A} \\mathbf{v}_{1} &=&\\left( \\begin{matrix}4&11&14\\\\ 8&7&-2\\end{matrix} \\right)  \\left( \\begin{matrix}1/3\\\\ 2/3\\\\ 2/3\\end{matrix} \\right)  =\\left( \\begin{matrix}18\\\\ 6\\end{matrix} \\right)  \\\\ \\mathbf{A} \\mathbf{v}_{2} &=&\\left( \\begin{matrix}4&11&14\\\\ 8&7&-2\\end{matrix} \\right)  \\left( \\begin{matrix}-2/3\\\\ -1/3\\\\ 2/3\\end{matrix} \\right)  =\\left( \\begin{matrix}3\\\\ -9\\end{matrix} \\right)  \\\\ \\mathbf{A} \\mathbf{v}_{3} &=&\\left( \\begin{matrix}4&11&14\\\\ 8&7&-2\\end{matrix} \\right)  \\left( \\begin{matrix}2/3\\\\ -2/3\\\\ 1/3\\end{matrix} \\right)  =\\left( \\begin{matrix}0\\\\ 0\\end{matrix} \\right)  \\end{array} $$\n",
    "<p style=\"text-align: right;\">$(3.209)$</p>\n",
    "\n",
    "Como hay dos valores singulares no nulos, tenemos\n",
    "\n",
    "$$\\mathbf{u}_{1} =\\displaystyle \\frac{1}{\\sigma_{1} } \\mathbf{A} \\mathbf{v}_{1} =\\displaystyle \\frac{1}{6\\sqrt{10} } \\left( \\begin{matrix}18\\\\ 6\\end{matrix} \\right)  =\\left( \\begin{array}{r}\\displaystyle \\frac{3}{\\sqrt{10} } \\\\ \\displaystyle \\frac{1}{\\sqrt{10} } \\end{array} \\right)  \\  ;\\  \\mathbf{u}_{2} =\\displaystyle \\frac{1}{\\sigma_{2} } \\mathbf{A} \\mathbf{v}_{2} =\\displaystyle \\frac{1}{3\\sqrt{10} } \\left( \\begin{matrix}3\\\\ -9\\end{matrix} \\right)  =\\left( \\begin{array}{r}\\displaystyle \\frac{1}{\\sqrt{10} } \\\\ -\\displaystyle \\frac{3}{\\sqrt{10} } \\end{array} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.210)$</p>\n",
    "\n",
    "Notemos que $\\beta=\\left\\{ \\mathbf{u}_{1} ,\\mathbf{u}_{2} \\right\\}$ es una base ortonormal para $\\mathbb{R}^{2}$. De esta manera, la descomposición en valores singulares de $\\mathbf{A}$ es\n",
    "\n",
    "$$\\mathbf{A} =\\underbrace{\\left( \\begin{array}{rr}\\displaystyle \\frac{3}{\\sqrt{10} } &\\displaystyle \\frac{1}{\\sqrt{10} } \\\\ \\displaystyle \\frac{1}{\\sqrt{10} } &-\\displaystyle \\frac{3}{\\sqrt{10} } \\end{array} \\right)  }_{\\mathbf{U} } \\underbrace{\\left( \\begin{matrix}6\\sqrt{10} &0&0\\\\ 0&3\\sqrt{10} &0\\end{matrix} \\right)  }_{\\mathbf{\\Sigma } } \\underbrace{\\left( \\begin{array}{rrr}1/3&2/3&2/3\\\\ -2/3&-1/3&2/3\\\\ 2/3&-2/3&1/3\\end{array} \\right)  }_{\\mathbf{V}^{\\top } }$$\n",
    "<p style=\"text-align: right;\">$(3.211)$</p>\n",
    "◼︎\n",
    "\n",
    "**Ejemplo 3.24:** Vamos a encontrar la descomposición en valores singulares de la matriz\n",
    "\n",
    "$$\\mathbf{A} =\\left( \\begin{matrix}1&0&1\\\\ -2&1&0\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.212)$</p>\n",
    "\n",
    "En efecto, primero construimos la matriz $\\mathbf{A}^{\\top} \\mathbf{A}$ como sigue,\n",
    "\n",
    "$$\\mathbf{A}^{\\top } \\mathbf{A} =\\left( \\begin{matrix}1&-2\\\\ 0&1\\\\ 1&0\\end{matrix} \\right)  \\left( \\begin{matrix}1&0&1\\\\ -2&1&0\\end{matrix} \\right)  =\\left( \\begin{matrix}5&-2&1\\\\ -2&1&0\\\\ 1&0&1\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.213)$</p>\n",
    "\n",
    "Queremos pues diagonalizar $\\mathbf{A}^{\\top } \\mathbf{A}$. Partimos calculando su polinomio característico,\n",
    "\n",
    "$$P_{\\mathbf{A}^{\\top } \\mathbf{A} }\\left( \\lambda \\right)  =\\left( \\begin{matrix}5-\\lambda &-2&1\\\\ -2&1-\\lambda &0\\\\ 1&0&1-\\lambda \\end{matrix} \\right)  =\\lambda \\left( \\lambda -6\\right)  \\left( \\lambda -1\\right)$$\n",
    "<p style=\"text-align: right;\">$(3.214)$</p>\n",
    "\n",
    "Por lo que los autovalores de $\\mathbf{A}^{\\top } \\mathbf{A}$ son $\\lambda_{1}=6,\\lambda_{2}=1$ y $\\lambda_{3}=0$. De esta manera, los valores singulares de $\\mathbf{A}^{\\top } \\mathbf{A}$ son $\\sigma_{1}=\\sqrt{6}, \\sigma_{2}=1$ y $\\sigma_{3}=0$. Debemos buscar ahora una base ortonormal de autovectores para $\\mathbf{A}^{\\top } \\mathbf{A}$ a partir del cálculo de sus autoespacios. De este modo tenemos que\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{u} \\in \\left( \\mathbb{R}^{3\\times 1} \\right)_{\\lambda }  &\\Longleftrightarrow &\\mathbf{u} \\in \\mathbb{R}^{3\\times 1} \\wedge \\left( \\mathbf{A}^{\\top } \\mathbf{A} \\right)  \\mathbf{u} =\\lambda \\mathbf{u} \\\\ &\\Longleftrightarrow &\\mathbf{u} =\\left( \\begin{matrix}x\\\\ y\\\\ z\\end{matrix} \\right)  \\wedge \\left( \\begin{matrix}5&-2&1\\\\ -2&1&0\\\\ 1&0&1\\end{matrix} \\right)  \\left( \\begin{matrix}x\\\\ y\\\\ z\\end{matrix} \\right)  =\\left( \\begin{matrix}\\lambda x\\\\ \\lambda y\\\\ \\lambda z\\end{matrix} \\right)  \\\\ &\\Longleftrightarrow &\\mathbf{u} =\\left( \\begin{matrix}x\\\\ y\\\\ z\\end{matrix} \\right)  \\wedge \\begin{cases}\\begin{array}{rcl}5x-2y+z&=&\\lambda x\\\\ -2x+y&=&\\lambda y\\\\ x+z&=&\\lambda z\\end{array} &\\end{cases} \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.215)$</p>\n",
    "\n",
    "Luego, para $\\lambda_{1}=6$,\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{u} \\in \\left( \\mathbb{R}^{3\\times 1} \\right)_{\\lambda =6}  &\\Longrightarrow &\\mathbf{u} =\\left( \\begin{matrix}x\\\\ y\\\\ z\\end{matrix} \\right)  \\wedge \\begin{array}{rcl}5x-2y+z&=&6x\\\\ -2x+y&=&6y\\\\ x+z&=&6z\\end{array} \\\\ &\\Longrightarrow &\\mathbf{u} =\\left( \\begin{matrix}x\\\\ y\\\\ z\\end{matrix} \\right)  \\wedge x=5z\\wedge y=-2z\\\\ &\\Longrightarrow &\\mathbf{u} =\\left( \\begin{matrix}5z\\\\ -2z\\\\ z\\end{matrix} \\right)  =z\\left( \\begin{matrix}5\\\\ -2\\\\ 1\\end{matrix} \\right)  \\\\ &\\Longrightarrow &\\left( \\mathbb{R}^{3\\times 1} \\right)_{\\lambda =6}  =\\left< \\left\\{ \\left( \\begin{matrix}5\\\\ -2\\\\ 1\\end{matrix} \\right)  \\right\\}  \\right>  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.216)$</p>\n",
    "\n",
    "Además, para $\\lambda_{2}=1$\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{u} \\in \\left( \\mathbb{R}^{3\\times 1} \\right)_{\\lambda =1}  &\\Longrightarrow &\\mathbf{u} =\\left( \\begin{matrix}x\\\\ y\\\\ z\\end{matrix} \\right)  \\wedge \\begin{array}{rcl}5x-2y+z&=&x\\\\ -2x+y&=&y\\\\ x+z&=&z\\end{array} \\\\ &\\Longrightarrow &\\mathbf{u} =\\left( \\begin{matrix}x\\\\ y\\\\ z\\end{matrix} \\right)  \\wedge x=0\\wedge z=2y\\\\ &\\Longrightarrow &\\mathbf{u} =\\left( \\begin{matrix}0\\\\ y\\\\ 2y\\end{matrix} \\right)  =y\\left( \\begin{matrix}0\\\\ 1\\\\ 2\\end{matrix} \\right)  \\\\ &\\Longrightarrow &\\left( \\mathbb{R}^{3\\times 1} \\right)_{\\lambda =1}  =\\left< \\left\\{ \\left( \\begin{matrix}0\\\\ 1\\\\ 2\\end{matrix} \\right)  \\right\\}  \\right>  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.217)$</p>\n",
    "\n",
    "Finalmente, para $\\lambda_{3}=0$,\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{u} \\in \\left( \\mathbb{R}^{3\\times 1} \\right)_{\\lambda =0}  &\\Longrightarrow &\\mathbf{u} =\\left( \\begin{matrix}x\\\\ y\\\\ z\\end{matrix} \\right)  \\wedge \\begin{array}{rcl}5x-2y+z&=&0x\\\\ -2x+y&=&0y\\\\ x+z&=&0z\\end{array} \\\\ &\\Longrightarrow &\\mathbf{u} =\\left( \\begin{matrix}x\\\\ y\\\\ z\\end{matrix} \\right)  \\wedge y=-2z\\wedge x=-z\\\\ &\\Longrightarrow &\\mathbf{u} =\\left( \\begin{matrix}-z\\\\ -2z\\\\ z\\end{matrix} \\right)  =z\\left( \\begin{matrix}-1\\\\ -2\\\\ 1\\end{matrix} \\right)  \\\\ &\\Longrightarrow &\\left( \\mathbb{R}^{3\\times 1} \\right)_{\\lambda =0}  =\\left< \\left\\{ \\left( \\begin{matrix}-1\\\\ -2\\\\ 1\\end{matrix} \\right)  \\right\\}  \\right>  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.218)$</p>\n",
    "\n",
    "Por lo tanto, una base ortonormal de autovectores para $\\mathbf{A}^{\\top}\\mathbf{A}$ es\n",
    "\n",
    "$$\\alpha =\\left\\{ \\frac{\\left( \\begin{matrix}5\\\\ -2\\\\ 1\\end{matrix} \\right)  }{\\left\\Vert \\left( \\begin{matrix}5\\\\ -2\\\\ 1\\end{matrix} \\right)  \\right\\Vert  } ,\\frac{\\left( \\begin{matrix}0\\\\ 1\\\\ 2\\end{matrix} \\right)  }{\\left\\Vert \\left( \\begin{matrix}0\\\\ 1\\\\ 2\\end{matrix} \\right)  \\right\\Vert  } ,\\frac{\\left( \\begin{matrix}-1\\\\ -2\\\\ 1\\end{matrix} \\right)  }{\\left\\Vert \\left( \\begin{matrix}-1\\\\ -2\\\\ 1\\end{matrix} \\right)  \\right\\Vert  } \\right\\}  =\\left\\{ \\frac{1}{\\sqrt{30} } \\left( \\begin{matrix}5\\\\ -2\\\\ 1\\end{matrix} \\right)  ,\\frac{1}{\\sqrt{5} } \\left( \\begin{matrix}0\\\\ 1\\\\ 2\\end{matrix} \\right)  ,\\frac{1}{\\sqrt{6} } \\left( \\begin{matrix}-1\\\\ -2\\\\ 1\\end{matrix} \\right)  \\right\\}$$\n",
    "<p style=\"text-align: right;\">$(3.219)$</p>\n",
    "\n",
    "Calculamos ahora $\\mathbf{A}\\mathbf{v}_{i}$ para $i=1,2,3$:\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{A} \\mathbf{v}_{1} &=&\\frac{1}{\\sqrt{30} } \\left( \\begin{matrix}1&0&1\\\\ -2&1&0\\end{matrix} \\right)  \\left( \\begin{matrix}5\\\\ -2\\\\ 1\\end{matrix} \\right)  =\\frac{1}{\\sqrt{30} } \\left( \\begin{matrix}6\\\\ -12\\end{matrix} \\right)  =\\left( \\begin{matrix}6/\\sqrt{30} \\\\ -12/\\sqrt{30} \\end{matrix} \\right)  \\\\ \\mathbf{A} \\mathbf{v}_{2} &=&\\frac{1}{\\sqrt{5} } \\left( \\begin{matrix}1&0&1\\\\ -2&1&0\\end{matrix} \\right)  \\left( \\begin{matrix}0\\\\ 1\\\\ 2\\end{matrix} \\right)  =\\frac{1}{\\sqrt{5} } \\left( \\begin{matrix}2\\\\ 1\\end{matrix} \\right)  =\\left( \\begin{matrix}2/\\sqrt{5} \\\\ 1/\\sqrt{5} \\end{matrix} \\right)  \\\\ \\mathbf{A} \\mathbf{v}_{3} &=&\\frac{1}{\\sqrt{6} } \\left( \\begin{matrix}1&0&1\\\\ -2&1&0\\end{matrix} \\right)  \\left( \\begin{matrix}-1\\\\ -2\\\\ 1\\end{matrix} \\right)  =\\frac{1}{\\sqrt{6} } \\left( \\begin{matrix}0\\\\ 0\\end{matrix} \\right)  =\\left( \\begin{matrix}0\\\\ 0\\end{matrix} \\right)  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.220)$</p>\n",
    "\n",
    "Como hay dos valores singulares no nulos, calculamos\n",
    "\n",
    "$$\\mathbf{u}_{1} =\\frac{1}{\\sigma_{1} } \\mathbf{A} \\mathbf{v}_{1} =\\frac{1}{\\sqrt{6} } \\frac{1}{\\sqrt{30} } \\left( \\begin{matrix}1&0&1\\\\ -2&1&0\\end{matrix} \\right)  \\left( \\begin{matrix}5\\\\ -2\\\\ 1\\end{matrix} \\right)  =\\left( \\begin{matrix}1/\\sqrt{5} \\\\ -2/\\sqrt{5} \\end{matrix} \\right)  \\wedge \\mathbf{u}_{2} =\\frac{1}{\\sigma_{2} } \\mathbf{A} \\mathbf{v}_{2} =\\frac{1}{\\sqrt{5} } \\left( \\begin{matrix}1&0&1\\\\ -2&1&0\\end{matrix} \\right)  \\left( \\begin{matrix}0\\\\ 1\\\\ 2\\end{matrix} \\right)  =\\left( \\begin{matrix}2/\\sqrt{5} \\\\ 1/\\sqrt{5} \\end{matrix} \\right)  $$\n",
    "<p style=\"text-align: right;\">$(3.221)$</p>\n",
    "\n",
    "Notemos que $\\beta =\\left\\{ \\mathbf{u}_{1} ,\\mathbf{u}_{2} \\right\\}  =\\left\\{ \\left( \\begin{matrix}1/\\sqrt{5} \\\\ -2/\\sqrt{5} \\end{matrix} \\right)  ,\\left( \\begin{matrix}2/\\sqrt{5} \\\\ 1/\\sqrt{5} \\end{matrix} \\right)  \\right\\}$ es una base ortonormal de $\\mathbb{R}^{2}$. Por lo tanto,\n",
    "\n",
    "$$\\mathbf{A} =\\underbrace{\\left( \\begin{array}{rr}\\displaystyle \\frac{1}{\\sqrt{5} } &\\displaystyle \\frac{2}{\\sqrt{5} } \\\\ -\\displaystyle \\frac{2}{\\sqrt{5} } &\\displaystyle \\frac{1}{\\sqrt{5} } \\end{array} \\right)  }_{\\mathbf{U} } \\underbrace{\\left( \\begin{matrix}\\sqrt{6} &0&0\\\\ 0&1&0\\end{matrix} \\right)  }_{\\mathbf{\\Sigma } } \\underbrace{\\left( \\begin{array}{rrr}\\displaystyle \\frac{5}{\\sqrt{30} } &0&-\\displaystyle \\frac{1}{\\sqrt{6} } \\\\ -\\displaystyle \\frac{2}{\\sqrt{30} } &\\displaystyle \\frac{1}{\\sqrt{5} } &-\\displaystyle \\frac{2}{\\sqrt{6} } \\\\ \\displaystyle \\frac{1}{\\sqrt{30} } &\\displaystyle \\frac{2}{\\sqrt{5} } &\\displaystyle \\frac{1}{\\sqrt{6} } \\end{array} \\right)  }_{\\mathbf{V}^{\\top } } $$\n",
    "<p style=\"text-align: right;\">$(3.222)$</p>\n",
    "\n",
    "es la descomposición en valores singulares buscada. ◼︎\n",
    "\n",
    "**Ejemplo 3.25 – Implementación *high-level* de la descomposición SVD en <font color='purple'>Numpy</font>:** La librería <font color='purple'>Numpy</font> nos provee con una implementación bastante eficiente de la descomposición en valores singulares por medio de su módulo `numpy.linalg`. Puntualmente, la función `svd()` permite obtener tal descomposición sobre cualquier arreglo bidimensional que represente una matriz.\n",
    "\n",
    "Consideremos la matriz del ejemplo anterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "63a6078d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos la matriz A.\n",
    "A = np.array([\n",
    "    [1, 0, 1],\n",
    "    [-2, 1, 0],\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99aac20",
   "metadata": {},
   "source": [
    "La función `svd()` requiere como único argumento la matriz sobre la cual deseamos operar. Como resultado, se retorna una tupla con las matrices `U`, `S` y `V_t`, que representan las componentes de la descomposición propiamente tal. No obstante, debemos notar que `S` no es la matriz de diagonal $\\mathbf{\\Sigma}$ que se usa en la descomposición, sino un arreglo unidimensional donde se almacenan únicamente los valores singulares de `A`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7761e085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos la descomposición en valores singulares de A.\n",
    "U, S, V_t = linalg.svd(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4ba65752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.447,  0.894],\n",
       "       [-0.894,  0.447]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectores singulares por la izquierda.\n",
    "U.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "44ab57f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.913, -0.365,  0.183],\n",
       "       [-0.   ,  0.447,  0.894],\n",
       "       [-0.408, -0.816,  0.408]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectores singulares por la derecha.\n",
    "V_t.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0acaf69b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.449, 1.   ])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Valores singulares de A.\n",
    "S.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b8986167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de valores singulares.\n",
    "E = np.eye(M=V_t.shape[0], N=U.shape[0]) * S.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ca0ac9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recorstruimos la matriz A a partir de la descomposición SVD.\n",
    "A =(U @ E @ V_t).round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8cc597bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  1.],\n",
       "       [-2.,  1.,  0.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y la imprimimos en pantalla.\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c076d14",
   "metadata": {},
   "source": [
    "Y ahí lo tenemos. Se trata de una implementación eficiente y muy útil en la construcción de modelos de alta complejidad y que, por supuesto, nos ahorra el enorme trabajo manual que habíamos hecho previamente. ◼︎"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "87298cce",
   "metadata": {},
   "source": [
    "## Aproximación matricial.\n",
    "Hemos considerado la descomposición en valores singulares como una forma de factorizar una matriz $\\mathbf{A}\\in \\mathbb{R}^{m\\times n}$ en términos del producto entre tres matrices $\\mathbf{U}\\in \\mathbb{R}^{m\\times m}$, $\\mathbf{\\Sigma}\\in \\mathbb{R}^{m\\times n}$ y $\\mathbf{V}\\in \\mathbb{R}^{n\\times n}$, donde $\\mathbf{U}$ y $\\mathbf{V}$ son matrices ortogonales y $\\mathbf{\\Sigma}$ contiene los valores singulares de $\\mathbf{A}$ en su diagonal. En vez de realizar la descomposición SVD completa, ahora investigaremos cómo dicha descomposición nos permite representar una matriz $\\mathbf{A}$ como una suma de matrices más simples (siendo “más simples” por tener un rango menor que el rango de $\\mathbf{A}$), el cual tiende a conformar un esquema de aproximación matricial que resulta mucho menos costoso de calcular que la descomposición SVD completa.\n",
    "\n",
    "Podemos construir una matriz de rango 1, $\\mathbf{A}_{i}\\in \\mathbb{R}^{m\\times n}$ como\n",
    "\n",
    "$$\\mathbf{A}_{i}=\\mathbf{u}_{i} \\mathbf{v}_{i}^{\\top}$$\n",
    "<p style=\"text-align: right;\">$(3.223)$</p>\n",
    "\n",
    "La cual está conformada por el producto exterior entre el $i$-ésimo vector columna de $\\mathbf{U}$ con $i$-ésimo vector columna de $\\mathbf{V}$. \n",
    "\n",
    "La Fig. (3.9) muestra una imagen de Stonehenge, la cual está constituida por una matriz $\\mathbf{A}\\in \\mathbb{R}^{1432\\times 1910}$ (bajo la etiqueta *\"original image\"*), donde cada elemento de la matriz representa un determinado pixel con una cierta intensidad (y que es, por supuesto, un número real). Usaremos esta imagen como ejemplo de lo que podemos hacer, en términos de la **compresión de información** y **representatividad**, utilizando como base la aproximación matricial aprovechando la descomposición en valores singulares.\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"figures/fig_3_9.png\" width=\"800\"></p>\n",
    "<p style=\"text-align: center;\">Fig. (3.9): Procesamiento de una imagen de Stonhenge por medio de una descomposición SVD; (a) La imagen original corresponde a una matriz $\\mathbf{A}$ de 1432$\\times$1910 pixeles, donde cada valor de la matriz es un número real $a_{ij}$ tal que $0\\leq a_{ij}\\leq 1$, donde 0 representa un color totalmente negro, y 1 un color totalmente blanco. De (b) a (f) se muestran matrices de rango 1 a 5, $\\mathbf{A}_{1},...,\\mathbf{A}_{5}$, y sus correspondientes valores singulares $\\sigma_{1},...,\\sigma_{5}$. La estructura de tipo grilla de cada matriz de rango 1 en esta imagen queda definida por el producto exterior de los vectores singulares por la izquierda y por la derecha ($\\mathbf{U}$ y $\\mathbf{V}$) (imagen tomada del fantástico libro \"Mathematics for Machine Learning\", (Deinsenroth, M., Faisal, A. & Soon Ong, C.; 2021))\n",
    "\n",
    "Una matriz $\\mathbf{A}\\in \\mathbb{R}^{m\\times n}$ de rango $r$ puede ser escrita como una **suma de matrices de rango 1**, digamos $\\mathbf{A}_{i}$, de manera tal que\n",
    "\n",
    "$$\\mathbf{A} =\\sum^{r}_{i=1} \\sigma_{i} \\mathbf{u}_{i} \\mathbf{v}^{\\top }_{i} =\\sum^{r}_{i=1} \\sigma_{i} \\mathbf{A}_{i}$$\n",
    "<p style=\"text-align: right;\">$(3.224)$</p>\n",
    "\n",
    "Donde las matrices $\\mathbf{A}_{i}$ resultantes de los productos exteriores se ponderan por el $i$-ésimo valor singular $\\sigma_{i}$. Podemos observar por qué se cumple la ecuación (3.224): La estructura diagonal de la matriz de valores singulares $\\mathbf{\\Sigma}$ multiplica únicamente los correspondientes vectores $\\mathbf{u}_{i}\\mathbf{v}_{i}^{\\top}$ y los escala por los correspondientes valores singulares $\\sigma_{i}$. Todos los términos $\\sum\\nolimits_{ij} \\sigma_{i} \\mathbf{u}_{i} \\mathbf{v}^{\\top }_{i}$ se anulan para $i\\neq j$, porque $\\mathbf{\\Sigma}$ es una matriz diagonal. Cualquier término para el cual $i>r$ también se anula, porque los correspondientes valores singulares son también nulos.\n",
    "\n",
    "En la ecuación (3.223) introducimos las matrices de rango 1, denotadas como $\\mathbf{A}_{i}$. En este caso, sumamos hasta la $r$-ésima matriz de rango 1 para obtener la matriz $\\mathbf{A}$ de rango $r$. Si la suma no se realiza sobre las $r$ matrices $\\mathbf{A}_{i}$, sino que hasta un valor $k<r$, obtenemos lo que llamamos una **aproximación de rango $k$** para la matriz $\\mathbf{A}$, y que denotamos como $\\widehat{\\mathbf{A} } \\left( k\\right)$. Es decir,\n",
    "\n",
    "$$\\widehat{\\mathbf{A} } \\left( k\\right)  :=\\sum^{k}_{i=1} \\sigma_{i} \\mathbf{u}_{i} \\mathbf{v}^{\\top }_{i} =\\sum^{k}_{i=1} \\sigma_{i} \\mathbf{A}_{i}$$\n",
    "<p style=\"text-align: right;\">$(3.225)$</p>\n",
    "\n",
    "Donde $\\rho(\\mathbf{A})=k$. La Fig. (3.10) permite observar el efecto de las aproximaciones de bajo rango $\\widehat{\\mathbf{A} } \\left( k\\right)$ para la imagen original de Stonehenge representada, en este caso, por la matriz $\\mathbf{A}\\in \\mathbb{R}^{1432\\times 1910}$. La forma de estas rocas se vuelve incrementalmente visible y claramente reconocible en la aproximación de rango $k=5$. Mientras que la imagen original requiere un total de 1432$\\times$1910 = 2735120 vectores para su representación, la aproximación de rango 5 requiere almacenar únicamente 5 vectores singulares por la izquierda y por la derecha, respectivamente, conforme filas y columnas; es decir, un total de 5$\\times$(1432 + 1910 + 1) = 16715 números. Aproximadamente un 0.6% del total de valores necesarios en la matriz original.\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"figures/fig_3_10.png\" width=\"800\"></p>\n",
    "<p style=\"text-align: center;\">Fig. (3.10): 5 aproxoimaciones de baja densidad para la matriz $\\mathbf{A}$ que representa la imagen de Stonehenge (imagen tomada del fantástico libro \"Mathematics for Machine Learning\", (Deinsenroth, M., Faisal, A. & Soon Ong, C.; 2021))\n",
    "    \n",
    "Tomémonos un tiempo para entender lo importante que fue lo anterior: Usando un 0.6% de la data original, podemos representar la imagen a un nivel tal que es posible reconocer la formación rocosa de Stonehenge sin mayores problemas. Con aproximaciones ligeramente de mayor densidad (digamos, un 10% de la data original), podemos *comprimir* la imagen sin perder demasiada información relevante, lo que es de extrema importancia en el contexto de los llamados **modelos de reducción de dimensionalidad**: Muchas veces, cuando disponemos de conjuntos de datos con demasiadas variables, muchas de ellas son **redundantes** o, en realidad, comunican **poca información**.\n",
    "    \n",
    "Para medir la diferencia (error) entre $\\mathbf{A}$ y su aproximación de rango $k$, podemos utilizar un tipo particular de norma de matrices, que requiere de una definición particular.\n",
    "\n",
    "**<font color='blue'>Definición 3.19 – Norma espectral de una matriz:</font>** Para $\\mathbf{x} \\in \\mathbb{R}^{n} -\\left\\{ \\mathbf{0} \\right\\}$, definimos la **norma espectral** de la matriz $\\mathbf{A}\\in \\mathbb{R}^{m\\times n}$ como\n",
    "    \n",
    "$$\\left\\Vert \\mathbf{A} \\right\\Vert_{2}  :=\\max_{\\mathbf{x} } \\left( \\frac{\\left\\Vert \\mathbf{A} \\mathbf{x} \\right\\Vert_{2}  }{\\left\\Vert \\mathbf{x} \\right\\Vert_{2}  } \\right)  =\\sqrt{\\max_{j} \\left( \\lambda_{j} \\right)  \\mathbf{A}^{\\top } \\mathbf{A} }$$\n",
    "<p style=\"text-align: right;\">$(3.225)$</p>\n",
    "\n",
    "Donde $\\max_{j} \\left( \\lambda_{j} \\right)$ hace referencia al mayor de los autovalores de $\\mathbf{A}$. De esta manera, la norma espectral de una matriz nos permite entender qué tan largo puede hacerse un vector, como máximo, cuando lo multiplicamos por la matriz $\\mathbf{A}$.\n",
    "\n",
    "La definición (3.19) motiva los siguientes teoremas.\n",
    "\n",
    "**<font color='crimson'>Teorema 3.17:</font>** *La norma espectral de una matriz $\\mathbf{A}\\in \\mathbb{R}^{m\\times n}$ es igual a su valor singular de mayor magnitud.* ◆\n",
    "\n",
    "**<font color='crimson'>Teorema 3.18 – Eckart-Young:</font>** *Consideremos la matriz $\\mathbf{A}\\in \\mathbb{R}^{m\\times n}$ de rango $\\rho(\\mathbf{A})=r$, y sea $\\mathbf{B}\\in \\mathbb{R}^{m\\times n}$ otra matriz de rango $\\rho(\\mathbf{B})=k$. Entonces, para todo $k\\neq r$, con $\\widehat{\\mathbf{A} } \\left( k\\right)  =\\sum\\nolimits^{k}_{i=1} \\sigma_{i} \\mathbf{u}_{i} \\mathbf{v}^{\\top }_{i}$, se tiene que*\n",
    "\n",
    "$$\\widehat{\\mathbf{A} } \\left( k\\right)  =\\mathrm{argmin}_{\\rho \\left( \\mathbf{B} \\right)  =k} \\left\\Vert \\mathbf{A} -\\mathbf{B} \\right\\Vert_{2}  \\  ;\\  \\left\\Vert \\mathbf{A} -\\widehat{\\mathbf{A} } \\left( k\\right)  \\right\\Vert_{2}  =\\sigma_{k+1}$$\n",
    "<p style=\"text-align: right;\">$(3.226)$</p>\n",
    "◆"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
