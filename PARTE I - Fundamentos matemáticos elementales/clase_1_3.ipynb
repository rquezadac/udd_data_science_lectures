{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12635fd3",
   "metadata": {},
   "source": [
    "# CLASE 1.3: Descomposiciones matriciales\n",
    "\n",
    "## Introducci√≥n.\n",
    "En las secciones anteriores estudiamos algunas formas de manipular y obtener ciertas m√©tricas para los vectores, proyecciones de esos vectores con respecto a determinados subespacios vectoriales y transformaciones lineales. Las aplicaciones y transformaciones que permiten operar con vectores pueden ser convenientemente descritas por medio de matrices. Adem√°s, la mayor√≠a de los conjuntos de datos *bien comportados* que podemos encontrar en el mundo real vienen especificados en estructuras que pueden ser arregladas y/o representadas igualmente por medio de matrices. Por ejemplo, las filas de estos conjuntos de datos suelen representar **registros** u **observaciones** (como personas, fechas, unidades, entre otras) y las columnas suelen representar diferentes **atributos** para cada fila (como edad, altura, propiedades extensivas de alg√∫n fen√≥meno o el valor de alguna variable en el tiempo). En esta secci√≥n, presentaremos tres aspectos relativos a las matrices: C√≥mo **resumirlas**, como **descomponerlas** y como utilizar tales descomposiciones para construir **aproximaciones** para determinadas matrices.\n",
    "\n",
    "A diferencia de la secci√≥n anterior, en √©sta volveremos a escribir algo de c√≥digo, a fin de corresponder algunos conceptos esenciales con librer√≠as tales como <font color='purple'>Numpy</font> o <font color='purple'>Scipy</font>. No ser√° demasiado c√≥digo, pero s√≠ el suficiente para darle algo de sentido pr√°ctico a los conceptos que desarrollaremos desde la perspectiva computacional."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff217d6b",
   "metadata": {},
   "source": [
    "## Determinantes.\n",
    "\n",
    "### Un interludio previo.\n",
    "El concepto de determinante corresponde a otro de los elementos m√°s importantes del √°lgebra lineal. Corresponde a un objeto matem√°tico que es importante en el an√°lisis y soluci√≥n de sistemas de ecuaciones lineales (los mismos que vimos en el inicio de la [clase 1.1](https://github.com/rquezadac/udd_data_science_lectures/blob/main/PARTE%20I%20-%20Fundamentos%20matem%C3%A1ticos%20elementales/clase_1_1.ipynb)) y que puede expresarse por medio de una funci√≥n, denominada como **funci√≥n determinante**, y que permite aplicar cualquier matriz cuadrada de orden $n$ en el cuerpo $\\mathbb{K}$ donde sus elementos est√°n definidos. Dicha funci√≥n, para una matriz $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$, se denota como $\\det(\\mathbf{A})=|\\mathbf{A}|$, y es tal que $\\left| \\  \\cdot \\  \\right|  :\\mathbb{K}^{n} \\times \\mathbb{K}^{n} \\longrightarrow \\mathbb{K}$, pudi√©ndose escribir el determinante de la matriz $\\mathbf{A}$ como\n",
    "\n",
    "$$\\mathbf{A} =\\left\\{ a_{ij}\\in \\mathbb{K} \\right\\}  =\\left( \\begin{matrix}a_{11}&a_{12}&\\cdots &a_{1n}\\\\ a_{21}&a_{22}&\\cdots &a_{2n}\\\\ \\vdots &\\vdots &\\ddots &\\vdots \\\\ a_{n1}&a_{n2}&\\cdots &a_{nn}\\end{matrix} \\right)  \\in \\mathbb{K}^{n\\times n} \\wedge \\det \\left( \\mathbf{A} \\right)  =\\left| \\begin{matrix}a_{11}&a_{12}&\\cdots &a_{1n}\\\\ a_{21}&a_{22}&\\cdots &a_{2n}\\\\ \\vdots &\\vdots &\\ddots &\\vdots \\\\ a_{n1}&a_{n2}&\\cdots &a_{nn}\\end{matrix} \\right|  \\in \\mathbb{K}$$\n",
    "<p style=\"text-align: right;\">$(3.1)$</p>\n",
    "\n",
    "**Ejemplo 3.1:** Comenzaremos a motivar el estudio de los determinantes explorando la posibilidad de que una matriz cuadrada, digamos $\\mathbf{A}\\in \\mathbb{R}^{n\\times n}$, sea **invertible**. Para los casos de menor dimensi√≥n, ya conocemos los casos que aseguran que $\\mathbf{A}$ cumpla con esta condici√≥n. Por ejemplo, si $\\mathbf{A}\\in \\mathbb{R}^{1\\times 1}$ (es decir, $\\mathbf{A}$ es un escalar), sabemos que $\\mathbf{A}=a\\ \\Longrightarrow \\mathbf{A}^{-1}=1/a$, lo que implica que $\\mathbf{A}$ tiene una inversa siempre que $a\\neq 0$. Para el caso de matrices de $2\\times 2$, sabemos que la inversa $\\mathbf{A}^{-1}$ cumple con la condici√≥n de que $\\mathbf{A}\\mathbf{A}^{-1}=\\mathbf{I}_{2}$. De esta manera, podemos escribir\n",
    "\n",
    "$$\\mathbf{A} =\\left( \\begin{matrix}a_{11}&a_{12}\\\\ a_{21}&a_{22}\\end{matrix} \\right)  \\Longrightarrow \\mathbf{A} \\mathbf{A}^{-1} =\\mathbf{I}_{2} \\Longleftrightarrow \\mathbf{A}^{-1} =\\frac{1}{a_{11}a_{22}-a_{12}a_{21}} \\left( \\begin{matrix}a_{22}&-a_{12}\\\\ -a_{21}&a_{11}\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.2)$</p>\n",
    "\n",
    "Por lo tanto, $\\mathbf{A}$ es invertible si y s√≥lo si $a_{11}a_{22}-a_{12}a_{21}\\neq 0$. Para matrices de $2\\times 2$, dicha cantidad corresponde al **determinante** de la matriz respectiva. De esta manera, para la matriz $\\mathbf{A} =\\left\\{ a_{ij}\\right\\}  \\in \\mathbb{R}^{2\\times 2}$, su determinante se define como\n",
    "\n",
    "$$\\det \\left( \\mathbf{A} \\right)  =\\det \\left( \\begin{matrix}a_{11}&a_{12}\\\\ a_{21}&a_{22}\\end{matrix} \\right)  =\\left| \\begin{matrix}a_{11}&a_{12}\\\\ a_{21}&a_{22}\\end{matrix} \\right|  =a_{11}a_{22}-a_{12}a_{21}$$\n",
    "<p style=\"text-align: right;\">$(3.3)$</p>\n",
    "‚óºÔ∏é\n",
    "\n",
    "El ejemplo (3.1) permite establecer un hecho que puede ser generalizado a conjuntos de mayor dimensi√≥n: Una matriz es invertible siempre que su determinante no sea nulo. Formalicemos este hecho mediante un teorema.\n",
    "\n",
    "**<font color='crimson'>Teorema 3.1 ‚Äì Existencia de matriz inversa:</font>** *Sea $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$ una matriz cuadrada con elementos en el cuerpo $\\mathbb{K}$. Entonces $\\mathbf{A}$ se dir√° **invertible** o **no singular** (es decir, existe la matriz inversa $\\mathbf{A}^{-1}$) si y solo si $\\det(\\mathbf{A})\\neq 0$.*\n",
    "‚óÜ\n",
    "\n",
    "Ya disponemos de una expresi√≥n cerrada que permite calcular el determinante de cualquier matriz de dimensi√≥n $2\\times 2$. Sin embargo, no es tan sencillo generalizar dicho c√°lculo para dimensiones superiores. Por ejemplo, para el caso de matrices de $3\\times 3$, es com√∫n el c√°lculo de sus determinantes mediante la llamada regla de Sarrus:\n",
    "\n",
    "$$\\det \\left( \\begin{matrix}a_{11}&a_{12}&a_{13}\\\\ a_{21}&a_{22}&a_{23}\\\\ a_{31}&a_{32}&a_{33}\\end{matrix} \\right)  =a_{11}a_{22}a_{33}+a_{21}a_{32}a_{13}+a_{31}a_{12}a_{23}-a_{31}a_{22}a_{13}-a_{11}a_{32}a_{23}-a_{21}a_{12}a_{33}$$\n",
    "<p style=\"text-align: right;\">$(3.4)$</p>\n",
    "\n",
    "Si bien, en un principio, la regla de Sarrus parece una f√≥rmula complicada de entender, √©sta no es m√°s que un recurso mnemot√©cnico, ya que los triples productos involucrados en la f√≥rmula y sus signos guardan relaci√≥n con las diagonales (y subdiagonales) presentes en la matriz correspondiente, como se observa en el esquema de la Fig. (3.1)\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"figures/fig_3_1.png\" width=\"650\"></p>\n",
    "<p style=\"text-align: center;\">Fig. (3.1): Esquema que ilustra c√≥mo opera la regla de Sarrus</p>\n",
    "\n",
    "Recordemos que, al resolver sistemas de ecuaciones lineales, nuestro objetivo era transformar la matriz ampliada de un sistema en una tal que s√≥lo tuviera elementos no nulos en su regi√≥n superior derecha (matriz triangular superior), aunque tambi√©n es posible operar para llegar al caso opuesto, donde la matriz resultante tenga elementos no nulos en su regi√≥n inferior izquierda (matriz triangular inferior). Este tipo de matrices fueron formalizadas previamente en la definici√≥n (1.7).\n",
    "\n",
    "Para una matriz triangular, digamos $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$, definimos su determinante como\n",
    "\n",
    "$$\\det \\left( \\mathbf{A} \\right)  =\\prod^{n}_{i=1} a_{ii}$$\n",
    "<p style=\"text-align: right;\">$(3.5)$</p>\n",
    "\n",
    "Donde $a_{ii}$ es el correspondiente elemento relativo a la diagonal principal de la matriz $\\mathbf{A}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2de076",
   "metadata": {},
   "source": [
    "### Interpretaci√≥n geom√©trica del determinante.\n",
    "Si una matriz $\\mathbf{A}\\in \\mathbb{R}^{n\\times n}$ tiene elementos $a_{ij}\\in \\mathbb{R}$, entonces puede ser utilizada para representar dos transformaciones lineales. Una que aplica la base can√≥nica de $\\mathbb{R}^{n}$ a las filas de $\\mathbf{A}$, y otra que aplica la misma base a las columnas de $\\mathbf{A}$. Cualquiera sea el caso, si la matriz $\\mathbf{A}$ es de $2\\times 2$, las im√°genes de cada uno de los vectores de la base can√≥nica de $\\mathbb{R}^{2}$ forman un paralel√≥gramo que representa la imagen del cuadrado unitario bajo la transformaci√≥n lineal respectiva, y cuyos v√©rtices se corresponden con combinaciones de los elementos de $\\mathbf{A}$, como se observa en la Fig. (3.2a).\n",
    "\n",
    "Si $\\mathbf{A} =\\left\\{ {}a_{ij}\\right\\}  \\in \\mathbb{R}^{2\\times 2}$ es la matriz que conforma los v√©rtices del paralel√≥gramo en la Fig. (3.2a), se tendr√° que el √°rea encerrada por el mismo ser√° igual a $\\det \\left( \\mathbf{A} \\right)  =a_{11}a_{22}-a_{12}a_{21}$. Para mostrar este resultado, podemos considerar que los elementos de la matriz $\\mathbf{A}$ corresponden a vectores que representan los v√©rtices del paralel√≥gramo. Si uno de los v√©rtices es el origen del sistema de coordenadas, los vectores $\\mathbf{a} =\\left( a_{11},a_{12}\\right)$ y $\\mathbf{b} =\\left( a_{21},a_{22}\\right)$ ser√°n los v√©rtices m√°s cercanos al origen, mientras que el v√©rtice opuesto ser√° igual a la suma $\\mathbf{a} +\\mathbf{b} =\\left( a_{11}+a_{21},a_{12}+a_{22}\\right)$. El √°rea del paralel√≥gramo puede expresarse igualmente como $\\left\\Vert \\mathbf{a} \\right\\Vert  \\left\\Vert \\mathbf{b} \\right\\Vert  \\mathrm{sen} \\left( \\theta \\right)$, donde $\\theta$ es el √°ngulo formado por los vectores $\\mathbf{a}$ y $\\mathbf{b}$. Si consideramos la proyecci√≥n ortogonal del vector $\\mathbf{a}$ sobre $\\mathbf{b}$ (que llamamos $\\pi_{\\mathbf{b}}(\\mathbf{a})$), podemos escribir\n",
    "\n",
    "$$\\mathrm{Area} =\\left\\Vert \\mathbf{a} \\right\\Vert  \\left\\Vert \\mathbf{b} \\right\\Vert  \\mathrm{sen} \\left( \\theta \\right)  =\\left\\Vert \\pi_{\\mathbf{b} } \\left( \\mathbf{a} \\right)  \\right\\Vert  \\left\\Vert \\mathbf{b} \\right\\Vert  \\cos \\left( \\frac{\\pi }{2} -\\theta \\right)  =a_{11}a_{12}-a_{12}a_{21}=\\det \\left( \\mathbf{A} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.6)$</p>\n",
    "\n",
    "La interpretaci√≥n geom√©trica anterior puede extenderse al caso de matrices de $3\\times 3$, considerando en este caso un paralelep√≠pedo generado por las submatrices columna que generan la matriz completa $\\mathbf{A} =\\left\\{ a_{ij}\\right\\}  \\in \\mathbb{R}^{3\\times 3}$. Si tales submatrices son representadas como $\\mathbf{r}_{1}$, $\\mathbf{r}_{2}$ y $\\mathbf{r}_{3}$, entonces el volumen del paralelep√≠pedo es igual a $V=\\det \\left( \\left( \\mathbf{r}_{1} ,\\mathbf{r}_{2} ,\\mathbf{r}_{3} \\right)  \\right)  =\\det \\left( \\mathbf{A} \\right)$, tal y como se ilustra en la Fig (3.2b).\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"figures/fig_3_2.png\" width=\"900\"></p>\n",
    "<p style=\"text-align: center;\">Fig. (3.2): (a) Esquema que muestra c√≥mo el determinante de una matriz de $2\\times 2$ permite transformar un cuadrado unitario en un paralel√≥gramo cuyos v√©rtices son las productos que componen dicho determinante y su √°rea ser√° igual al valor de tal determinante; (b) Misma transformaci√≥n para el caso de una matriz de $3\\times 3$. En este caso, la transformaci√≥n se aplica sobre un paralelep√≠pedo, obteni√©ndose un trapezoedro cuyo volumen es igual al correspondiente determinante</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517bd95f",
   "metadata": {},
   "source": [
    "### Propiedades de los determinantes.\n",
    "El c√°lculo de un determinante para una matriz arbitraria $\\mathbf{A} =\\left\\{ a_{ij}\\right\\}  \\in \\mathbb{R}^{n\\times n}$ requiere de un algoritmo generalizado para poder resolver los casos en los cuales $n>3$. Existen varios procedimientos para ello, siendo indudablemente el m√°s popular el **m√©todo de Laplace**, que definiremos a continuaci√≥n.\n",
    "\n",
    "**<font color='blue'>Definici√≥n 3.1 ‚Äì Determinante:</font>** Sea $\\mathbf{A} =\\left\\{ a_{ij}\\right\\}  \\in \\mathbb{R}^{n\\times n}$. Definimos el **determinante** de la matriz $\\mathbf{A}$ como\n",
    "\n",
    "1. Respecto a la columna $j$: $\\det \\left( \\mathbf{A} \\right)  =\\sum^{n}_{k=1} \\left( -1\\right)^{k+j}  a_{kj}\\det \\left( \\mathbf{A}_{kj} \\right)$.\n",
    "2. Respecto a la fila $j$: $\\det \\left( \\mathbf{A} \\right)  =\\sum^{n}_{k=1} \\left( -1\\right)^{k+j}  a_{jk}\\det \\left( \\mathbf{A}_{jk} \\right)$.\n",
    "\n",
    "En la f√≥rmulas anteriores, conocidas en la pr√°ctica como **expansiones de Laplace**, $\\mathbf{A}_{jk}$ corresponde a la submatriz resultante de eliminar de $\\mathbf{A}$ la fila $j$ y la columna $k$ y se denomina como **menor complementario** en la posici√≥n $(j, k)$, mientras que el n√∫mero real $\\triangle_{jk} =\\left( -1\\right)^{k+j}  a_{jk}$ es llamado **cofactor** asociado al $jk$-√©simo elemento de la matriz $\\mathbf{A}$.\n",
    "\n",
    "Resulta sencillo darnos cuenta de que, si bien las expansiones de Laplace nos permiten obtener una f√≥rmula cerrada para el c√°lculo del determinante de cualquier matriz cuadrada, su tiempo de ejecuci√≥n y complejidad computacional escala enormemente con la dimensi√≥n de la matriz para la cual queremos calcular su determinante. Por esa raz√≥n, en t√©rminos algebraicos, es mejor considerar ciertas propiedades que se desprenden directamente de la definici√≥n que hemos ido construyendo del determinante a fin de disponer de m√©todos m√°s efectivos para su c√°lculo en dimensiones superiores (considerando, adem√°s, las transformaciones elementales sobre matrices que hemos aprendido previamente). Vamos, por tanto, a desarrollar tales propiedades:\n",
    "\n",
    "- **(P1) ‚Äì Invariancia ante la transposici√≥n:** Si $\\mathbf{A} =\\left\\{ a_{{}ij}\\right\\}  \\in \\mathbb{R}^{n\\times n}$, entonces, de la definici√≥n (3.1), se tiene que $\\det(\\mathbf{A})=\\det(\\mathbf{A}^{\\top})$, ya que $\\det \\left( \\mathbf{A} \\right)  =\\sum^{n}_{k=1} \\triangle_{jk} \\det \\left( \\mathbf{A}_{jk} \\right)  =\\sum^{n}_{s=1} \\triangle_{sj} \\det \\left( \\mathbf{A}_{sj} \\right)$.\n",
    "- **(P2) ‚Äì Columna o fila nula:** Si $\\mathbf{A} =\\left\\{ a_{{}ij}\\right\\}  \\in \\mathbb{R}^{n\\times n}$ posee una columna o fila nula (conformada √∫nicamente por elementos iguales a cero), entonces $\\det(\\mathbf{A})=0$.\n",
    "- **(P3) ‚Äì Determinante de un producto de matrices:** Si $\\mathbf{A},\\mathbf{B}\\in \\mathbb{R}^{n\\times n}$, entonces $\\det(\\mathbf{A}\\mathbf{B})=\\det(\\mathbf{A})\\det(\\mathbf{B})$.\n",
    "- **(P4) ‚Äì Determinante de la matriz inversa:** Si $\\mathbf{A} =\\left\\{ a_{{}ij}\\right\\}  \\in \\mathbb{R}^{n\\times n}$ es una matriz no singular, entonces $\\det \\left( \\mathbf{A}^{-1} \\right)  =1/\\det \\left( \\mathbf{A} \\right)$.\n",
    "- **(P5) ‚Äì Invariancia ante operaciones elementales:** Cualquier matriz $\\mathbf{A}\\in \\mathbb{R}^{n\\times n}$ mantiene el valor de su determinante, aunque hayamos operado sobre ella mediante cualquier de las transformaciones elementales vistas en la [clase 1.1](https://github.com/rquezadac/udd_data_science_lectures/blob/main/PARTE%20I%20-%20Fundamentos%20matem%C3%A1ticos%20elementales/clase_1_1.ipynb).\n",
    "- **(P6) ‚Äì Escalamiento del determinante:** Sea $\\mathbf{A} =\\left\\{ a_{ij}\\right\\}  \\in \\mathbb{R}^{n\\times n}$ y $\\lambda \\in \\mathbb{R}$. Entonces $\\det \\left( \\lambda \\mathbf{A} \\right)  =\\lambda^{n} \\det \\left( \\mathbf{A} \\right)$.\n",
    "- **(P7) ‚Äì Columna o fila repetida:** Si una matriz $\\mathbf{A} =\\left\\{ a_{ij}\\right\\}  \\in \\mathbb{R}^{n\\times n}$ tiene columnas o filas repetidas (o, m√°s general, linealmente dependientes), entonces $\\det(\\mathbf{A})=0$. Esta propiedad generaliza **(P2)** y establece que toda matriz $\\mathbf{A}$ no singular tiene rango completo.\n",
    "\n",
    "**Ejemplo 3.2:** Calcularemos el determinante de la matriz $\\mathbf{A}\\in \\mathbb{R}^{5\\times 5}$, definida como\n",
    "\n",
    "$$\\mathbf{A} =\\left( \\begin{matrix}0&1&0&1&0\\\\ -1&a&0&0&0\\\\ 0&0&a&0&0\\\\ -1&0&0&a&0\\\\ 0&0&0&0&a\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.7)$</p>\n",
    "\n",
    "En efecto, aplicando la definici√≥n (3.1) y transformaciones elementales,\n",
    "\n",
    "$$\\begin{array}{rcl}\\det \\left( \\begin{matrix}0&1&0&1&0\\\\ -1&a&0&0&0\\\\ 0&0&a&0&0\\\\ -1&0&0&a&0\\\\ 0&0&0&0&a\\end{matrix} \\right)  &\\overbrace{=}^{\\mathrm{definicion} } &\\underbrace{a}_{\\mathrm{cofactor} \\  \\triangle_{55} } \\det \\left( \\begin{matrix}0&1&0&1\\\\ -1&a&0&0\\\\ 0&0&a&0\\\\ -1&0&0&a\\end{matrix} \\right)  \\\\ &\\overbrace{=}^{F_{42}\\left( -1\\right)  } &a\\det \\left( \\begin{matrix}0&1&0&1\\\\ -1&a&0&0\\\\ 0&0&a&0\\\\ 0&-a&0&a\\end{matrix} \\right)  \\\\ &\\overbrace{=}^{\\mathrm{definicion} } &\\underbrace{1}_{\\mathrm{cofactor} \\  \\triangle_{21} } \\cdot a\\det \\left( \\begin{matrix}1&0&1\\\\ 0&a&0\\\\ -a&0&a\\end{matrix} \\right)  \\\\ &\\overbrace{=}^{\\mathrm{definicion} } &a^{2}\\det \\left( \\begin{matrix}1&1\\\\ -a&a\\end{matrix} \\right)  \\overbrace{=}^{\\mathrm{definicion} } 2a^{3}\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.8)$</p>\n",
    "‚óºÔ∏é\n",
    "\n",
    "**Ejemplo 3.3 ‚Äì Los determinantes en <font color='purple'>Numpy</font>:** En librer√≠as de Python especializadas en el uso de arreglos vectorizados, como <font color='purple'>Numpy</font>, es razonable esperar que existan rutinas prefabricadas para el c√°lculo de determinantes. En particular, podemos usar la funci√≥n `det()`, del m√≥dulo de √°lgebra lineal `numpy.linalg()` para calcular el determinante de cualquier matriz expresada por medio de un arreglo bidimensional. Por ejemplo, si consideramos la matriz $\\mathbf{A}\\in \\mathbb{R}^{5\\times 5}$, definida como\n",
    "\n",
    "$$\\mathbf{A} =\\left( \\begin{matrix}0&-1&2&-1&0\\\\ 3&-1&2&2&0\\\\ 6&-1&0&0&9\\\\ 0&1&4&-5&9\\\\ 2&2&-4&5&-3\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.9)$</p>\n",
    "\n",
    "Podemos calcular su determinante f√°cilmente en <font color='purple'>Numpy</font> definiendo, primeramente, un arreglo bidimensional, digamos `A`, donde almacenamos esta matriz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4930228f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8ae75cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el arreglo en cuesti√≥n.\n",
    "A = np.array([\n",
    "    [0, -1, 2, -1, 0],\n",
    "    [3, -1, 2, 2, 0],\n",
    "    [6, -1, 0, 0, 9],\n",
    "    [0, 1, 4, -5, 9],\n",
    "    [2, 2, -4, 5, -3],\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edb371a",
   "metadata": {},
   "source": [
    "Y luego aplicando la funci√≥n `np.linalg.det()` para calcular su determinante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "babed61a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-162.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculamos el determinante de A (redondeado a 3 decimales).\n",
    "np.around(np.linalg.det(A), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808cdb32",
   "metadata": {},
   "source": [
    "Vemos pues que no fue nada dif√≠cil calcular el determinante de una matriz de $5\\times 5$ en <font color='purple'>Numpy</font>. Sin embargo, tal y como comentamos previamente, el c√°lculo de determinantes corresponde a un esfuerzo computacional ostensiblemente grande y que escala enormemente a medida que aumentan las dimensiones de las matrices de inter√©s. Incluso trabajando con una librer√≠a muy eficiente como <font color='purple'>Numpy</font>, los tiempos de ejecuci√≥n pueden verse muy afectados. Para ejemplificar aquello, consideraremos el c√°lculo de los determinantes de cuatro matrices $\\mathbf{A}\\in \\mathbb{R}^{10\\times 10}$, $\\mathbf{B}\\in \\mathbb{R}^{100\\times 100}$, $\\mathbf{C}\\in \\mathbb{R}^{1000\\times 1000}$ y $\\mathbf{D}\\in \\mathbb{R}^{10000\\times 10000}$, las que representaremos mediante los arreglos bidimensionales `A`, `B`, `C` y `D`, y que estar√°n compuestas por n√∫meros reales uniformemente distribuidos entre 0 y 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b78e9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos una semilla aleatoria fija.\n",
    "rng = np.random.default_rng(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16246b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos algunas matrices de distintos tama√±os.\n",
    "A = rng.random(size=(10, 10))\n",
    "B = rng.random(size=(100, 100))\n",
    "C = rng.random(size=(1000, 1000))\n",
    "D = rng.random(size=(10000, 10000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc009552",
   "metadata": {},
   "source": [
    "Vamos a estimar el tiempo de ejecuci√≥n asociado al c√°lculo de los determinantes de estas matrices, a fin de observar qu√© tal escala con respecto al incremento en dimensionalidad de las mismas. Notemos que cada matriz es 10 veces m√°s grande que su antecesora:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d33dea85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.04 ¬µs ¬± 85.6 ns per loop (mean ¬± std. dev. of 7 runs, 100,000 loops each)\n",
      "91.4 ¬µs ¬± 2.22 ¬µs per loop (mean ¬± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/felipequezada/opt/anaconda3/lib/python3.9/site-packages/numpy/linalg/linalg.py:2158: RuntimeWarning: overflow encountered in det\n",
      "  r = _umath_linalg.det(a, signature=signature)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.18 ms ¬± 603 ¬µs per loop (mean ¬± std. dev. of 7 runs, 100 loops each)\n",
      "3.93 s ¬± 444 ms per loop (mean ¬± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.linalg.det(A)\n",
    "%timeit np.linalg.det(B)\n",
    "%timeit np.linalg.det(C)\n",
    "%timeit np.linalg.det(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e00d0d3",
   "metadata": {},
   "source": [
    "Podemos observar que el c√°lculo del determinante de `B` tiene un tiempo de ejecuci√≥n 12 veces mayor que el c√°lculo del determinante de `A`. El c√°lculo del determinante de `C` tiene un tiempo de ejecuci√≥n de aproximadamente unas 82 veces superior al del c√°lculo del determinante de `B` (y, por extensi√≥n, 1038 veces m√°s lento que el c√°lculo del determinante de `A`). Y el c√°lculo del determinante de `D` tiene un tiempo de ejecuci√≥n aproximadamente unas 445 veces m√°s lento que el c√°lculo del determinante de `C` (y, por extensi√≥n... ¬°es m√°s de 36000 veces m√°s lento que el c√°lculo del determinante de `B`, y m√°s de 460000 veces m√°s lento que el c√°lculo del determinante de `A`!). Esto definitivamente nos har√° pensarlo dos veces antes de calcular determinantes en el mundo real, donde resulta com√∫n vernos enfrentados a bases de datos con cientos de miles de registros. ‚óºÔ∏é \n",
    "\n",
    "**Ejemplo 3.4:** Vamos a demostrar que\n",
    "\n",
    "$$\\det \\left( \\begin{matrix}1&1&1\\\\ x&y&z\\\\ x^{2}&y^{2}&z^{2}\\end{matrix} \\right)  =\\left( x-y\\right)  \\left( y-z\\right)  \\left( z-x\\right)$$\n",
    "<p style=\"text-align: right;\">$(3.10)$</p>\n",
    "\n",
    "En efecto, utilizando transformaciones elementales, propiedades de los determinantes y la definici√≥n (3.1), tenemos que\n",
    "\n",
    "$$\\begin{array}{rcl}\\det \\left( \\begin{matrix}1&1&1\\\\ x&y&z\\\\ x^{2}&y^{2}&z^{2}\\end{matrix} \\right)  &\\overbrace{=}^{F_{21}\\left( -x\\right)  } &\\det \\left( \\begin{matrix}1&1&1\\\\ 0&y-x&z-x\\\\ x^{2}&y^{2}&z^{2}\\end{matrix} \\right)  \\\\ &\\overbrace{=}^{F_{31}\\left( -x^{2}\\right)  } &\\det \\left( \\begin{matrix}1&1&1\\\\ 0&y-x&z-x\\\\ 0&y^{2}-x^{2}&z^{2}-x^{2}\\end{matrix} \\right)  \\\\ &\\overbrace{=}^{\\mathrm{definicion} } &\\det \\left( \\begin{matrix}y-x&z-x\\\\ y^{2}-x^{2}&z^{2}-x^{2}\\end{matrix} \\right)  \\\\ &=&\\left( y-z\\right)  \\left( z^{2}-x^{2}\\right)  -\\left( z-x\\right)  \\left( y^{2}-x^{2}\\right)  \\\\ &=&\\left( y-x\\right)  \\left( z-x\\right)  \\left( z+x-y+x\\right)  \\\\ &=&\\left( y-x\\right)  \\left( z-x\\right)  \\left( z-y\\right)  \\\\ &=&\\left( x-y\\right)  \\left( y-z\\right)  \\left( z-x\\right)  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.11)$</p>\n",
    "\n",
    "Tal como quer√≠amos demostrar. ‚óºÔ∏é\n",
    "\n",
    "**Ejemplo 3.5:** Vamos a determinar todos los valores de $a\\in \\mathbb{R}$ tales que\n",
    "\n",
    "$$\\det \\left( \\begin{matrix}1&a&a^{2}&\\left( a^{3}+6\\right)  \\\\ 1&1&1&\\left( 1+6a\\right)  \\\\ 1&2&4&\\left( 8+3a\\right)  \\\\ 1&3&9&\\left( 27+2a\\right)  \\end{matrix} \\right)  =0$$\n",
    "<p style=\"text-align: right;\">$(3.12)$</p>\n",
    "\n",
    "En efecto,\n",
    "\n",
    "$$\\begin{array}{rcl}\\det \\left( \\begin{matrix}1&a&a^{2}&\\left( a^{3}+6\\right)  \\\\ 1&1&1&\\left( 1+6a\\right)  \\\\ 1&2&4&\\left( 8+3a\\right)  \\\\ 1&3&9&\\left( 27+2a\\right)  \\end{matrix} \\right)  &\\overbrace{=}^{\\begin{array}{c}F_{21}\\left( -1\\right)  \\\\ F_{31}\\left( -1\\right)  \\\\ F_{41}\\left( -1\\right)  \\end{array} } &\\det \\left( \\begin{matrix}1&a&a^{2}&\\left( a^{3}+6\\right)  \\\\ 0&\\left( 1-a\\right)  &\\left( 1-a^{2}\\right)  &\\left( 1-a^{3}+6\\left( a-1\\right)  \\right)  \\\\ 0&\\left( 2-a\\right)  &\\left( 4-a^{2}\\right)  &\\left( 8-a^{3}+3a-6\\right)  \\\\ 0&\\left( 3-a\\right)  &\\left( 9-a^{2}\\right)  &\\left( 27-a^{3}+2a-6\\right)  \\end{matrix} \\right)  \\\\ &\\overbrace{=}^{\\mathrm{definicion} } &\\det \\left( \\begin{matrix}\\left( 1-a\\right)  &\\left( 1-a^{2}\\right)  &\\left( 1-a^{3}+6\\left( a-1\\right)  \\right)  \\\\ \\left( 2-a\\right)  &\\left( 4-a^{2}\\right)  &\\left( 8-a^{3}+3a-6\\right)  \\\\ \\left( 3-a\\right)  &\\left( 9-a^{2}\\right)  &\\left( 27-a^{3}+2a-6\\right)  \\end{matrix} \\right)  \\\\ &=&\\det \\left( \\begin{matrix}\\left( 1-a\\right)  &\\left( 1-a^{2}\\right)  &\\left[ \\left( 1-a\\right)  \\left( a^{2}+a+1\\right)  +6\\left( a-1\\right)  \\right]  \\\\ \\left( 2-a\\right)  &\\left( 4-a^{2}\\right)  &\\left[ \\left( 2-a\\right)  \\left( a^{2}+2a+4\\right)  +3\\left( a-2\\right)  \\right]  \\\\ \\left( 3-a\\right)  &\\left( 9-a^{2}\\right)  &\\left[ \\left( 3-a\\right)  \\left( a^{2}+3a+9\\right)  +2\\left( a-3\\right)  \\right]  \\end{matrix} \\right)  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.13)$</p>\n",
    "\n",
    "Es claro, conforme el desarrollo anterior, que el determinante se anula cuando $a=1$, $a=2$ o $a=3$. Para $a\\neq 1$, $a\\neq 2$ y $a\\neq 3$, proseguimos con el desarrollo del determinante, con lo cual,\n",
    "\n",
    "$$\\begin{array}{rcl}\\det \\left( \\begin{matrix}1&a&a^{2}&\\left( a^{3}+6\\right)  \\\\ 1&1&1&\\left( 1+6a\\right)  \\\\ 1&2&4&\\left( 8+3a\\right)  \\\\ 1&3&9&\\left( 27+2a\\right)  \\end{matrix} \\right)  &\\overbrace{=}^{\\mathrm{propiedades} } &\\left( 1-a\\right)  \\left( 2-a\\right)  \\left( 3-a\\right)  \\det \\left( \\begin{matrix}1&\\left( 1+a\\right)  &\\left( a^{2}+a-5\\right)  \\\\ 1&\\left( 2+a\\right)  &\\left( a^{2}+2a+1\\right)  \\\\ 1&\\left( 3+a\\right)  &\\left( a^{2}+3a+7\\right)  \\end{matrix} \\right)  \\\\ &\\overbrace{=}^{\\begin{array}{c}F_{21}\\left( -1\\right)  \\\\ F_{31}\\left( -1\\right)  \\end{array} } &\\left( 1-a\\right)  \\left( 2-a\\right)  \\left( 3-a\\right)  \\det \\left( \\begin{matrix}1&\\left( 1+a\\right)  &\\left( a^{2}+a-5\\right)  \\\\ 0&1&\\left( a+6\\right)  \\\\ 0&2&\\left( a+6\\right)  \\end{matrix} \\right)  \\\\ &=&2\\left( 1-a\\right)  \\left( 2-a\\right)  \\left( 3-a\\right)  \\det \\left( \\begin{matrix}1&\\left( 1+a\\right)  &\\left( a^{2}+a-5\\right)  \\\\ 0&1&\\left( a+6\\right)  \\\\ 0&1&\\left( a+6\\right)  \\end{matrix} \\right)  \\\\ &=&0\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.14)$</p>\n",
    "\n",
    "Luego tenemos que $\\det(\\mathbf{A})=0$ para todo $a\\in \\mathbb{R}$. ‚óºÔ∏é"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e704cec",
   "metadata": {},
   "source": [
    "## Diagonalizaci√≥n de matrices.\n",
    "Una vez estudiado el concepto de determinante, vamos a ocuparnos de un problema m√°s general y que consiste en saber cuando, para una transformaci√≥n lineal del tipo $T:\\mathbb{R}^{n}\\longrightarrow \\mathbb{R}^{n}$, es posible encontrar una base $\\alpha$ con respecto a la cual la matriz asociada $\\mathbf{A}=[T]_{\\alpha}^{\\alpha}$ sea de tipo **diagonal**. De manera equivalente, queremos determinar las condiciones para las cuales una matriz $\\mathbf{A}\\in \\mathbb{R}^{n\\times n}$ puede *descomponerse* de la forma\n",
    "\n",
    "$$\\mathbf{A} =\\mathbf{P} \\mathbf{D} \\mathbf{P}^{-1}$$\n",
    "<p style=\"text-align: right;\">$(3.15)$</p>\n",
    "\n",
    "Donde $\\mathbf{D}\\in \\mathbb{R}^{n\\times n}$ es una matriz diagonal.\n",
    "\n",
    "Este problema de naturaleza puramente algebraica tiene una cantidad significativa de aplicaciones en otras ramas de las matem√°ticas, como en ecuaciones diferenciales, estad√≠stica y, por supuesto, en machine learning. Puntualmente, la diagonalizaci√≥n es un procedimiento esencial en la derivaci√≥n de la descomposici√≥n de matrices en **valores singulares** y que, a su vez, constituye la base del **an√°lisis de componentes principales**, uno de los modelos de aprendizaje no supervisado m√°s utilizados para la reducci√≥n de la dimensi√≥n de conjuntos de datos con un elevado n√∫mero de variables, sin perder una cantidad significativa de informaci√≥n. Tal vez el teorema m√°s importante de esta subsecci√≥n es el que dice que toda matriz sim√©trica puede representarse mediante la expresi√≥n (3.15).\n",
    "\n",
    "### Autovalores y autovectores.\n",
    "Para comenzar con el estudio de la diagonalizaci√≥n de matrices, primero introduciremos algunos conceptos y resultados esenciales.\n",
    "\n",
    "**<font color='blue'>Definici√≥n 3.2 ‚Äì Autovalores y autovectores:</font>** Sea $V$ un $\\mathbb{K}$-espacio vectorial y $T:V\\longrightarrow V$ una transformaci√≥n lineal. Diremos que $v\\in V$ es un **autovector o vector propio** de $T$ si se cumplen las siguientes condiciones:\n",
    "\n",
    "- **(C1):** $v\\in O_{V}$.\n",
    "- **(C2):** Existe un escalar $\\lambda \\in \\mathbb{K}$ tal que $T(v)=\\lambda v$.\n",
    "\n",
    "El escalar $\\lambda$ se denomina **autovalor o valor propio** asociado al autovector $v$.\n",
    "\n",
    "Equivalentemente, diremos que $v\\in V-\\left\\{ O_{V}\\right\\}$ es un autovector asociado a la matriz $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$ si $v$ es un autovector de la transformaci√≥n lineal $T:V\\longrightarrow V$ expl√≠citamente definida como $T(v)=\\mathbf{A}v$. Es decir, existe un escalar $\\lambda \\in \\mathbb{K}$ tal que $\\mathbf{A}v=\\lambda v$. De la misma forma, diremos que $\\lambda$ es un autovalor de la matriz $\\mathbf{A}$.\n",
    "\n",
    "**<font color='crimson'>Teorema 3.2:</font>** *Dada una matriz $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$, si $\\lambda \\in \\mathbb{K}$ es un autovalor de $\\mathbf{A}$, entonces las siguientes expresiones son equivalentes:*\n",
    "\n",
    "- **(T1):** $\\exists v\\neq O_{V}\\  |\\  \\mathbf{A} v=\\lambda v$, *donde $V$ es un $\\mathbb{K}$-espacio vectorial y $v$ es un autovector de la matriz $\\mathbf{A}$.*\n",
    "- **(T2):** $\\exists v\\in V$ *tal que $v$ es una soluci√≥n no trivial del sistema de ecuaciones $\\left( \\mathbf{A} -\\lambda \\mathbf{I}_{n} \\right)  v=O_{V}$.*\n",
    "- **(T3):** $\\ker \\left( \\mathbf{A} -\\lambda \\mathbf{I}_{n} \\right)  \\neq \\left\\{ O_{V}\\right\\}$.\n",
    "- **(T4):** $\\left( \\mathbf{A} -\\lambda \\mathbf{I}_{n} \\right)$ *es una matriz no invertible*.\n",
    "‚óÜ\n",
    "\n",
    "Queda claro pues que necesitamos una forma sencilla de determinar qu√© valores de $\\lambda \\in \\mathbb{K}$ son, en efecto, autovalores. Para ello, es √∫til reconocer que la condici√≥n **(T4)** en el teorema (3.2) puede expresarse como una ecuaci√≥n en la variable $\\lambda$. Por supuesto, es ac√° donde cobra sentido el desarrollo que hicimos del concepto de determinante de una matriz $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$, puesto que, como ya verificamos con el teorema (3.1), toda matriz es no singular (invertible) si su determinante es no nulo. Por lo tanto, la condici√≥n **(T4)** puede expresarse como $\\det \\left( \\mathbf{A} -\\lambda \\mathbf{I}_{n} \\right)  =0$, donde $\\mathbf{I}_{n}$ es la matriz identidad.\n",
    "\n",
    "Tiene sentido, por lo tanto, la siguiente definici√≥n.\n",
    "\n",
    "**<font color='blue'>Definici√≥n 3.3 ‚Äì Polinomio caracter√≠stico:</font>** Sea $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$ una matriz tal que √©sta coincide con la representaci√≥n matricial de una transformaci√≥n lineal $T:V\\longrightarrow V$ que opera sobre el $\\mathbb{K}$-espacio vectorial $V$. La expresi√≥n $P_{T}\\left( \\lambda \\right)  =\\det \\left( \\mathbf{A} -\\lambda \\mathbf{I}_{n} \\right)  \\in \\mathbb{K}_{n} \\left[ \\lambda \\right]$ ser√° llamada **polinomio caracter√≠stico** de la matriz $\\mathbf{A}$ (y, por extensi√≥n, de la transformaci√≥n lineal $T$).\n",
    "\n",
    "De la definici√≥n (3.2), se tiene que $v\\in V$ es un autovector de $\\mathbf{A}$ si $v\\neq O_{V}$ y existe un escalar $\\lambda \\in \\mathbb{K}$ tal que $\\mathbf{A}v=\\lambda v$. De esta manera, podemos verificar que $\\lambda$ es un autovalor de la matriz $\\mathbf{A}$ si y s√≥lo si es una soluci√≥n no nula de la ecuaci√≥n\n",
    "\n",
    "$$\\left( \\mathbf{A} -\\lambda \\mathbf{I}_{n} \\right)  v=O_{V}$$\n",
    "<p style=\"text-align: right;\">$(3.16)$</p>\n",
    "\n",
    "As√≠ pues, todo escalar $\\lambda \\in \\mathbb{K}$ que satisfaga (3.16) ser√° un autovalor de $\\mathbf{A}$. Notemos adem√°s que, conforme la expresi√≥n anterior, si $v$ es un autovector, tambi√©n lo es cualquier otro vector que sea linealmente dependiente con respecto a $v$. Es decir, si ùë£ es un autovector de la matriz $\\mathbf{A}$, entonces tambi√©n lo es $\\alpha v;\\forall v\\in \\mathbb{K}$. M√°s a√∫n, si $v$ es un autovector, cualquier autovalor asociado a $v$ es √∫nico, puesto que si $T(v)=\\lambda_{1}v=\\lambda_{2}v$, entonces $(\\lambda_{1}-\\lambda_{2})v=O_{V}$. Como $v\\neq O_{V}$, entonces $\\lambda_{1}-\\lambda_{2}=0$, lo que implica que $\\lambda_{1}=\\lambda_{2}$.\n",
    "\n",
    "**<font color='blue'>Definici√≥n 3.4 ‚Äì Autoespacio:</font>** Sea $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$ una matriz tal que √©sta coincide con la representaci√≥n matricial de una transformaci√≥n lineal $T:V\\longrightarrow V$, siendo $V$ un $\\mathbb{K}$-espacio vectorial. Para cada autovalor $\\lambda$ de $\\mathbf{A}$ definimos el **autoespacio o espacio propio** de $\\lambda$, denotado como $W_{\\lambda}$, como\n",
    "\n",
    "$$W_{\\lambda }=\\ker \\left( \\mathbf{A} -\\lambda \\mathbf{I}_{n} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.17)$</p>\n",
    "\n",
    "**<font color='blue'>Definici√≥n 3.5 ‚Äì Similitud entre matrices:</font>** Sean $\\mathbf{A} ,\\mathbf{B} \\in \\mathbb{K}^{n\\times n}$ dos matrices no singulares. Diremos que $\\mathbf{A}$ y $\\mathbf{B}$ son **similares** si existe otra matriz $\\mathbf{P}\\in \\mathbb{K}^{n\\times 1}$ tal que\n",
    "\n",
    "$$\\mathbf{A} =\\mathbf{P} \\mathbf{B} \\mathbf{P}^{-1}$$\n",
    "<p style=\"text-align: right;\">$(3.18)$</p>\n",
    "\n",
    "**<font color='crimson'>Teorema 3.3 ‚Äì Preservaci√≥n de autovalores en matrices similares:</font>** *Sean $\\mathbf{A} ,\\mathbf{B} \\in \\mathbb{K}^{n\\times n}$ dos matrices similares entre s√≠. Entonces ambas matrices tienen los mismos autovalores (y, por extensi√≥n, el mismo polinomio caracter√≠stico).*\n",
    "\n",
    "Vamos a demostrar el teorema (3.3) a fin de entender completamente este resultado. En efecto, sean $\\mathbf{A} ,\\mathbf{B} \\in \\mathbb{K}^{n\\times n}$, tal que ambas matrices son similares (es decir, $\\mathbf{A} =\\mathbf{P} \\mathbf{B} \\mathbf{P}^{-1}$). Construyendo la expresi√≥n $\\mathbf{A} -\\lambda \\mathbf{I}_{n}$, tenemos que\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{A} -\\lambda \\mathbf{I}_{n} &=&\\mathbf{P} \\mathbf{B} \\mathbf{P}^{-1} -\\lambda \\mathbf{P} \\mathbf{P}^{-1} =\\mathbf{P} \\left( \\mathbf{B} -\\lambda \\mathbf{I}_{n} \\right)  \\mathbf{P}^{-1} \\\\ &\\Longrightarrow &\\det \\left( \\mathbf{A} -\\lambda \\mathbf{I}_{n} \\right)  =\\det \\left( \\mathbf{P} \\left( \\mathbf{B} -\\lambda \\mathbf{I}_{n} \\right)  \\mathbf{P}^{-1} \\right)  \\\\ &\\Longrightarrow &\\det \\left( \\mathbf{P} \\right)  \\det \\left( \\mathbf{B} -\\lambda \\mathbf{I}_{n} \\right)  \\det \\left( \\mathbf{P}^{-1} \\right)  \\  \\left( \\mathrm{pero} \\  \\det \\left( \\mathbf{P}^{-1} \\right)  =\\frac{1}{\\det \\left( \\mathbf{P} \\right)  } \\right)  \\\\ &\\Longrightarrow &\\det \\left( \\mathbf{B} -\\lambda \\mathbf{I}_{n} \\right)  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.19)$</p>\n",
    "\n",
    "As√≠, efectivamente, $\\mathbf{A}$ y $\\mathbf{B}$ tienen el mismo polinomio caracter√≠stico y, por tanto, los mismos autovalores.\n",
    "\n",
    "**Ejemplo 3.6:** Sea $T:\\mathbb{R}^{3}\\longrightarrow \\mathbb{R}^{3}$ una transformaci√≥n lineal definida como $T(x,y,z)=(3x+2y+z,3y+2z,-z)$. Vamos a determinar los autovalores y autovectores asociados a $T$. \n",
    "\n",
    "En primer lugar, debemos construir la matriz $\\mathbf{A}$ asociada a $T$ considerando la base can√≥nica de vectores en $\\mathbb{R}^{3}$ (que, recordemos, es $\\mathbf{e}(3)=\\left\\{ \\mathbf{e}_{1} ,\\mathbf{e}_{2} ,\\mathbf{e}_{3} \\right\\}  =\\left\\{ \\left( 1,0,0\\right)  ,\\left( 0,1,0\\right)  ,\\left( 0,0,1\\right)  \\right\\}$). Esto resulta sencillo, ya que\n",
    "\n",
    "$$T\\left( x,y,z\\right)  =\\left( 3x+2y+z,3y+2z,-z\\right)  =x\\begin{pmatrix}3\\\\ 0\\\\ 0\\end{pmatrix} +y\\begin{pmatrix}2\\\\ 3\\\\ 0\\end{pmatrix} +z\\begin{pmatrix}1\\\\ 2\\\\ -1\\end{pmatrix} \\Longrightarrow \\mathbf{A}= \\begin{pmatrix}3&2&1\\\\ 0&3&2\\\\ 0&0&-1\\end{pmatrix}$$\n",
    "<p style=\"text-align: right;\">$(3.20)$</p>\n",
    "\n",
    "Ahora debemos resolver la ecuaci√≥n $P_{\\mathbf{A}}(\\lambda)=0$. En efecto,\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{A} =\\begin{pmatrix}3&2&1\\\\ 0&3&2\\\\ 0&0&-1\\end{pmatrix} &\\Longrightarrow &P_{\\mathbf{A} }\\left( \\lambda \\right)  =\\det \\left( \\begin{matrix}3-\\lambda &2&1\\\\ 0&3-\\lambda &2\\\\ 0&0&-1-\\lambda \\end{matrix} \\right)  =0\\\\ &\\Longrightarrow &P_{\\mathbf{A} }\\left( \\lambda \\right)  =\\left( \\lambda -3\\right)^{3}  \\left( \\lambda +1\\right)  =0\\Longleftrightarrow \\lambda_{1} =3\\wedge \\lambda_{2} =-1\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.21)$</p>\n",
    "\n",
    "Por lo tanto, los autovalores de la matriz $\\mathbf{A}$ (y, por extensi√≥n, de $T$) son $\\lambda_{1}=3$ y $\\lambda_{2}=-1$. Ahora determinamos los autoespacios respectivos,\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{u} \\in \\left( \\mathbb{R}^{3} \\right)_{\\lambda }  &\\Longleftrightarrow &\\mathbf{u} \\in \\mathbb{R}^{3} \\wedge T\\left( \\mathbf{u} \\right)  =\\lambda \\mathbf{u} \\\\ &\\Longleftrightarrow &\\mathbf{u} =\\left( x,y,z\\right)  \\wedge T\\left( x,y,z\\right)  =\\lambda \\left( x,y,z\\right)  \\\\ &\\Longleftrightarrow &\\mathbf{u} =\\left( x,y,z\\right)  \\wedge T\\left( x,y,z\\right)  =\\left( \\lambda x,\\lambda y,\\lambda z\\right)  \\\\ &\\Longleftrightarrow &\\mathbf{u} =\\left( x,y,z\\right)  \\wedge \\left( 3x+2y+z,3y+2z,-z\\right)  =\\left( \\lambda x,\\lambda y,\\lambda z\\right)  \\\\ &\\Longleftrightarrow &\\mathbf{u} =\\left( x,y,z\\right)  \\wedge \\begin{cases}\\begin{array}{rcl}3x+2y+z&=&\\lambda x\\\\ 3y+2z&=&\\lambda y\\\\ -z&=&\\lambda z\\end{array} &\\end{cases} \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.22)$</p>\n",
    "\n",
    "Debemos por tanto evaluar el sistema de ecuaciones determinado en (3.22) usando los autovalores determinados previamente. As√≠ tenemos, para $\\lambda_{1}=3$,\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{u} \\in \\left( \\mathbb{R}^{3} \\right)_{\\lambda =3} &\\Longleftrightarrow &\\mathbf{u} =\\left( x,y,z\\right)  \\wedge \\begin{cases}\\begin{array}{rcl}3x+2y+z&=&\\lambda x\\\\ 3y+2z&=&\\lambda y\\\\ -z&=&\\lambda z\\end{array} &\\end{cases} \\\\ &\\Longleftrightarrow &\\mathbf{u} =\\left( x,y,z\\right)  \\wedge z=0,y=0\\\\ &\\Longleftrightarrow &\\mathbf{u} =\\left( x,0,0\\right)  ;x\\in \\mathbb{R} \\\\ &\\Longleftrightarrow &\\mathbf{u} =x\\left( 1,0,0\\right)  \\\\ &\\Longleftrightarrow &\\left( \\mathbb{R}^{3} \\right)_{\\lambda =3}  =\\left< \\left\\{ \\left( 1,0,0\\right)  \\right\\}  \\right>  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.23)$</p>\n",
    "\n",
    "Por otro lado, para $\\lambda_{2}=-1$, tenemos,\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{u} \\in \\left( \\mathbb{R}^{3} \\right)_{\\lambda =-1}  &\\Longleftrightarrow &\\mathbf{u} =\\left( x,y,z\\right)  \\wedge \\begin{cases}\\begin{array}{rcl}3x+2y+z&=&-x\\\\ 3y+2z&=&-y\\\\ -z&=&-z\\end{array} &\\end{cases} \\\\ &\\Longleftrightarrow &\\mathbf{u} =\\left( x,y,z\\right)  \\wedge z=2y,x=0\\\\ &\\Longleftrightarrow &\\mathbf{u} =\\left( 0,y,-2y\\right)  ;y\\in \\mathbb{R} \\\\ &\\Longleftrightarrow &\\mathbf{u} =y\\left( 0,1,-2\\right)  \\\\ &\\Longleftrightarrow &\\left( \\mathbb{R}^{3} \\right)_{\\lambda =-1}  =\\left< \\left\\{ \\left( 0,1,-2\\right)  \\right\\}  \\right>  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.24)$</p>\n",
    "‚óºÔ∏é\n",
    "\n",
    "Sea $T:\\mathbb{K}^{n}\\longrightarrow \\mathbb{K}^{n}$ una transformaci√≥n lineal y sea $\\mathbf{A}=[T]_{\\alpha}^{\\alpha}$ la matriz asociada a $T$ en la base can√≥nica $\\alpha$. Supongamos que existe una base de autovectores de $\\mathbf{A}$, definida como $\\beta =\\left\\{ \\mathbf{v}_{1},...,\\mathbf{v}_{n}\\right\\}$, siendo $\\beta$ por tanto una base de $\\mathbb{K}^{n}$. Para todo $i\\in \\mathbb{N}$, definimos $\\lambda_{i}\\in \\mathbb{K}$ tal que $\\mathbf{A}\\lambda \\mathbf{v}_{i}=\\lambda_{i} \\mathbf{v}_{i}$. Vemos que la matriz asociada a $T$ en la base $\\beta$, que denominamos como $\\mathbf{D}=[T]_{\\beta}^{\\beta}$, es diagonal, ya que\n",
    "\n",
    "$$\\begin{array}{rcl}\\mathbf{A} \\mathbf{v}_{1} =\\lambda_{1} \\mathbf{v}_{1} &\\Longrightarrow &\\lambda_{1} \\mathbf{v}_{1} =\\lambda_{1} \\mathbf{v}_{1} +0\\mathbf{v}_{2} +\\cdots +0\\mathbf{v}_{n} \\\\ \\mathbf{A} \\mathbf{v}_{2} =\\lambda_{2} \\mathbf{v}_{2} &\\Longrightarrow &\\lambda_{2} \\mathbf{v}_{2} =0\\mathbf{v}_{1} +\\lambda_{2} \\mathbf{v}_{2} +\\cdots +0\\mathbf{v}_{n} \\\\ &\\vdots &\\\\ \\mathbf{A} \\mathbf{v}_{n} =\\lambda_{n} \\mathbf{v}_{n} &\\Longrightarrow &\\lambda_{n} \\mathbf{v}_{n} =0\\mathbf{v}_{1} +0\\mathbf{v}_{2} +\\cdots +\\lambda_{n} \\mathbf{v}_{n} \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.25)$</p>\n",
    "\n",
    "Por lo tanto, podemos escribir\n",
    "\n",
    "$$\\left[ T\\right]^{\\beta }_{\\beta }  =\\left( \\left[ T\\left( \\mathbf{v}_{1} \\right)  \\right]_{\\beta }  ,\\left[ T\\left( \\mathbf{v}_{2} \\right)  \\right]_{\\beta }  ,...,\\left[ T\\left( \\mathbf{v}_{n} \\right)  \\right]_{\\beta }  \\right)  =\\left( \\begin{matrix}\\lambda_{1} &0&\\cdots &0\\\\ 0&\\lambda_{2} &\\cdots &0\\\\ \\vdots &\\vdots &\\ddots &\\vdots \\\\ 0&0&\\cdots &\\lambda_{n} \\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.26)$</p>\n",
    "\n",
    "As√≠ que, efectivamente, la matriz $\\mathbf{D}=[ T]^{\\beta }_{\\beta }$ y, por ende, la matriz $\\mathbf{A}$ puede expresarse mediante la **descomposici√≥n propia** (o auto-descomposici√≥n) definida como $\\mathbf{A}=\\mathbf{P}\\mathbf{D}\\mathbf{P}^{-1}$. Esto resulta conveniente, ya que algunas ventajas de conocer la matriz diagonal $\\mathbf{D}$ son las siguientes:\n",
    "\n",
    "- **(V1):** $\\rho(\\mathbf{A})=\\rho(\\mathbf{D})=$ n√∫mero de autovalores no nulos de $\\mathbf{A}$ (y, por extensi√≥n, de $\\mathbf{D}$). Recordemos que $\\rho(\\mathbf{A})$ denota el rango de la matriz $\\mathbf{A}$.\n",
    "- **(V2):** $\\det(\\mathbf{A})=\\det(\\mathbf{P}\\mathbf{D}\\mathbf{P}^{-1})=\\det(\\mathbf{P})\\det(\\mathbf{D})\\det(\\mathbf{P}^{-1})=\\det(\\mathbf{D})=\\prod^{n}_{k=1} \\lambda_{k}$.\n",
    "- **(V3):** Si $\\mathbf{A}$ es una matriz no singular (de decir, si $\\det(\\mathbf{A})\\neq 0$), entonces, para cada $\\lambda_{k}\\neq 0$ y $\\mathbf{A}^{-1}=\\mathbf{P}\\mathbf{D^{-1}}\\mathbf{P^{-1}}$, donde\n",
    "\n",
    "$$\\mathbf{D}^{-1} =\\left( \\begin{matrix}\\lambda^{-1}_{1} &0&\\cdots &0\\\\ 0&\\  \\lambda^{-1}_{2} &\\cdots &0\\\\ \\vdots &\\vdots &\\ddots &\\vdots \\\\ 0&0&\\cdots &\\lambda^{-1}_{n} \\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.27)$</p>\n",
    "\n",
    "- **(V4):** $\\mathbf{A}^{m} =\\left( \\mathbf{P} \\mathbf{D} \\mathbf{P}^{-1} \\right)^{m}  =\\left( \\mathbf{P} \\mathbf{D} \\mathbf{P}^{-1} \\right)  \\left( \\mathbf{P} \\mathbf{D} \\mathbf{P}^{-1} \\right)  \\overbrace{\\cdots }^{m\\  \\mathrm{veces} } \\left( \\mathbf{P} \\mathbf{D} \\mathbf{P}^{-1} \\right)$. As√≠ que,\n",
    "\n",
    "$$\\mathbf{A}^{m} =\\mathbf{P} \\left( \\begin{matrix}\\lambda^{-m}_{1} &0&\\cdots &0\\\\ 0&\\lambda^{-m}_{2} &\\cdots &0\\\\ \\vdots &\\vdots &\\ddots &\\vdots \\\\ 0&0&\\cdots &\\lambda^{-m}_{n} \\end{matrix} \\right)  \\mathbf{P}^{-1}$$\n",
    "<p style=\"text-align: right;\">$(3.28)$</p>\n",
    "\n",
    "Como hemos visto, $\\mathbf{A} =\\mathbf{P} \\mathbf{D} \\mathbf{P}^{-1}$, donde $\\mathbf{P}=[\\mathbf{e}]_{\\beta}^{\\alpha}$ (donde $\\mathbf{e}(n)$ es la base can√≥nica de vectores de $\\mathbb{K}^{n}$). Es decir, tenemos que expresar los correspondientes autovectores en t√©rminos de la base can√≥nica de $\\mathbb{K}^{n}$. De esta manera, $\\mathbf{P}=(\\mathbf{v}_{1},...,\\mathbf{v}_{n})$. Tiene sentido entonces la siguiente definici√≥n.\n",
    "\n",
    "**<font color='blue'>Definici√≥n 3.6 ‚Äì Matriz diagonalizable:</font>** Sea $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$ una matriz no singular. Diremos que $\\mathbf{A}$ es **diagonalizable** si $\\mathbb{K}^{n}$ admite una base de autovectores de $\\mathbf{A}$ (es decir, los autovectores de $\\mathbf{A}$ conforman un sistema de generadores para $\\mathbb{K}^{n}$ y son linealmente independientes).\n",
    "\n",
    "Con esta definici√≥n, ya podemos enunciar dos importantes teoremas relativos al proceso de diagonalizaci√≥n.\n",
    "\n",
    "**<font color='crimson'>Teorema 3.4:</font>** *Sea $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$ una matriz no singular. Entonces $\\mathbf{A}$ es diagonalizable si y s√≥lo si $\\mathbf{A}$ es similar a una matriz diagonal.* ‚óÜ\n",
    "\n",
    "**<font color='crimson'>Teorema 3.5:</font>** *Sea $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$ una matriz no singular y sea $\\left\\{ \\lambda_{i} \\right\\}^{k}_{i=1}$ un conjunto de autovalores de $\\mathbf{A}$ (todos distintos). Si $\\left\\{ v_{i}\\right\\}^{k}_{i=1}$ representa el conjunto de autovectores de $\\mathbf{A}$ para el cual se tiene que $\\mathbf{A} v_{i}=\\lambda_{i} v_{i}$, entonces $\\left\\{ v_{i}\\right\\}^{k}_{i=1}$ es un conjunto linealmente independiente.* ‚óÜ\n",
    "\n",
    "Antes de proesguir, veremos la extensi√≥n natural del concepto de suma directa para m√°s de dos subespacios vectoriales. Definamos primero los $k$ subespacios $U_{1},...,U_{k}$ de $V$. Definiremos la **suma** de tales subespacios como\n",
    "\n",
    "$$\\sum^{k}_{i=1} U_{i}=U_{1}+\\cdots +U_{k}\\triangleq \\left\\{ v=\\sum^{k}_{i=1} u_{i}\\  |\\  \\forall i\\in \\left\\{ 1,...,k\\right\\}  ,u_{i}\\in U_{i}\\right\\}$$\n",
    "<p style=\"text-align: right;\">$(3.29)$</p>\n",
    "\n",
    "La suma de subespacios as√≠ definida tambi√©n es, como cabr√≠a esperar, un subespacio de $V$. Estamos en condiciones, por tanto, de establecer la siguiente definici√≥n.\n",
    "\n",
    "**<font color='blue'>Definici√≥n 3.7 ‚Äì Suma directa m√∫ltiple:</font>** Sea $V$ un $\\mathbb{K}$-espacio vectorial y $\\left\\{ U_{i}\\right\\}^{k^{}}_{i=1}$ una colecci√≥n de $k$ subespacios vectoriales de $V$. Diremos que el subespacio $Z=\\sum^{k}_{i=1} U_{i}$ es la **suma directa** de $\\left\\{ U_{i}\\right\\}^{k^{}}_{i=1}$, lo que denotamos como $Z=\\bigoplus^{k}_{i=1} U_{i}=U_{1}\\oplus \\cdots \\oplus U_{k}$ si, para todo $v\\in Z$, $v$ se escribe de manera √∫nica como $v=\\sum^{k}_{i=1} u_{i}$, donde $u_{i}\\in U_{i}$, para $i=1,...,k$. Es decir,\n",
    "\n",
    "$$Z=\\bigoplus^{k}_{i=1} U_{i}=U_{1}\\oplus \\cdots \\oplus U_{k}\\Longleftrightarrow v=\\sum^{k}_{i=1} u_{i}\\  ;\\  u_{i}\\in U_{i},\\forall i\\in \\left\\{ 1,...,k\\right\\}$$\n",
    "<p style=\"text-align: right;\">$(3.30)$</p>\n",
    "\n",
    "La suma directa m√∫ltiple de subespacios cumple con las siguientes propiedades:\n",
    "\n",
    "- **(P1):** $Z=\\bigoplus^{k}_{i=1} U_{i}\\Longleftrightarrow \\left( Z=\\sum^{k}_{i=1} U_{i}\\wedge \\forall j\\in \\left\\{ 1,...,k\\right\\}  ,U_{j}\\cap \\left( \\sum^{k}_{\\begin{matrix}i=1\\\\ i\\neq j\\end{matrix} } U_{i}\\right)  =\\left\\{ O_{V}\\right\\}  \\right)$.\n",
    "- **(P2):** Si $Z=\\sum^{k}_{i=1} U_{i}$ y $Z$ es de dimensi√≥n finita (lo que suele denotarse como $\\dim(Z)<\\infty$), entonces las siguientes proposiciones son equivalentes:\n",
    "    - $Z=\\bigoplus^{k}_{i=1} U_{i}$.\n",
    "    - $\\left( \\forall i=\\left\\{ 1,...,k\\right\\}  \\right)  \\left( \\forall u_{i}\\in U_{i}-\\left\\{ O_{V}\\right\\}  \\right)  ,\\left\\{ u_{1},...,u_{k}\\right\\}$ es linealmente independiente.\n",
    "    - La yuxtaposici√≥n de las bases de las bases de los subespacios $\\left\\{ U_{i}\\right\\}^{k}_{i=1}$ es una base (y no s√≥lo un sistema de generadores) para $Z$.\n",
    "    - $\\dim(Z)=\\sum^{k}_{i=1} \\dim \\left( U_{i}\\right)$.\n",
    "\n",
    "Las propiedades que derivan de la definici√≥n (3.7) nos permiten formular el siguiente teorema.\n",
    "\n",
    "**<font color='crimson'>Teorema 3.6:</font>** *Sea $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$ una matriz no singular tal que $\\left\\{ \\lambda_{i} \\right\\}^{k}_{i=1}$ es el conjunto de autovalores de $\\mathbf{A}$ (todos distintos) y $W_{\\lambda_{i}}=\\ker(\\mathbf{A}-\\lambda \\mathbf{I}_{n})$ es el autoespacio asociado al autovalor $\\lambda_{i}$. Si $W=\\sum^{k}_{i=1} W_{\\lambda_{i} }$, entonces tenemos que*\n",
    "\n",
    "$$W=\\bigoplus^{k}_{i=1} W_{\\lambda_{i} }$$\n",
    "<p style=\"text-align: right;\">$(3.31)$</p>\n",
    "\n",
    "*En particular, $\\mathbf{A}$ es diagonalizable si y s√≥lo si*\n",
    "\n",
    "$$\\mathbb{K}^{n} =\\bigoplus^{k}_{i=1} W_{\\lambda_{i} }$$\n",
    "<p style=\"text-align: right;\">$(3.32)$</p>\n",
    "‚óÜ\n",
    "\n",
    "**<font color='DodgerBlue'>Corolario 3.1:</font>** *Sea $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$ una matriz no singular tal que $\\left\\{ \\lambda_{i} \\right\\}^{k}_{i=1}$ es el conjunto de autovalores de $\\mathbf{A}$ (todos distintos) y $W_{\\lambda_{i}}=\\ker(\\mathbf{A}-\\lambda \\mathbf{I}_{n})$ es el autoespacio asociado al autovalor $\\lambda_{i}$. Si $W=\\sum^{k}_{i=1} W_{\\lambda_{i} }$, entonces tenemos que*\n",
    "\n",
    "- **(T1):** $W_{\\lambda_{i} }=\\ker \\left( \\mathbf{A} -\\lambda \\mathbf{I}_{n} \\right)$ *es de dimensi√≥n 1.*\n",
    "- **(T2):** *Sea $v_{i}\\in W_{\\lambda_{i}}$ con $v_{i}\\neq O_{V}$. Entonces $\\left\\{ v_{1},...,v_{n}\\right\\}$ es una base de autovectores.*\n",
    "‚óÜ\n",
    "\n",
    "Este resultado nos entrega una condici√≥n suficiente (pero no necesaria) para establecer que una matriz $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$ sea diagonalizable: En s√≠ntesis, se tiene que $\\mathbf{A}$ es diagonalizable si $\\mathbf{A}$ tiene $n$ autovalores distintos. Sin embargo, esto no excluyente.\n",
    "\n",
    "**<font color='blue'>Definici√≥n 3.8 ‚Äì Multiplicidad geom√©trica:</font>** Sea $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$ una matriz no singular y $\\lambda$ un autovalor de $\\mathbf{A}$. Definimos la **multiplicidad geom√©trica** de $\\lambda$, denotada como $\\gamma_{\\mathbf{A}}(\\lambda)$, como la dimensi√≥n del autoespacio $W_{\\lambda }=\\ker \\left( \\mathbf{A} -\\lambda \\mathbf{I}_{n} \\right)$. Es decir, $\\gamma_{\\mathbf{A} } \\left( \\lambda \\right)  =\\dim \\left( W_{\\lambda }\\right)$.\n",
    "\n",
    "**<font color='blue'>Definici√≥n 3.9 ‚Äì Multiplicidad algebraica:</font>** Sea $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$ una matriz no singular y $\\lambda$ un autovalor de $\\mathbf{A}$. Definimos la **multiplicidad algebraica** del autovalor $\\lambda$, denotada como $\\alpha_{\\mathbf{A}}(\\lambda)$, como la m√°xima potencia de $(x-\\lambda)$ que es divisor del polinomio caracter√≠stico de $\\mathbf{A}$.\n",
    "\n",
    "Con ambas definiciones de multiplicidad ya establecidas, y con los resultados obtenidos previamente, estamos en condiciones de establecer el siguiente teorema, que establece las condiciones necesarias y suficientes para garantizar que una matriz es diagonalizable.\n",
    "\n",
    "**<font color='crimson'>Teorema 3.7 ‚Äì Criterio de diagonalizaci√≥n de matrices:</font>** *Sea $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$ una matriz no singular y $P_{\\mathbf{A}}(\\lambda)$ su polinomio caracter√≠stico. Se tiene entonces que $\\mathbf{A}$ es diagonalizable si y s√≥lo si $P_{\\mathbf{A}}(\\lambda)$ puede descomponerse en $\\mathbb{K}$ en una serie de factores lineales. Es decir,*\n",
    "\n",
    "$$P_{\\mathbf{A} }\\left( \\lambda \\right)  =c_{\\mathbf{A} }\\left( \\lambda -\\lambda_{1} \\right)^{\\alpha_{\\mathbf{A} } \\left( \\lambda_{1} \\right)  }  \\left( \\lambda -\\lambda_{2} \\right)^{\\alpha_{\\mathbf{A} } \\left( \\lambda_{2} \\right)  }  \\cdots \\left( \\lambda -\\lambda_{k} \\right)^{\\alpha_{\\mathbf{A} } \\left( \\lambda_{k} \\right)  }  =\\prod^{k}_{i=1} c_{\\mathbf{A} }\\left( \\lambda -\\lambda_{i} \\right)^{\\alpha_{\\mathbf{A} } \\left( \\lambda_{i} \\right)  }$$\n",
    "<p style=\"text-align: right;\">$(3.33)$</p>\n",
    "\n",
    "*Adem√°s, para cada autovalor $\\lambda$ de $\\mathbf{A}$, se debe tener que $\\gamma_{\\mathbf{A}}(\\lambda)=\\alpha_{\\mathbf{A}}(\\lambda)$.* ‚óÜ\n",
    "\n",
    "**Ejemplo 3.7:** Determinaremos los valores de $a$ y $b$ para los cuales la matriz\n",
    "\n",
    "$$\\mathbf{A} =\\left( \\begin{matrix}2a-b&0&2a-2b\\\\ 1&a&2\\\\ -a+b&0&-a+2b\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.34)$</p>\n",
    "\n",
    "es diagonalizable.\n",
    "\n",
    "En efecto, partimos calculando el polinomio caracter√≠stico de $\\mathbf{A}$,\n",
    "\n",
    "$$\\begin{array}{rcl}P_{\\mathbf{A} }\\left( \\lambda \\right)  &=&\\det \\left( \\mathbf{A} -\\lambda \\mathbf{I}_{3} \\right)  \\\\ &=&\\det \\left( \\begin{matrix}2a-b-\\lambda &0&2a-2b\\\\ 1&a-\\lambda &2\\\\ -a+b&0&-a+2b-\\lambda \\end{matrix} \\right)  \\\\ &=&\\left( a-\\lambda \\right)  \\det \\left( \\begin{matrix}2a-b-\\lambda &2a-2b\\\\ -a+b&-a+2b-\\lambda \\end{matrix} \\right)  \\\\ &\\underbrace{=}_{F_{21}\\left( 1\\right)  } &\\left( a-\\lambda \\right)  \\det \\left( \\begin{matrix}2a-b-\\lambda &2a-2b\\\\ a-\\lambda &a-\\lambda \\end{matrix} \\right)  \\\\ &=&\\left( a-\\lambda \\right)^{2}  \\det \\left( \\begin{matrix}2a-b-\\lambda &2a-2b\\\\ 1&1\\end{matrix} \\right)  \\\\ &=&\\left( a-\\lambda \\right)^{2}  \\left( b-\\lambda \\right)  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.36)$</p>\n",
    "\n",
    "As√≠ pues las ra√≠ces del polinomio caracter√≠stico $P_{\\mathbf{A} }\\left( \\lambda \\right)$ son $\\lambda=a$ y $\\lambda=b$. Por lo tanto, separamos la soluci√≥n de este problema en dos casos posibles, en los cuales se puede tener que $a\\neq b$ o $a=b$. Entonces, si $a\\neq b$, los autovalores asociados a $\\mathbf{A}$ son $\\lambda_{1}=a$ y $\\lambda_{2}=b$. La multiplicidad algebraica de $\\lambda_{1}=a$ es 2. Para determinar su multiplicidad geom√©trica, debemos determinar la dimensi√≥n del autoespacio asociado a este autovalor. As√≠ tenemos que,\n",
    "\n",
    "$$W_{a}=\\ker \\left( \\mathbf{A} -a\\mathbf{I}_{3} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.37)$</p>\n",
    "\n",
    "Luego,\n",
    "\n",
    "$$\\begin{array}{lll}\\left( \\mathbf{A} -a\\mathbf{I}_{3} \\right)  \\mathbf{x} =\\left( \\begin{matrix}0\\\\ 0\\\\ 0\\end{matrix} \\right)  &\\Longleftrightarrow &\\mathbf{x} =\\left( \\begin{matrix}x_{1}\\\\ x_{2}\\\\ x_{3}\\end{matrix} \\right)  \\in \\mathbb{R}^{3\\times 1} \\wedge a\\neq b;\\left( \\begin{matrix}a-b&0&2a-2b\\\\ 1&0&2\\\\ -a+b&0&-2a+2b\\end{matrix} \\right)  \\left( \\begin{matrix}x_{1}\\\\ x_{2}\\\\ x_{3}\\end{matrix} \\right)  =\\left( \\begin{matrix}0\\\\ 0\\\\ 0\\end{matrix} \\right)  \\\\ &\\Longleftrightarrow &\\mathbf{x} =\\left( \\begin{matrix}x_{1}\\\\ x_{2}\\\\ x_{3}\\end{matrix} \\right)  \\in \\mathbb{R}^{3\\times 1} \\wedge a\\neq b;\\left( \\begin{matrix}1&0&2\\\\ a-b&0&2a-2b\\\\ -a+b&0&-2a+2b\\end{matrix} \\right)  \\left( \\begin{matrix}x_{1}\\\\ x_{2}\\\\ x_{3}\\end{matrix} \\right)  =\\left( \\begin{matrix}0\\\\ 0\\\\ 0\\end{matrix} \\right)  \\\\ &\\Longleftrightarrow &\\mathbf{x} =\\left( \\begin{matrix}x_{1}\\\\ x_{2}\\\\ x_{3}\\end{matrix} \\right)  \\in \\mathbb{R}^{3\\times 1} \\wedge a\\neq b;\\left( \\begin{matrix}1&0&2\\\\ 0&0&0\\\\ 0&0&0\\end{matrix} \\right)  \\left( \\begin{matrix}x_{1}\\\\ x_{2}\\\\ x_{3}\\end{matrix} \\right)  =\\left( \\begin{matrix}0\\\\ 0\\\\ 0\\end{matrix} \\right)  \\\\ &\\Longrightarrow &\\rho \\left( \\mathbf{A} -a\\mathbf{I}_{3} \\right)  =1\\\\ &\\Longrightarrow &\\dim \\left( W_{a}\\right)  =2\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.38)$</p>\n",
    "\n",
    "Por lo tanto, la multiplicidad geom√©trica de $\\lambda_{1}=a$ es 2. Para $\\lambda_{2}=b$, la multiplicidad geom√©trica es 1, ya que su multiplicidad algebraica es tambi√©n igual a 1. En resumen, si $a\\neq b$, se tiene que\n",
    "\n",
    "$$\\begin{array}{l}\\lambda_{1} =a\\  ;\\  \\gamma_{\\mathbf{A} } \\left( \\lambda_{1} \\right)  =2\\  ;\\  \\alpha_{\\mathbf{A} } \\left( \\lambda_{1} \\right)  =2\\\\ \\lambda_{2} =b\\  ;\\  \\gamma_{\\mathbf{A} } \\left( \\lambda_{2} \\right)  =1\\  ;\\  \\alpha_{\\mathbf{A} } \\left( \\lambda_{2} \\right)  =1\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.39)$</p>\n",
    "\n",
    "As√≠ que, por el teorema (3.7), la matriz $\\mathbf{A}$ es diagonalizable cuando $a\\neq b$.\n",
    "\n",
    "Cuando $a=b$, tenemos un √∫nico autovalor ùúÜ=ùëé con multiplicidad algebraica $\\alpha_{\\mathbf{A}}(\\lambda)=3$. Calculamos por tanto su multiplicidad geom√©trica como sigue\n",
    "\n",
    "$$\\begin{array}{lll}\\left( \\mathbf{A} -a\\mathbf{I}_{3} \\right)  \\mathbf{x} =\\left( \\begin{matrix}0\\\\ 0\\\\ 0\\end{matrix} \\right)  &\\Longleftrightarrow &\\mathbf{x} =\\left( \\begin{matrix}x_{1}\\\\ x_{2}\\\\ x_{3}\\end{matrix} \\right)  \\in \\mathbb{R}^{3} \\wedge \\left( \\begin{matrix}0&0&0\\\\ 1&0&2\\\\ 0&0&0\\end{matrix} \\right)  \\left( \\begin{matrix}x_{1}\\\\ x_{2}\\\\ x_{3}\\end{matrix} \\right)  =\\left( \\begin{matrix}0\\\\ 0\\\\ 0\\end{matrix} \\right)  \\\\ &\\Longrightarrow &\\rho \\left( \\mathbf{A} -a\\mathbf{I}_{3} \\right)  =1\\\\ &\\Longrightarrow &\\dim \\left( W_{a}\\right)  =1\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.40)$</p>\n",
    "\n",
    "Por lo tanto, la multiplicidad geom√©trica de $\\lambda=a$ es igual a 1. De esta manera, conforme el teorema (3.7), dado que las multiplicidades algebraica y geom√©trica de $\\lambda=a$ son distintas, deducimos que $\\mathbf{A}$ no es diagonalizable cuando $a=b$. ‚óºÔ∏é\n",
    "\n",
    "**Ejemplo 3.8:** Consideremos la familia de isomorfismos $f_{a,b}:\\mathbb{R}^{3}\\longrightarrow \\mathbb{R}^{3}$, definida como $f(x,y,z)=(z,by,ax)$, donde $a,b\\in \\mathbb{R}$. Vamos a determinar los valores de $a$ y $b$ tales que $f_{a,b}$ es diagonalizable y, en caso de serlo, localizar su forma diagonal.\n",
    "\n",
    "En efecto, por el teorema (3.7), sabemos que, para que $f_{a,b}$ sea diagonalizable, su polinomio caracter√≠stico debe tener ra√≠ces no nulas y sus multiplicidades algebraicas y geom√©tricas deben ser iguales. En efecto, primero necesitamos encontrar una matriz asociada a $f_{a,b}$, la cual perfectamente puede ser construida tomando la base can√≥nica de $\\mathbb{R}^{3}$. En ese caso, tenemos que\n",
    "\n",
    "$$f_{a,b}\\left( x,y,z\\right)  =\\left( z,by,ax\\right)  =x\\left( \\begin{matrix}0\\\\ 0\\\\ a\\end{matrix} \\right)  +y\\left( \\begin{matrix}0\\\\ b\\\\ 0\\end{matrix} \\right)  +z\\left( \\begin{matrix}1\\\\ 0\\\\ 0\\end{matrix} \\right)  \\Longrightarrow \\mathbf{A} =\\left( \\begin{matrix}0&0&a\\\\ 0&b&0\\\\ 1&0&0\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.41)$</p>\n",
    "\n",
    "Con esta informaci√≥n, construimos el polinomio caracter√≠stico de $\\mathbf{A}$ como sigue,\n",
    "\n",
    "$$P_{\\mathbf{A} }\\left( \\lambda \\right)  =\\det \\left( \\mathbf{A} -\\lambda \\mathbf{I}_{3} \\right)  =\\det \\left( \\begin{matrix}-\\lambda &0&a\\\\ 0&b-\\lambda &0\\\\ 1&0&-\\lambda \\end{matrix} \\right)  =\\left( \\lambda^{2} -a\\right)  \\left( \\lambda -b\\right)$$\n",
    "<p style=\"text-align: right;\">$(3.42)$</p>\n",
    "\n",
    "Por lo tanto, tenemos que $\\lambda =\\pm a\\vee \\lambda =b$. Distinguimos de este modo cinco casos de inter√©s:\n",
    "\n",
    "<font color='DarkTurquoise'>Caso 1:</font> Si $a>0\\wedge b\\neq \\pm \\sqrt{a}$, entonces $\\mathbf{A}$ es diagonalizable, porque tiene tres autovalores distintos. Su forma diagonal es, por tanto,\n",
    "\n",
    "$$\\mathbf{D} =\\begin{pmatrix}\\sqrt{a} &0&0\\\\ 0&-\\sqrt{a} &0\\\\ 0&0&b\\end{pmatrix}$$\n",
    "<p style=\"text-align: right;\">$(3.43)$</p>\n",
    "\n",
    "<font color='DarkTurquoise'>Caso 2:</font> Si $a=b=0$ entonces $\\lambda =0$ es el √∫nico autovalor de $\\mathbf{A}$ con multiplicidad algebraica igual a 3. En este caso, el autoespacio de $\\lambda$ es $W_{\\lambda =0}=\\left\\{ \\left( x,y,0\\right)  ;x,y\\in \\mathbb{R} \\right\\}$. Por lo tanto, $\\dim(W_{\\lambda =0})=2$, siendo entonces la multiplicidad geom√©trica de $\\lambda$ igual a 2. De esta manera, conforme el teorema (3.7), $\\mathbf{A}$ no es diagonalizable.\n",
    "\n",
    "<font color='DarkTurquoise'>Caso 3:</font> Si $a=0$ y $b\\neq 0$, entonces $\\mathbf{A}$ tiene dos autovalores distintos: $\\lambda_{1}=0$, con multiplicidad algebraica igual a 2, y $\\lambda_{2}=b$, con multiplicidad algebraica igual a 1. Pero $W_{\\lambda_{1} =0}=\\left\\{ \\left( x,0,0\\right)  ;x\\in \\mathbb{R} \\right\\}  \\Longrightarrow \\dim \\left( W_{\\lambda_{1} =0}\\right)  =1$, con lo cual $\\alpha_{\\mathbf{A} } \\left( \\lambda_{1} \\right)  \\neq \\gamma_{\\mathbf{A} } \\left( \\lambda_{1} \\right)$. De esta manera, por el teorema (3.7), deducimos que $\\mathbf{A}$ no es diagonalizable.\n",
    "\n",
    "<font color='DarkTurquoise'>Caso 4:</font> Si $a>0\\wedge b=\\sqrt{a}$, entonces $\\mathbf{A}$ tiene dos autovalores distintos, $\\lambda_{1}=\\sqrt{a}$ (con multiplicidad algebraica $\\gamma_{\\mathbf{A} } \\left( \\lambda_{1} \\right)  =2$) y $\\lambda_{2}=-\\sqrt{a}$ (con multiplicidad algebraica $\\gamma_{\\mathbf{A} } \\left( \\lambda_{2} \\right)  =1$). Luego tenemos que $W_{\\lambda_{1} =\\sqrt{a} }=\\left\\{ \\left( x,y,\\sqrt{a} x\\right)  |\\  x,y\\in \\mathbb{R} \\right\\}$, lo que implica que $W_{\\lambda_{1} =\\sqrt{a} }=2=\\gamma_{\\mathbf{A} } \\left( \\lambda_{1} \\right)$. De esta manera, por el teorema (3.7), deducimos que $\\mathbf{A}$ es diagonalizable. Su forma diagonal es\n",
    "\n",
    "$$\\mathbf{D} =\\left( \\begin{matrix}\\sqrt{a} &0&0\\\\ 0&\\sqrt{a} &0\\\\ 0&0&-\\sqrt{a} \\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.44)$</p>\n",
    "\n",
    "<font color='DarkTurquoise'>Caso 5:</font> Este caso es tot√°lmente an√°logo al anterior. La matriz $\\mathbf{A}$ es diagonalizable, y su forma diagonal es\n",
    "\n",
    "$$\\mathbf{D} =\\left( \\begin{matrix}\\sqrt{a} &0&0\\\\ 0&-\\sqrt{a} &0\\\\ 0&0&-\\sqrt{a} \\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.45)$</p>\n",
    "‚óºÔ∏é\n",
    "\n",
    "**Ejemplo 3.9:** En <font color='purple'>Numpy</font> es sencillo determinar los autovalores y autovectores asociados a una matriz. Si consideramos el problema abordado en el ejemplo (3.6) y la matriz $\\mathbf{A}$ determinada en la ecuaci√≥n (3.20), podemos resolver r√°pidamente la diagonalizaci√≥n por medio de la funci√≥n `eig()`, dependiente del m√≥dulo de √°lgebra lineal de <font color='purple'>Numpy</font> (`numpy.linalg`). Esta funci√≥n retorna una tupla con dos arreglos: El primero contiene los autovalores de la matriz de inter√©s, y el segundo los autovectores asociados a cada autovalor (columna a columna):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "744a057b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d357f8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos la matriz A.\n",
    "A = np.array([\n",
    "    [3, 2, 1],\n",
    "    [0, 3, 2],\n",
    "    [0, 0, -1],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cef9b8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos los autovalores y autovectores de la matriz A.\n",
    "lambda_, v = linalg.eig(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7fdbae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.,  3., -1.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mostramos los autovalores en pantalla.\n",
    "lambda_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a50a88de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.    , -1.    ,  0.    ],\n",
       "       [ 0.    ,  0.    , -0.4472],\n",
       "       [ 0.    ,  0.    ,  0.8944]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mostramos los autovectores en pantalla.\n",
    "v.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a64ae1d",
   "metadata": {},
   "source": [
    "Vemos pues que este c√°lculo fue muy sencillo y bastante r√°pido. La raz√≥n de esto es que <font color='purple'>Numpy</font>, como bien sabemos, es una librer√≠a que est√° optimizada para el tratamiento de arreglos gracias a sus operaciones vectorizadas. Adem√°s, el c√°lculo de los autovalores y autovectores de una matriz en <font color='purple'>Numpy</font> no requiere del c√°lculo de determinantes (por el efecto del polinomio caracter√≠stico) de manera directa, ya que cuenta con rutinas de **aproximaci√≥n** extremadamente eficientes que se ejecutan *tras bambalinas*. Por ejemplo, si verificamos los tiempos de ejecuci√≥n del c√°lculo de autovalores y autovectores por medio de la funci√≥n `eig()` sobre matrices cuyos tama√±os se duplican cada vez, veremos que los tiempos de ejecuci√≥n no escalan de manera significativa con respecto a estos tama√±os:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd517fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos una semilla aleatoria fija.\n",
    "rng = np.random.default_rng(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3015b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construimos tres matrices, cada una el doble de grande que la anterior, conformadas por\n",
    "# n√∫meros aleatorios enteros, uniformamente distribuidos entre -9 y 9.\n",
    "A = rng.integers(low=-10, high=10, size=(3, 3))\n",
    "B = rng.integers(low=-10, high=10, size=(6, 6))\n",
    "C = rng.integers(low=-10, high=10, size=(12, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86d8af66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.4 ¬µs ¬± 2.36 ¬µs per loop (mean ¬± std. dev. of 7 runs, 10,000 loops each)\n",
      "36.3 ¬µs ¬± 2.13 ¬µs per loop (mean ¬± std. dev. of 7 runs, 10,000 loops each)\n",
      "52.8 ¬µs ¬± 433 ns per loop (mean ¬± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit linalg.eig(A)\n",
    "%timeit linalg.eig(B)\n",
    "%timeit linalg.eig(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fa47cf",
   "metadata": {},
   "source": [
    "‚óºÔ∏é \n",
    "\n",
    "### Espectro de una matriz sim√©trica.\n",
    "Sea $\\mathbf{A}\\in \\mathbb{R}^{n\\times n}$ una matriz sim√©trica (es decir, $\\mathbf{A}=\\mathbf{A}^{\\top}$). Entonces podemos escribir\n",
    "\n",
    "$$\\forall \\mathbf{u} ,\\mathbf{v} \\in \\mathbb{R}^{n} :\\left< \\mathbf{A} \\mathbf{u} ,\\mathbf{v} \\right>  =\\left< \\mathbf{u} ,\\mathbf{A} \\mathbf{v} \\right>$$\n",
    "<p style=\"text-align: right;\">$(3.46)$</p>\n",
    "\n",
    "Consideremos adem√°s el producto interno can√≥nico de matrices, definido para las matrices $\\mathbf{A}\\wedge \\mathbf{B}\\in \\mathbb{R}^{n\\times n}$ como\n",
    "\n",
    "$$\\left< \\mathbf{A} ,\\mathbf{B} \\right>  =\\mathrm{tr} \\left( \\mathbf{B}^{\\top } \\mathbf{A} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.47)$</p>\n",
    "\n",
    "En efecto, por las propiedades de este producto interno, tenemos que $\\left< \\mathbf{A} \\mathbf{u} ,\\mathbf{v} \\right>  =\\left( \\mathbf{A} \\mathbf{u} \\right)^{\\top }  \\mathbf{v} =\\mathbf{u}^{\\top } \\mathbf{A}^{\\top } \\mathbf{v} =\\mathbf{u}^{\\top } \\mathbf{A} \\mathbf{v} =\\left< \\mathbf{u} ,\\mathbf{A} \\mathbf{v} \\right>$. Probaremos ahora que, si $\\mathbf{A}\\in \\mathbb{R}^{n\\times n}$ y satisface la ecuaci√≥n (3.46), entonces $\\mathbf{A}$ es sim√©trica. Podemos deducir aquello a partir del siguiente hecho: Si $\\mathbf{e} \\left( n\\right)  =\\left\\{ \\mathbf{e}_{1} ,...,\\mathbf{e}_{n} \\right\\}$ es la base can√≥nica de $\\mathbb{R}^{n}$, entonces\n",
    "\n",
    "$$\\left< \\mathbf{A} \\mathbf{e}_{j} ,\\mathbf{e}_{i} \\right>  =a_{ji}$$\n",
    "<p style=\"text-align: right;\">$(3.48)$</p>\n",
    "\n",
    "Luego, usando la ecuaci√≥n (3.46), obtenemos\n",
    "\n",
    "$$a_{ji}\\left< \\mathbf{A} \\mathbf{e}_{j} ,\\mathbf{e}_{i} \\right>  =\\left< \\mathbf{e}_{j} ,\\mathbf{A} \\mathbf{e}_{i} \\right>  =\\left< \\mathbf{A} \\mathbf{e}_{i} ,\\mathbf{e}_{j} \\right>$$\n",
    "\n",
    "Con esto en mente, vamos a construir la siguiente definici√≥n.\n",
    "\n",
    "**<font color='blue'>Definici√≥n 3.10 ‚Äì Matriz transpuesta conjugada:</font>** Sea $\\mathbf{A}\\in \\mathbb{C}^{n\\times n}$ una matriz definida expl√≠citamente como $\\mathbf{A} =\\left\\{ a_{jk}\\right\\}  ,a_{jk}\\in \\mathbb{C}$. Definimos la **matriz transpuesta conjugada** de $\\mathbf{A}$, denotada como $\\mathbf{A}^{\\ast }$, a la matriz $\\mathbf{A}^{\\ast } =\\left\\{ \\overline{a}_{jk} \\right\\}$, donde $\\overline{a}_{jk}$ es el n√∫mero complejo conjugado de $a_{jk}$ (es decir, para $a_{jk}=u_{jk}+iv_{jk}$, donde $u_{jk},v_{jk}\\in \\mathbb{R}^{2}$, se tiene que $\\overline{a}_{jk} =u_{jk}-iv_{jk}$, donde $i=\\sqrt{-1}$ es la unidad imaginaria).\n",
    "\n",
    "Cuando $\\mathbf{A}=\\mathbf{A}^{\\ast}$, decimos que la matriz $\\mathbf{A}$ es **herm√≠tica**. De esta manera, se tiene que toda matriz sim√©trica cuyos elementos est√°n definidos sobre el cuerpo $\\mathbb{R}$ es, por tanto, herm√≠tica.\n",
    "\n",
    "De forma an√°loga al caso sim√©trico, se prueba que\n",
    "\n",
    "$$\\forall u,v\\in \\mathbb{C} :\\left< \\mathbf{A} u,v\\right>  =\\left< u,\\mathbf{A} v\\right>$$\n",
    "<p style=\"text-align: right;\">$(3.49)$</p>\n",
    "\n",
    "si y s√≥lo si $\\mathbb{A}$ es herm√≠tica. As√≠, en particular, $\\forall \\mathbf{u} ,\\mathbf{v} \\in \\mathbb{C}^{n} ,\\left< \\mathbf{A} \\mathbf{u} ,\\mathbf{v} \\right>  =\\left< \\mathbf{u} ,\\mathbf{A} \\mathbf{v} \\right>$, si $\\mathbb{A}\\in \\mathbb{R}^{n\\times n}$ y $\\mathbb{A}$ es sim√©trica.\n",
    "\n",
    "Si $\\mathbb{A}\\in \\mathbb{C}^{n\\times n}$, entonces el polinomio caracter√≠stico $P_{\\mathbb{A}}(\\lambda)=\\det(\\mathbb{A}-\\lambda \\mathbb{I}_{n})$ es un polinomio de grado $n$ con coeficientes complejos y, por lo tanto, conforme el teorema fundamental del √°lgebra, tiene $n$ ra√≠ces en $\\mathbb{C}$ (posiblemente repetidas). Denotaremos por\n",
    "\n",
    "$$\\sigma \\left( \\mathbf{A} \\right)  =\\left\\{ \\lambda \\in \\mathbb{C} :\\det \\left( \\mathbf{A} -\\lambda \\mathbf{I}_{n} \\right)  =0\\right\\}$$\n",
    "<p style=\"text-align: right;\">$(3.50)$</p>\n",
    "\n",
    "al conjunto de todas las ra√≠ces del polinimo caracter√≠stico $P_{\\mathbb{A}}(\\lambda)$. Llamaremos a este conjunto **espectro** de la matriz $\\mathbf{A}$. Como $\\mathbb{R}^{n\\times n}\\subset \\mathbb{C}^{n\\times n}$, podemos de igual forma definir el espectro de $\\mathbf{A}$ para $\\mathbf{A}\\in \\mathbb{R}^{n\\times n}$. Sin embargo, el espectro de una matriz $\\mathbf{A}$ es un subconjunto de $\\mathbb{C}$ aun cuando $\\mathbf{A}\\in \\mathbb{R}^{n\\times n}$.\n",
    "\n",
    "**Ejemplo 3.10:** Consideremos la matriz $\\mathbf{B}$ definida como\n",
    "\n",
    "$$\\mathbf{B} =\\left( \\begin{matrix}0&1&0\\\\ -1&0&0\\\\ 0&0&1\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.51)$</p>\n",
    "\n",
    "Construimos el polinomio caracter√≠stico de $\\mathbf{B}$ como sigue,\n",
    "\n",
    "$$P_{\\mathbf{B} }\\left( \\lambda \\right)  =\\det \\left( \\mathbf{B} -\\lambda \\mathbf{I}_{3} \\right)  =\\det \\left( \\begin{matrix}-\\lambda &1&0\\\\ -1&-\\lambda &0\\\\ 0&0&1-\\lambda \\end{matrix} \\right)  =\\left( 1-\\lambda \\right)  \\left( \\lambda^{2} +1\\right)$$\n",
    "<p style=\"text-align: right;\">$(3.52)$</p>\n",
    "\n",
    "Las ra√≠ces del polinomio caracter√≠stico $P_{\\mathbf{B} }\\left( \\lambda \\right)$ son $\\lambda =1$ y $\\lambda =\\pm i$. Si $\\mathbf{B}$ es considerada como una matriz de elementos reales (es decir, estamos trabajando en $\\mathbb{R}^{n\\times n}$, $\\mathbf{B}$ tiene s√≥lo un autovalor, que es $\\lambda=1$. Si el cuerpo considerado es $\\mathbb{C}$, entonces $\\mathbf{B}$ tiene tres autovalores, que se resumen en el conjunto $\\sigma \\left( \\mathbf{B} \\right)  =\\left\\{ 1,i,-i\\right\\}$. Este conjunto es, por tanto, el espectro de $\\mathbf{B}$.\n",
    "\n",
    "Estamos en condiciones, por lo tanto, de formular una serie de teoremas y corolarios que nos permitir√°n llegar a uno de los resultados m√°s importantes de esta secci√≥n.\n",
    "\n",
    "**<font color='crimson'>Teorema 3.8:</font>** *Sea $\\mathbf{A}\\in \\mathbb{C}^{n\\times n}$ una matriz herm√≠tica. Entonces el espectro de $\\mathbf{A}$ es tal que $\\sigma(\\mathbf{A})\\subseteq \\mathbb{R}$.* ‚óÜ\n",
    "\n",
    "**<font color='DodgerBlue'>Corolario 3.2:</font>** *Sea $\\mathbf{A}\\in \\mathbb{R}^{n\\times n}$ una matriz sim√©trica. Entonces $\\sigma(\\mathbf{A})\\subseteq \\mathbb{R}$.* ‚óÜ\n",
    "\n",
    "**<font color='crimson'>Teorema 3.9:</font>** *Sea $\\mathbf{A}\\in \\mathbb{C}^{n\\times n}$ una matriz herm√≠tica, con $\\lambda_{1}$ y $\\lambda_{2}$ autovalores de $\\mathbf{A}$. Si $\\lambda_{1}\\neq \\lambda_{2}$ y $v_{1},v_{2}$ son los autovectores asociados a $\\lambda_{1}$ y $\\lambda_{2}$, respectivamente. Entonces $\\left< v_{1},v_{2}\\right>  =0$ con el producto interno can√≥nico en $\\mathbb{C}$. Es decir, los autovectores $v_{1}$ y $v_{2}$ son ortogonales.* ‚óÜ\n",
    "\n",
    "**<font color='DodgerBlue'>Corolario 3.3:</font>** *Sea $\\mathbf{A}\\in \\mathbb{R}^{n\\times n}$ una matriz sim√©trica. Entonces los autovectores de $\\mathbf{A}$ asociados a autovalores distintos son ortogonales.* ‚óÜ\n",
    "\n",
    "**<font color='blue'>Definici√≥n 3.11 ‚Äì Transformaci√≥n sim√©trica:</font>** Sean $W$ un subespacio de $\\mathbb{R}$ y $T:W\\longrightarrow W$ una transformaci√≥n lineal. Diremos que $T$ es una **transformaci√≥n sim√©trica** en $W$ si se cumple que\n",
    "\n",
    "$$\\left< T\\left( \\mathbf{u} \\right)  ,\\mathbf{v} \\right>  =\\left< \\mathbf{u} ,T\\left( \\mathbf{v} \\right)  \\right>  ;\\forall \\mathbf{u} ,\\mathbf{v} \\in W$$\n",
    "<p style=\"text-align: right;\">$(3.53)$</p>\n",
    "\n",
    "Observemos que $T:\\mathbb{R}^{n}\\longrightarrow \\mathbb{R}^{n}$ es una transformaci√≥n sim√©trica si y s√≥lo si la matriz asociada a $T$ en base can√≥nica de $\\mathbb{R}^{n}$ es sim√©trica.\n",
    "\n",
    "El siguiente es uno de los resultados m√°s importantes de esta secci√≥n.\n",
    "\n",
    "**<font color='crimson'>Teorema 3.10 ‚Äì Descomposici√≥n espectral:</font>** *Sea $W$ un subespacio de $\\mathbb{R}^{n}$, tal que $\\dim(W)\\geq 1$ y $T:W\\longrightarrow W$ una transformaci√≥n lineal sim√©trica. Entonces existe una base ortonormal de $W$ compuesta por autovectores de $T$.* ‚óÜ\n",
    "\n",
    "**<font color='DodgerBlue'>Corolario 3.4:</font>** *Sea $\\mathbf{A}\\in \\mathbb{R}^{n\\times n}$ una matriz sim√©trica. Entonces $\\mathbf{A}$ es diagonalizable y, adem√°s, $\\mathbf{A}=\\mathbf{P} \\mathbf{D} \\mathbf{P}^{\\top}$, donde $\\mathbf{P}=(v_{1},...,v_{n})$ y $\\alpha =\\left\\{ v_{1},...,v_{n}\\right\\}$ es una base ortonormal de autovectores de $\\mathbf{A}$. Entonces $\\mathbf{D}$ es la matriz diagonal de los autovalores asociados a los autovectores que constituyen la base $\\alpha$.* ‚óÜ\n",
    "\n",
    "Del corolario (3.4) podemos establecer que toda matriz $\\mathbf{P}$ que satisface la expresi√≥n $\\mathbf{P}^{\\top}\\mathbf{P}=\\mathbf{I}_{n}$ se denomina **ortogonal** o **unitaria**. Entonces, si $\\mathbf{P}$ es una matriz ortogonal, entonces $\\mathbf{P}$ tambi√©n preserva la norma inducida por el producto interno can√≥nico de $\\mathbb{R}^{n\\times n}$. Es decir, $\\left\\Vert \\mathbf{P} \\mathbf{u} \\right\\Vert  =\\left< \\mathbf{P} \\mathbf{u} ,\\mathbf{P} \\mathbf{u} \\right>  =\\left< \\mathbf{u} ,\\mathbf{u} \\right>  =\\left\\Vert \\mathbf{u} \\right\\Vert^{2}$. Notemos adem√°s que no hay unicidad en la base de autovectores y, por lo tanto, una matriz diagonalizable $\\mathbf{A}$ puede descomponerse en la forma $\\mathbf{P} \\mathbf{D} \\mathbf{P}^{-1}$ de muchas maneras. Lo que hemos establecido para el caso de una matriz sim√©trica, es que se puede tomar una base ortonormal de autovectores y que, para esta base, se tiene la descomposici√≥n $\\mathbf{P} \\mathbf{D} \\mathbf{P}^{\\top}$.\n",
    "\n",
    "**Ejemplo 3.11:** Consideremos la matriz $\\mathbf{A}$, definida expl√≠citamente como\n",
    "\n",
    "$$\\mathbf{A} =\\left( \\begin{matrix}2&0&0\\\\ 0&2&0\\\\ 0&0&1\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.54)$</p>\n",
    "\n",
    "Observemos que $P_{\\mathbf{A} }\\left( \\lambda \\right)  =\\det \\left( \\mathbf{A} -\\lambda \\mathbf{I}_{3} \\right)  =\\left( \\begin{matrix}2-\\lambda &0&0\\\\ 0&2-\\lambda &0\\\\ 0&0&1-\\lambda \\end{matrix} \\right)  =\\left( 2-\\lambda \\right)^{2}  \\left( 1-\\lambda \\right)$. Se tienen pues los autovalores $\\lambda_{1}=2$ (con multiplicidad algebraica igual a 2) y $\\lambda_{2}=1$.\n",
    "\n",
    "Calculemos el autoespacio asociado a $\\lambda_{1}=2$,\n",
    "\n",
    "$$W_{\\lambda_{1} =2}=\\ker \\left( \\mathbf{A} -2\\mathbf{I}_{3} \\right)  \\Longrightarrow \\left( x,y,z\\right)  \\in \\mathbb{R}^{3} \\wedge \\left( \\begin{matrix}0&0&0\\\\ 0&0&0\\\\ 0&0&-1\\end{matrix} \\right)  \\left( \\begin{matrix}x\\\\ y\\\\ z\\end{matrix} \\right)  =\\left( \\begin{matrix}0\\\\ 0\\\\ 0\\end{matrix} \\right)  \\Longleftrightarrow W_{\\lambda_{1} =2}=\\left\\{ \\left( x,y,z\\right)  \\in \\mathbb{R}^{3} :z=0\\right\\}$$\n",
    "<p style=\"text-align: right;\">$(3.55)$</p>\n",
    "\n",
    "Determinamos ahora el autoespacio $\\lambda_{2}=1$,\n",
    "\n",
    "$$W_{\\lambda_{2} =1}=\\ker \\left( \\mathbf{A} -\\mathbf{I}_{3} \\right)  \\Longrightarrow \\left( x,y,z\\right)  \\in \\mathbb{R}^{3} \\wedge \\left( \\begin{matrix}1&0&0\\\\ 0&1&0\\\\ 0&0&0\\end{matrix} \\right)  \\left( \\begin{matrix}x\\\\ y\\\\ z\\end{matrix} \\right)  =\\left( \\begin{matrix}0\\\\ 0\\\\ 0\\end{matrix} \\right)  \\Longleftrightarrow W_{\\lambda_{2} =1}=\\left\\{ \\left( x,y,z\\right)  \\in \\mathbb{R}^{3} :x=y=0\\right\\}$$\n",
    "<p style=\"text-align: right;\">$(3.56)$</p>\n",
    "\n",
    "Por lo tanto, una base de autovectores para $\\mathbb{R}^{3}$ es\n",
    "\n",
    "$$\\alpha =\\left\\{ \\left( \\begin{matrix}1\\\\ 0\\\\ 0\\end{matrix} \\right)  ,\\left( \\begin{matrix}1\\\\ 1\\\\ 0\\end{matrix} \\right)  ,\\left( \\begin{matrix}0\\\\ 0\\\\ 1\\end{matrix} \\right)  \\right\\}$$\n",
    "<p style=\"text-align: right;\">$(3.57)$</p>\n",
    "\n",
    "Luego,\n",
    "\n",
    "$$\\mathbf{A} =\\mathbf{P} \\mathbf{D} \\mathbf{P}^{-1} \\wedge \\mathbf{P} =\\left( \\begin{matrix}1&1&0\\\\ 0&1&0\\\\ 0&0&1\\end{matrix} \\right)  ,\\mathbf{P}^{-1} =\\left( \\begin{matrix}1&-1&0\\\\ 0&1&0\\\\ 0&0&1\\end{matrix} \\right)  ,\\mathbf{D} =\\left( \\begin{matrix}2&0&0\\\\ 0&2&0\\\\ 0&0&1\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.58)$</p>\n",
    "\n",
    "Pero $\\mathbf{A}$ tambi√©n posee una base ortonormal de autovectores,\n",
    "\n",
    "$$\\beta =\\left\\{ \\left( \\begin{matrix}1\\\\ 0\\\\ 0\\end{matrix} \\right)  ,\\left( \\begin{matrix}0\\\\ 1\\\\ 0\\end{matrix} \\right)  ,\\left( \\begin{matrix}0\\\\ 0\\\\ 1\\end{matrix} \\right)  \\right\\}  \\wedge \\mathbf{A} =\\mathbf{P} \\mathbf{D} \\mathbf{P}^{-1} =\\left( \\begin{matrix}1&0&0\\\\ 0&1&0\\\\ 0&0&1\\end{matrix} \\right)  \\left( \\begin{matrix}2&0&0\\\\ 0&2&0\\\\ 0&0&1\\end{matrix} \\right)  \\left( \\begin{matrix}1&0&0\\\\ 0&1&0\\\\ 0&0&1\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.59)$</p>\n",
    "\n",
    "Cabe se√±alar que, en el caso de que si no hubiese sido sencillo obtener a simple vista una base ortonormal del autoespacio $W_{\\lambda_{1}=2}$, podemos utilizar el m√©todo de Gram-Schmidt para obtener la respectiva base ortonormal partiendo desde dicho subespacio. En general, la base ortonormal de autovectores de $\\mathbb{R}^{n}$ (que sabemos que existe, en el caso de que $\\mathbf{A}$ sea sim√©trica, en el caso del corolario (3.3)) puede obtenerse mediante la aplicaci√≥n del m√©todo de Gram-Schmidt en cada autoespacio. ‚óºÔ∏é"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c09ea3",
   "metadata": {},
   "source": [
    "## Formas.\n",
    "\n",
    "### Formas lineales.\n",
    "La diagonalizaci√≥n de matrices (y de operadores lineales) nos ha permitido formular (y encontrar una forma de resolver) un problema que es conocido en √°lgebra lineal como descomposici√≥n matricial. El problema de descomponer una matriz pasa, en general, por rescribir una matriz $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$ como una multiplicaci√≥n sucesiva de otras matrices que nos transmiten informaci√≥n valiosa respecto de la matriz original. Lo que vimos en la subsecci√≥n anterior es un tipo de descomposici√≥n basada en matrices diagonales ‚Äìllamada descomposici√≥n propia o autodescomposici√≥n‚Äì y que es del tipo $\\mathbf{A} =\\mathbf{P} \\mathbf{D} \\mathbf{P}^{-1}$. En adelante, veremos otras descomposiciones interesantes que nos permitir√°n construir la base de, como cabr√≠a esperar, varios tipos de desarrollos propios del campo del aprendizaje autom√°tico. En particular, estas descomposiciones nos permitir√°n expresar objetos geom√©tricos importantes de $\\mathbb{R}^{2}$ y $\\mathbb{R}^{3}$ mediante t√©cnicas de √°lgebra lineal.\n",
    "\n",
    "Consideremos entonces, una vez m√°s, el problema elemental del √°lgebra lineal: Dado un $\\mathbb{K}$-espacio vectorial normado $V$ y una base $\\alpha =\\left\\{ v_{1},...,v_{n}\\right\\}$ de $V$, sabemos que todo vector $v\\in V$ puede escribirse como una √∫nica combinaci√≥n lineal de los elementos de $\\alpha$. Es decir,\n",
    "\n",
    "$$v=\\sum^{n}_{k=1} a_{k}v_{k}\\  ;\\  a_{k}\\in \\mathbb{K} \\Longleftrightarrow \\left[ v\\right]_{\\alpha }  =\\left( \\begin{matrix}a_{1}\\\\ \\vdots \\\\ a_{n}\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.60)$</p>\n",
    "\n",
    "Queremos determinar las coordenadas del vector $v$ de forma tal que dicha especificaci√≥n sea independiente del producto interno que caracteriza a $V$. Para ello, observemos que cada elemento de la base $\\alpha$ puede escribirse como\n",
    "\n",
    "$$\\left[ v_{1}\\right]_{\\alpha }  =\\left( \\begin{matrix}1\\\\ 0\\\\ \\vdots \\\\ 0\\end{matrix} \\right)  ;\\left[ v_{2}\\right]_{\\alpha }  =\\left( \\begin{matrix}0\\\\ 1\\\\ \\vdots \\\\ 0\\end{matrix} \\right)  ;\\  \\cdots \\  ;\\left[ v_{n}\\right]_{\\alpha }  =\\left( \\begin{matrix}0\\\\ 0\\\\ \\vdots \\\\ 1\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.61)$</p>\n",
    "\n",
    "Por lo tanto, podemos imaginar la expresi√≥n anterior como unn especie de *lector de coordenadas* para los elementos de la base $\\alpha$, de tal forma que √©ste lee un 1 en la posici√≥n $k$, y un 0 en otra posici√≥n. Por otro lado, en el caso general, un vector $v\\in V$ se expresa, en la base $\\alpha$, como\n",
    "\n",
    "$$\\left[ v\\right]_{\\alpha }  =a_{1}\\left( \\begin{matrix}1\\\\ 0\\\\ \\vdots \\\\ 0\\end{matrix} \\right)  +a_{2}\\left( \\begin{matrix}0\\\\ 1\\\\ \\vdots \\\\ 0\\end{matrix} \\right)  +\\  \\cdots \\  +a_{n}\\left( \\begin{matrix}0\\\\ 0\\\\ \\vdots \\\\ 1\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.62)$</p>\n",
    "\n",
    "De lo anterior, observamos que cada $v\\in V$ necesita un total de $n$ lectores, pues dicho lector debe ser, como m√≠nimo, lineal y entregar como valor un escalar. Tiene sentido entonces la siguiente definici√≥n.\n",
    "\n",
    "**<font color='blue'>Definici√≥n 3.12 ‚Äì $\\alpha$-lector:</font>** Sea $V$ un $\\mathbb{K}$-espacio vectorial y $\\alpha =\\left\\{ v_{1},...,v_{n}\\right\\}$ una base de $V$. Llamaremos $\\alpha$-lector al conjunto $\\alpha^{\\ast } =\\left\\{ v^{\\ast }_{1},...,v^{\\ast }_{n}\\right\\}$, donde, para cada $k=1,...,n$, se tiene que\n",
    "\n",
    "$$\\begin{array}{lll}v^{\\ast }_{k}&:&V\\longmapsto \\mathbb{K} \\\\ &&v\\longmapsto a_{k}\\end{array} \\Longleftrightarrow \\left[ v\\right]_{\\alpha }  =\\left( \\begin{matrix}a_{1}\\\\ a_{2}\\\\ \\vdots \\\\ a_{n}\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.63)$</p>\n",
    "\n",
    "En particular, un $\\alpha$-lector tiene la siguiente propiedad\n",
    "\n",
    "$$v^{\\ast }_{i}\\left( v_{j}\\right)  =\\begin{cases}1&;\\  \\mathrm{si} \\  i=j\\\\ 0&;\\  \\mathrm{si} \\  i\\neq j\\end{cases}$$\n",
    "<p style=\"text-align: right;\">$(3.64)$</p>\n",
    "\n",
    "Observamos que, para cada $k=1,...,n$, $v_{k}^{\\ast}$ es una transformaci√≥n lineal del espacio vectorial $V$ en su cuerpo de escalares $\\mathbb{K}$. En s√≠mbolos, para cada $k=1,...,n$, $v_{k}^{\\ast}\\in \\mathbb{L}_{\\mathbb{K}}(V,\\mathbb{K})$. En efecto, si $v=\\sum^{n}_{k=1} a_{k}v_{k}\\wedge u=\\sum^{n}_{k=1} b_{k}v_{k}$, entonces se tiene que\n",
    "\n",
    "$$v^{\\ast }_{s}\\left( u+v\\right)  =v^{\\ast }_{s}\\left[ \\sum^{n}_{k=1} \\left( a_{i}+b_{i}\\right)  v_{i}\\right]  =a_{s}+b_{s}=v^{\\ast }_{s}\\left( v\\right)  +v^{\\ast }_{s}\\left( u\\right)$$\n",
    "<p style=\"text-align: right;\">$(3.65)$</p>\n",
    "\n",
    "An√°logamente, si $\\lambda \\in \\mathbb{K}$, entonces\n",
    "\n",
    "$$v^{\\ast }_{s}\\left( \\lambda v\\right)  =v^{\\ast }_{s}\\left[\\displaystyle  \\sum^{n}_{k=1} \\left( \\lambda a_{i}\\right)  v_{i}\\right]  =\\lambda a_{s}=\\lambda v^{\\ast }_{s}\\left( v\\right)$$\n",
    "<p style=\"text-align: right;\">$(3.66)$</p>\n",
    "\n",
    "**<font color='crimson'>Teorema 3.11:</font>** $\\alpha^{\\ast }$ *es una base del espacio vectorial $\\mathbb{L}_{\\mathbb{K}}(V,\\mathbb{K})$.* ‚óÜ\n",
    "\n",
    "En efecto, primero probaremos que $\\alpha^{\\ast}$ es un sistema de generadores para $\\mathbb{L}_{\\mathbb{K}}(V,\\mathbb{K})$. Sea $T\\in \\mathbb{L}_{\\mathbb{K}}(V,\\mathbb{K})$, entonces\n",
    "\n",
    "$$\\begin{array}{rcl}v=\\sum^{n}_{k=1} a_{k}v_{k}&\\Longrightarrow &T\\left( v\\right)  =\\displaystyle \\sum^{n}_{k=1} a_{k}T\\left( v_{k}\\right)  \\  ;\\  \\left( T\\  \\mathrm{es} \\  \\mathrm{lineal} \\right)  \\\\ a_{k}=v^{\\ast }_{k}\\left( v\\right)  &\\Longrightarrow &T\\left( v\\right)  =\\displaystyle \\sum^{n}_{k=1} v^{\\ast }_{k}\\left( v\\right)  T\\left( v_{k}\\right)  \\  ;\\  \\left( \\mathrm{definicion} \\  \\left( 3.12\\right)  \\right)  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.67)$</p>\n",
    "\n",
    "As√≠ que,\n",
    "\n",
    "$$T\\left( v\\right)  =\\sum^{n}_{k=1} T\\left( v_{k}\\right)  v^{\\ast }_{k}\\left( v\\right)  =\\left[ \\sum^{n}_{k=1} T\\left( v_{k}\\right)  v^{\\ast }_{k}\\right]  \\left( v\\right)$$\n",
    "<p style=\"text-align: right;\">$(3.68)$</p>\n",
    "\n",
    "Aplicando la definici√≥n de igualdad de funciones, tenemos que\n",
    "\n",
    "$$T=\\sum^{n}_{k=1} \\underbrace{T\\left( v_{k}\\right)  }_{\\in \\mathbb{K} } v^{\\ast }_{k}$$\n",
    "<p style=\"text-align: right;\">$(3.69)$</p>\n",
    "\n",
    "O equivalentemente,\n",
    "\n",
    "$$T\\in \\left< \\left\\{ v^{\\ast }_{1},v^{\\ast }_{2},...,v^{\\ast }_{n}\\right\\}  \\right>$$\n",
    "<p style=\"text-align: right;\">$(3.70)$</p>\n",
    "\n",
    "Ahora debemos probar que $\\alpha^{\\ast}$ es un conjunto linealmente independiente en $\\mathbb{L}_{\\mathbb{K}}(V,\\mathbb{K})$. En efecto, si suponemos que $\\sum^{n}_{k=1} r_{k}v^{\\ast }_{k}=0$, entonces debemos verificar que, para cada $v\\in V$, la funci√≥n $v_{k}^{\\ast}$ ser√° nula en $V$. O, en otras palabras, $\\ker \\left( \\sum^{n}_{k=1} r_{k}v^{\\ast }_{k}\\right)  =V$. Observemos que, si esta funci√≥n se anula en todo el espacio vectorial $V$, en particular, tambi√©n anula a los elementos $v_{1},...,v_{n}$ de la base $\\alpha$. As√≠ que, para cada $s=1,...,n$, por la definici√≥n (3.12), tendremos que\n",
    "\n",
    "$$0=\\sum^{n}_{k=1} r_{k}v^{\\ast }_{k}\\left( v_{s}\\right)  =r_{s}$$\n",
    "<p style=\"text-align: right;\">$(3.71)$</p>\n",
    "\n",
    "En conclusi√≥n, $\\alpha^{\\ast}$ es una base del espacio vectorial $\\mathbb{L}_{\\mathbb{K}}(V,\\mathbb{K})$.\n",
    "\n",
    "**<font color='blue'>Definici√≥n 3.13 ‚Äì Espacio dual y base dual:</font>** Sea $V$ un $\\mathbb{K}$-espacio vectorial y $\\alpha =\\left\\{ v_{1},...,v_{n}\\right\\}$ una base de $V$. Diremos que $V^{\\ast}=\\mathbb{L}_{\\mathbb{K}}(V,\\mathbb{K})$ se llamar√° **espacio dual** de $V$. La base $\\alpha^{\\ast } =\\left\\{ v^{\\ast }_{1},...,v^{\\ast }_{n}\\right\\}$, por consiguiente, se llamar√° **base dual** de la base $\\alpha$.\n",
    "\n",
    "En conclusi√≥n, si $V$ es un $\\mathbb{K}$-espacio vectorial, y $\\alpha =\\left\\{ v_{1},...,v_{n}\\right\\}$ es una base de $V$, entonces\n",
    "\n",
    "$$v=\\sum^{n}_{k=1} v^{\\ast }_{k}\\left( v\\right)  v_{k}\\Longleftrightarrow \\left[ v\\right]_{\\alpha }  =\\left( \\begin{matrix}v^{\\ast }_{1}\\left( v\\right)  \\\\ v^{\\ast }_{2}\\left( v\\right)  \\\\ \\vdots \\\\ v^{\\ast }_{n}\\left( v\\right)  \\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.72)$</p>\n",
    "\n",
    "**Ejemplo 3.12:** Dados tres n√∫meros reales distintos $r_{1},r_{2}$ y $r_{3}$, podemos definir tres funciones como sigue\n",
    "\n",
    "$$\\begin{array}{ll}T_{i}:&\\mathbb{R}_{2} \\left[ x\\right]  \\longmapsto \\mathbb{R} \\\\ &p\\left( x\\right)  \\longmapsto p\\left( r_{i}\\right)  \\end{array} \\  ;\\  i=1,2,3$$\n",
    "<p style=\"text-align: right;\">$(3.73)$</p>\n",
    "\n",
    "En primer lugar, vamos a demostrar $T_{i}$ es una transformaci√≥n lineal entre $\\mathbb{R}_{2}[x]$ y $\\mathbb{R}$ (es decir, $T_{i}\\in \\mathbb{L}_{\\mathbb{R}}(\\mathbb{R}_{2}[x],\\mathbb{R})$ para $i=1,2,3$). En efecto, sean $p(x),q(x)\\in \\mathbb{R}_{2}[x]$ y $\\lambda \\in \\mathbb{R}$. En primer lugar, debemos probar que $T_{i}\\left( p\\left( x\\right)  +q\\left( x\\right)  \\right)  =T_{i}\\left( p\\left( x\\right)  \\right)  +T_{i}\\left( q\\left( x\\right)  \\right)$. De este modo tenemos que\n",
    "\n",
    "$$T_{i}\\left( p\\left( x\\right)  +q\\left( x\\right)  \\right)  =p\\left( r_{i}\\right)  +q\\left( r_{i}\\right)  =T_{i}\\left( p\\left( x\\right)  \\right)  +T_{i}\\left( q\\left( x\\right)  \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.74)$</p>\n",
    "\n",
    "Ahora debemos mostrar que $T_{i}\\left( \\lambda p\\left( x\\right)  \\right)  =\\lambda T_{i}\\left( p\\left( x\\right)  \\right)$. En efecto,\n",
    "\n",
    "$$T_{i}\\left( \\lambda p\\left( x\\right)  \\right)  =\\lambda p\\left( r_{i}\\right)  =\\lambda T_{i}\\left( p\\left( x\\right)  \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.75)$</p>\n",
    "\n",
    "As√≠ que, efectivamente, $T_{i}\\in \\mathbb{L}_{\\mathbb{R}}(\\mathbb{R}_{2}[x],\\mathbb{R})=\\left( \\mathbb{R}_{2} \\left[ x\\right]  \\right)^{\\ast }$ para $i=1,2,3$. Ahora probaremos que $\\alpha^{\\ast } =\\left\\{ T_{1},T_{2},T_{3}\\right\\}$ es un conjunto linealmente independiente en $\\left( \\mathbb{R}_{2} \\left[ x\\right]  \\right)^{\\ast }$. De esta manera,\n",
    "\n",
    "$$a_{1}T_{1}+a_{2}T_{2}+a_{3}T_{3}=0\\Longleftrightarrow \\left( a_{1}T_{1}+a_{2}T_{2}+a_{3}T_{3}\\right)  \\left( p\\left( x\\right)  \\right)  =0\\  ;\\  \\forall p\\left( x\\right)  \\in \\mathbb{R}_{2} \\left[ x\\right]$$\n",
    "<p style=\"text-align: right;\">$(3.76)$</p>\n",
    "\n",
    "En particular, tenemos que, usando la base can√≥nica de $\\mathbb{R}_{2}[x]$,\n",
    "\n",
    "$$\\begin{array}{rcl}\\left( a_{1}T_{1}+a_{2}T_{2}+a_{3}T_{3}\\right)  \\left( 1\\right)  &=&0\\\\ \\left( a_{1}T_{1}+a_{2}T_{2}+a_{3}T_{3}\\right)  \\left( x\\right)  &=&0\\\\ \\left( a_{1}T_{1}+a_{2}T_{2}+a_{3}T_{3}\\right)  \\left( x^{2}\\right)  &=&0\\end{array} \\  \\Longrightarrow \\  \\begin{array}{rcl}a_{1}+a_{2}+a_{3}&=&0\\\\ a_{1}r_{1}+a_{2}r_{2}+a_{3}r_{3}&=&0\\\\ a_{1}r^{2}_{1}+a_{2}r^{2}_{2}+a_{3}r^{2}_{3}&=&0\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.77)$</p>\n",
    "\n",
    "Si escribimos este sistema en forma matricial, obtenemos\n",
    "\n",
    "$$\\underbrace{\\left( \\begin{matrix}1&1&1\\\\ r_{1}&r_{2}&r_{3}\\\\ r^{2}_{1}&r^{2}_{2}&r^{2}_{3}\\end{matrix} \\right)  }_{=\\mathbf{A} } \\left( \\begin{matrix}a_{1}\\\\ a_{2}\\\\ a_{3}\\end{matrix} \\right)  =\\left( \\begin{matrix}0\\\\ 0\\\\ 0\\end{matrix} \\right)  \\  \\Longrightarrow \\  \\det \\left( \\mathbf{A} \\right)  =\\left( r_{1}-r_{2}\\right)  \\left( r_{2}-r_{3}\\right)  \\left( r_{3}-r_{1}\\right)$$\n",
    "<p style=\"text-align: right;\">$(3.78)$</p>\n",
    "\n",
    "Sabemos que los n√∫meros $r_{1},r_{2},r_{3}$ son distintos. Por lo tanto, es claro que $\\det(\\mathbf{A})\\neq 0$ y la soluci√≥n del sistema (3.78) es trivial; es decir, $a_{1}=a_{2}=a_{3}=0$. As√≠ que, efectivamente, $\\alpha^{\\ast } =\\left\\{ T_{1},T_{2},T_{3}\\right\\}$ es un conjunto linealmente independiente en $\\left( \\mathbb{R}_{2} \\left[ x\\right]  \\right)^{\\ast }$.\n",
    "\n",
    "Probaremos ahora que $\\alpha^{\\ast } =\\left\\{ T_{1},T_{2},T_{3}\\right\\}$ es una base de $\\left( \\mathbb{R}_{2} \\left[ x\\right]  \\right)^{\\ast }$. En efecto, tenemos que $\\dim\\limits_{\\mathbb{R} } \\left( \\mathbb{R}_{2} \\left[ x\\right]  \\right)^{\\ast }  =\\dim\\limits_{\\mathbb{R} } \\left( \\mathbb{R}_{2} \\left[ x\\right]  \\right)  =3$, as√≠ que, efectivamente, $\\alpha^{\\ast }$ es una base de $\\left( \\mathbb{R}_{2} \\left[ x\\right]  \\right)^{\\ast }$.\n",
    "\n",
    "Finalmente, determinaremos la correspondiente base $\\alpha$ de $\\mathbb{R}_{2}[x]$. Supongamos entonces que $\\alpha =\\left\\{ p_{1}\\left( x\\right)  ,p_{2}\\left( x\\right)  ,p_{3}\\left( x\\right)  \\right\\}$ es una base, donde\n",
    "\n",
    "$$p_{1}\\left( x\\right)  =c_{01}+c_{11}x+c_{21}x^{2}\\  ;\\  p_{2}\\left( x\\right)  =c_{02}+c_{12}x+c_{22}x^{2}\\  ;\\  p_{3}\\left( x\\right)  =c_{13}+c_{23}x+c_{33}x^{2}$$\n",
    "<p style=\"text-align: right;\">$(3.79)$</p>\n",
    "\n",
    "Entonces, usando la definici√≥n (3.13), debemos tener, para $T_{1}$\n",
    "\n",
    "$$\\begin{array}{lll}T_{1}\\left( p_{1}\\left( x\\right)  \\right)  &=&1\\\\ T_{2}\\left( p_{2}\\left( x\\right)  \\right)  &=&0\\\\ T_{3}\\left( p_{3}\\left( x\\right)  \\right)  &=&0\\end{array} \\  \\Longrightarrow \\  \\begin{array}{lll}c_{01}+c_{11}r_{1}+c_{21}r^{2}_{1}&=&1\\\\ c_{02}+c_{12}r_{1}+c_{22}r^{2}_{1}&=&0\\\\ c_{03}+c_{13}r_{1}+c_{23}r^{2}_{1}&=&0\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.80)$</p>\n",
    "\n",
    "Equivalentemente, para $T_{2}$ y $T_{3}$,\n",
    "\n",
    "$$\\begin{array}{lll}T_{1}\\left( p_{1}\\left( x\\right)  \\right)  &=&0\\\\ T_{2}\\left( p_{2}\\left( x\\right)  \\right)  &=&1\\\\ T_{3}\\left( p_{3}\\left( x\\right)  \\right)  &=&0\\end{array} \\  \\Longrightarrow \\  \\begin{array}{lll}c_{01}+c_{11}r_{2}+c_{21}r^{2}_{2}&=&0\\\\ c_{02}+c_{12}r_{2}+c_{22}r^{2}_{2}&=&1\\\\ c_{03}+c_{13}r_{2}+c_{23}r^{2}_{2}&=&0\\end{array} \\  \\wedge \\  \\begin{array}{lll}T_{1}\\left( p_{1}\\left( x\\right)  \\right)  &=&0\\\\ T_{2}\\left( p_{2}\\left( x\\right)  \\right)  &=&0\\\\ T_{3}\\left( p_{3}\\left( x\\right)  \\right)  &=&1\\end{array} \\  \\Longrightarrow \\  \\begin{array}{lll}c_{01}+c_{11}r_{3}+c_{21}r^{2}_{3}&=&0\\\\ c_{02}+c_{12}r_{3}+c_{22}r^{2}_{3}&=&0\\\\ c_{03}+c_{13}r_{3}+c_{23}r^{2}_{3}&=&1\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.81)$</p>\n",
    "\n",
    "Para el polinomio $p_{1}(x)$ tenemos que\n",
    "\n",
    "$$\\begin{array}{lll}c_{01}+c_{11}r_{1}+c_{21}r^{2}_{1}&=&1\\\\ c_{01}+c_{11}r_{2}+c_{21}r^{2}_{2}&=&0\\\\ c_{01}+c_{11}r_{3}+c_{21}r^{2}_{3}&=&0\\end{array} \\  \\Longrightarrow \\  \\begin{array}{lll}c_{11}\\left( r_{1}-r_{2}\\right)  +c_{21}\\left( r^{2}_{1}-r^{2}_{2}\\right)  &=&1\\\\ c_{11}\\left( r_{2}-r_{3}\\right)  +c_{21}\\left( r^{2}_{2}-r^{2}_{3}\\right)  &=&0\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.82)$</p>\n",
    "\n",
    "Entonces,\n",
    "\n",
    "$$\\begin{array}{lll}c_{11}+c_{21}\\left( r_{1}+r_{2}\\right)  &=&\\displaystyle \\frac{1}{r_{1}-r_{2}} \\\\ c_{11}+c_{21}\\left( r_{2}+r_{3}\\right)  &=&0\\end{array} \\  \\Longrightarrow \\  c_{21}=\\frac{1}{\\left( r_{1}-r_{2}\\right)  \\left( r_{2}-r_{3}\\right)  }$$\n",
    "<p style=\"text-align: right;\">$(3.83)$</p>\n",
    "\n",
    "As√≠ que,\n",
    "\n",
    "$$p_{1}\\left( x\\right)  =\\frac{\\left( x-r_{2}\\right)  \\left( x-r_{3}\\right)  }{\\left( r_{1}-r_{2}\\right)  \\left( r_{2}-r_{3}\\right)  }$$\n",
    "<p style=\"text-align: right;\">$(3.84)$</p>\n",
    "\n",
    "De manera an√°loga, podemos mostrar que\n",
    "\n",
    "$$p_{2}\\left( x\\right)  =\\frac{\\left( x-r_{1}\\right)  \\left( x-r_{3}\\right)  }{\\left( r_{2}-r_{1}\\right)  \\left( r_{2}-r_{3}\\right)  } \\wedge p_{3}\\left( x\\right)  =\\frac{\\left( x-r_{1}\\right)  \\left( x-r_{2}\\right)  }{\\left( r_{3}-r_{1}\\right)  \\left( r_{3}-r_{2}\\right)  }$$\n",
    "<p style=\"text-align: right;\">$(3.85)$</p>\n",
    "‚óºÔ∏é"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63037d91",
   "metadata": {},
   "source": [
    "### Formas bilineales.\n",
    "Sea $V$ un $\\mathbb{R}$-espacio vectorial y supongamos que $V$ est√° equipado con un producto interno. Sabemos que\n",
    "\n",
    "- El producto interno es una funci√≥n $\\left( u,v\\right)  \\in V\\times V\\longmapsto \\left< u,v\\right>  \\in \\mathbb{R}$.\n",
    "- Para cada $v\\in V$, la funci√≥n $\\left<, v\\right>\\in V^{\\ast}$ (el espacio dual de $V$), definimos $u\\in V\\longmapsto \\left<u,v\\right>\\in \\mathbb{R}$. Del mismo modo, para cada $u\\in V$, la funci√≥n $\\left<u,\\right>\\in V^{\\ast}$, definimos $v\\in V\\longmapsto \\left<u,v\\right>\\in \\mathbb{R}$.\n",
    "- Supongamos que $\\alpha =\\left\\{ v_{1},...,v_{n}\\right\\}$ es una base de $V$. Entonces podemos utilizar la propiedad lineal de ambos vectores (coordenadas) $u$ y $v$ como sigue: Para $u=\\sum_{i=1}^{n} a_{i}v_{i}$ y $v=\\sum_{i=1}^{n} b_{i}v_{i}$, se tiene que\n",
    "\n",
    "$$\\left< u,v\\right>  =\\left< \\sum^{n}_{i=1} a_{i}v_{i},\\sum^{n}_{i=1} b_{i}v_{i}\\right>  =\\sum^{n}_{i=1} \\sum^{n}_{j=1} a_{i}b_{i}\\left< v_{i},v_{j}\\right>$$\n",
    "<p style=\"text-align: right;\">$(3.86)$</p>\n",
    "\n",
    "Equivalentemente, si interpretamos $\\left( \\left< v_{i},v_{j}\\right>  \\right)  \\in \\mathbb{R}^{n\\times n}$, entonces\n",
    "\n",
    "$$\\begin{array}{lll}\\left< u,v\\right>  &=&\\left( \\begin{matrix}a_{1}&\\cdots &a_{n}\\end{matrix} \\right)  \\left( \\left< v_{i},v_{j}\\right>  \\right)  \\left( \\begin{matrix}b_{1}\\\\ \\vdots \\\\ b_{n}\\end{matrix} \\right)  \\\\ &=&\\left[ u\\right]^{\\top }_{\\alpha }  \\underbrace{\\left( \\left< v_{i},v_{j}\\right>  \\right)  }_{\\left[ \\left< \\  ,\\  \\right>  \\right]^{\\alpha }_{\\alpha }  } \\left[ v\\right]_{\\alpha }  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.87)$</p>\n",
    "\n",
    "En particular,\n",
    "\n",
    "- **(C1):** Si $\\alpha$ es una base ortonomal, entonces $\\left< u,v\\right>  =\\left( \\begin{matrix}a_{1}&\\cdots &a_{n}\\end{matrix} \\right)  \\left( \\begin{matrix}b_{1}\\\\ \\vdots \\\\ b_{n}\\end{matrix} \\right)  =\\left[ u\\right]^{\\top }_{\\alpha }  \\left[ v\\right]_{\\alpha }  =\\sum^{n}_{i=1} a_{i}b_{i}$.\n",
    "- **(C2):** Si $u=v$ y $\\alpha$ es una base ortonormal, entonces $\\left< u,u\\right>  =\\sum^{n}_{i=1} a^{2}_{i}=\\left\\Vert u\\right\\Vert^{2}$.\n",
    "- **(C3):** En general, si $\\left[ \\left< \\  ,\\  \\right>  \\right]^{\\alpha }_{\\alpha }$ es diagonalizable y $\\beta$ es la base ortonormal de autovectores de $\\left< \\  ,\\  \\right>$, entonces se tiene que\n",
    "\n",
    "$$\\begin{array}{lll}\\left< u,v\\right>  &=&\\left[ u\\right]^{\\top }_{\\alpha }  \\left[ \\mathbf{I} \\right]^{\\beta }_{\\alpha }  \\underbrace{\\left( \\left< v_{i},v_{j}\\right>  \\right)  }_{=\\left[ \\left< \\  ,\\  \\right>  \\right]^{\\beta }_{\\beta }  } \\left[ \\mathbf{I} \\right]^{\\beta }_{\\alpha }  \\left[ v\\right]_{\\alpha }  \\\\ &=&\\left[ u\\right]^{\\top }_{\\alpha }  \\left( \\left[ \\mathbf{I} \\right]^{\\beta }_{\\alpha }  \\right)^{\\top }  \\underbrace{\\left( \\left< v_{i},v_{j}\\right>  \\right)  }_{=\\left[ \\left< \\  ,\\  \\right>  \\right]^{\\beta }_{\\beta }  } \\left[ \\mathbf{I} \\right]^{\\beta }_{\\alpha }  \\left[ v\\right]_{\\alpha }  \\\\ &=&\\left( \\left[ \\mathbf{I} \\right]^{\\beta }_{\\alpha }  \\left[ u\\right]_{\\alpha }  \\right)^{\\top }  \\underbrace{\\left( \\left< v_{i},v_{j}\\right>  \\right)  }_{=\\left[ \\left< \\  ,\\  \\right>  \\right]^{\\beta }_{\\beta }  } \\left[ \\mathbf{I} \\right]^{\\beta }_{\\alpha }  \\left[ v\\right]_{\\alpha }  \\\\ &=&\\left[ u\\right]^{\\top }_{\\beta }  \\underbrace{\\left( \\left< v_{i},v_{j}\\right>  \\right)  }_{=\\left[ \\left< \\  ,\\  \\right>  \\right]^{\\beta }_{\\beta }  } \\left[ v\\right]_{\\beta }  \\\\ &=&\\left[ u\\right]^{\\top }_{\\beta }  \\mathrm{diag} \\left\\{ \\lambda_{1} ,...,\\lambda_{n} \\right\\}  \\left[ v\\right]_{\\beta }  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.88)$</p>\n",
    "\n",
    "Donde $\\mathrm{diag} \\left\\{ \\lambda_{1} ,...,\\lambda_{n} \\right\\}$ es la matriz diagonal que tiene como elementos no nulos a los autovalores de $\\left< \\  ,\\  \\right>$.\n",
    "\n",
    "- **(C4):** En particular, si $\\beta =\\left\\{ w_{1},...,w_{n}\\right\\}$, entonces\n",
    "\n",
    "$$\\left< u,u\\right>  =\\sum^{n}_{i=1} \\lambda_{i} \\left< u,w_{i}\\right>^{2}$$\n",
    "<p style=\"text-align: right;\">$(3.89)$</p>\n",
    "\n",
    "Tiene sentido, despu√©s de este arduo trabajo, la siguiente definici√≥n.\n",
    "\n",
    "**<font color='blue'>Definici√≥n 3.14 ‚Äì Forma bilineal:</font>** Sea $V$ un $\\mathbb{K}$-espacio vectorial y $B:V\\times V\\longrightarrow \\mathbb{K}$ una funci√≥n tal que $B(u,v)\\in \\mathbb{K}$ para cada par de vectores $(u,v)\\in V\\times V$. Diremos que $B$ es una **forma bilineal** si $B$ es lineal en cada coordenada. Es decir, $B$ es lineal para cada $v\\in V, B_{v}\\in V^{\\ast}$, donde $B_{v}(u)=B(u,v)$, y para cada $u\\in V, B_{u}\\in V^{\\ast}$, donde $B_{u}(v)=B(v,u)$.\n",
    "\n",
    "Antes de, como siempre, entrar en ejemplos m√°s pr√°cticos, explicitaremos los beneficios te√≥ricos que se desprenden de esta condici√≥n de bilinearidad. En este caso, si $\\alpha =\\left\\{ v_{1},...,v_{n}\\right\\}$ es una base de vectores de $V$, entonces para $u=\\sum^{n}_{i=1} a_{i}v_{i}\\wedge v=\\sum^{n}_{i=1} b_{i}v_{i}$, se tiene que\n",
    "\n",
    "$$\\begin{array}{lll}B\\left( u,v\\right)  &=&B\\left( \\sum^{n}_{i=1} a_{i}v_{i},\\sum^{n}_{j=1} b_{j}v_{j}\\right)  \\\\ &=&B\\left( \\sum^{n}_{i=1} \\sum^{n}_{j=1} a_{i}b_{j}B\\left( v_{i},v_{j}\\right)  \\right)  \\\\ &=&\\left( \\begin{matrix}a_{1}&\\cdots &a_{n}\\end{matrix} \\right)  B\\left( v_{i},v_{j}\\right)  \\left( \\begin{matrix}b_{1}\\\\ \\vdots \\\\ b_{n}\\end{matrix} \\right)  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.90)$</p>\n",
    "\n",
    "Es decir, hemos encontrado una igualdad importante (de hecho, fundamental):\n",
    "\n",
    "$$B\\left( u,v\\right)  =\\left[ u\\right]^{\\top }_{\\alpha }  \\underbrace{B\\left( v_{i},v_{j}\\right)  }_{\\left[ B\\right]^{\\alpha }_{\\alpha }  } \\left[ v\\right]_{\\alpha }$$\n",
    "<p style=\"text-align: right;\">$(3.91)$</p>\n",
    "\n",
    "Lo anterior nos lleva al siguiente resultado.\n",
    "\n",
    "**<font color='crimson'>Teorema 3.12:</font>** *Sea $V$ un $\\mathbb{K}$-espacio vectorial y designemos por $B(V)$ al conjunto de todas las formas bilineales que existen en $V$. Si $\\dim_{\\mathbb{K}}(V)=n$, entonces $B\\left( V\\right)  \\cong \\mathbb{K}^{n\\times n}$. Es decir, estos conjuntos son isomorfos.* ‚óÜ\n",
    "\n",
    "**Ejemplo 3.13:** Sea $\\mathbf{A} =\\left( \\begin{matrix}1&2\\\\ 3&4\\end{matrix} \\right)$. Entonces definimos\n",
    "\n",
    "$$\\begin{array}{lll}B\\left( \\left( x_{1},y_{1}\\right)  ,\\left( x_{2},y_{2}\\right)  \\right)  &=&\\left( x_{1},y_{1}\\right)  \\left( \\begin{matrix}1&2\\\\ 3&4\\end{matrix} \\right)  \\left( \\begin{matrix}x_{2}\\\\ y_{2}\\end{matrix} \\right)  \\\\ &=&\\left( x_{1}+3y_{1},2x_{1}+y_{1}\\right)  \\left( \\begin{matrix}x_{2}\\\\ y_{2}\\end{matrix} \\right)  \\\\ &=&x_{1}x_{2}+3x_{2}y_{1}+2x_{1}y_{2}+4y_{1}y_{2}\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.92)$</p>\n",
    "\n",
    "La funci√≥n $B\\left( \\mathbf{x}_{1} ,\\mathbf{x}_{2} \\right)  =B\\left( \\left( x_{1},y_{1}\\right)  ,\\left( x_{2},y_{2}\\right)  \\right)  =x_{1}x_{2}+3x_{2}y_{1}+2x_{1}y_{2}+4y_{1}y_{2}$ construida de esta manera es, evidentemente, una forma bilineal. ‚óºÔ∏é\n",
    "\n",
    "**<font color='blue'>Definici√≥n 3.15 ‚Äì Matriz de una forma bilineal:</font>** Sea $V$ un $\\mathbb{K}$-espacio vectorial y $\\alpha =\\left\\{ v_{1},...,v_{n}\\right\\}$ una base de vectores de $V$. Sea $B:V\\times V\\longrightarrow \\mathbb{K}$ una forma bilineal sobre $V$. Entonces $\\left[ B\\right]^{\\alpha }_{\\alpha }  =\\left( B\\left( v_{i},v_{j}\\right)  \\right)$ ser√° llamada **matriz de la forma bilineal** $B$ respecto de la base $\\alpha$.\n",
    "\n",
    "Si, para alguna base $\\alpha$, se tiene que $B(v_{i},v_{j})=B(v_{j},v_{i})$ para $i,j=1,...,n$, entonces $[B]_{\\alpha}^{\\alpha}$ es una matriz sim√©trica y, rec√≠procamente, si $[B]_{\\alpha}^{\\alpha}$ es sim√©trica para alguna base $\\alpha$, entonces $B(v_{i},v_{j})=B(v_{j},v_{i})$ para $u,v\\in V$. En tal caso, diremos que $B$ es una **forma bilineal sim√©trica**.\n",
    "\n",
    "**Ejemplo 3.14:** Sea $\\mathbf{A} =\\left( \\begin{matrix}1&2\\\\ 2&5\\end{matrix} \\right)$. Entonces, en la base can√≥nica de $\\mathbb{R}^{2}$, se tiene que\n",
    "\n",
    "$$\\begin{array}{lll}B_{\\mathbf{A} }\\left( \\left( x_{1},y_{1}\\right)  ,\\left( x_{2},y_{2}\\right)  \\right)  &=&\\left( x_{1},y_{1}\\right)  \\left( \\begin{matrix}1&2\\\\ 2&5\\end{matrix} \\right)  \\left( \\begin{matrix}x_{2}\\\\ y_{2}\\end{matrix} \\right)  \\\\ &=&\\left[ \\left( x_{1},y_{1}\\right)  \\right]^{\\top }_{\\mathbf{e} \\left( 2\\right)  }  \\left[ \\mathbf{A} \\right]^{\\mathbf{e} \\left( 2\\right)  }_{\\mathbf{e} \\left( 2\\right)  }  \\left[ \\left( x_{2},y_{2}\\right)  \\right]_{\\mathbf{e} \\left( 2\\right)  }  \\\\ &=&\\left( x_{1}+2y_{1},2x_{1}+5y_{1}\\right)  \\left( \\begin{matrix}x_{2}\\\\ y_{2}\\end{matrix} \\right)  \\\\ &=&x_{1}x_{2}+2x_{2}y_{1}+2x_{1}y_{2}+5y_{1}y_{2}\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.93)$</p>\n",
    "\n",
    "En particular, si $(x_{1},y_{1})=(x_{2},y_{2})=(x,y)$, se tendr√° que\n",
    "\n",
    "$$B_{\\mathbf{A} }\\left( \\left( x,y\\right)  ,\\left( x,y\\right)  \\right)  =x^{2}+4xy+5y^{2}$$\n",
    "<p style=\"text-align: right;\">$(3.94)$</p>\n",
    "‚óºÔ∏é"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c73e40",
   "metadata": {},
   "source": [
    "### Formas cuadr√°ticas.\n",
    "Procedemos ahora a estudiar, por tanto, otro tipo de descomposici√≥n. Puntualmente, estudiaremos las **formas cuadr√°ticas** inducidas por una matriz sim√©trica. Las formas cuadr√°ticas son la base de varios problemas de optimizaci√≥n (que tienen como objetivo encontrar m√°ximos o m√≠nimos relativos de una funci√≥n definida en alg√∫n subespacio determinado). En particular, queremos caracterizar un tipo especial de matrices, llamadas **matrices definidas positivas**, y mostraremos una aplicaci√≥n sencilla de las formas cuadr√°ticas en la descripci√≥n (y transformaci√≥n) de ciertos dominios de $\\mathbb{R}^{2}$ conocidos como secciones c√≥nicas, y que resultan de la intersecci√≥n de un cilindro circular recto con un plano con cierta orientaci√≥n.\n",
    "\n",
    "**<font color='blue'>Definici√≥n 3.16 ‚Äì Forma cuadr√°tica:</font>** Dada la matriz sim√©trica $\\mathbf{A}\\in \\mathbb{R}^{n\\times n}$, definimos la aplicaci√≥n\n",
    "\n",
    "$$\\begin{array}{ll}q:&\\mathbb{R}^{n} \\longrightarrow \\mathbb{R} \\\\ &\\mathbf{x} \\longmapsto q\\left( \\mathbf{x} \\right)  =\\mathbf{x}^{\\top } \\mathbf{A} \\mathbf{x} \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.95)$</p>\n",
    "\n",
    "y que es llamada **forma cuadr√°tica** en $\\mathbb{R}^{n}$.\n",
    "\n",
    "Notemos que una forma cuadr√°tica en $\\mathbb{R}^{n}$ es **homog√©nea de grado 2**. Es decir, $\\forall \\mathbf{x} \\in \\mathbb{R}^{n} ,q\\left( \\lambda \\mathbf{x} \\right)  =\\lambda^{2} q\\left( \\mathbf{x} \\right)$. En efecto, $q\\left( \\lambda \\mathbf{x} \\right)  =\\left( \\lambda \\mathbf{x} \\right)^{\\top }  \\mathbf{A} \\left( \\lambda \\mathbf{x} \\right)  =\\lambda \\mathbf{x}^{\\top } \\mathbf{A} \\lambda \\mathbf{x} =\\lambda^{2} \\mathbf{x}^{\\top } \\mathbf{A} \\mathbf{x} =\\lambda^{2} q\\left( \\mathbf{x} \\right)$.\n",
    "\n",
    "Lo anterior motiva la siguiente definici√≥n.\n",
    "\n",
    "**<font color='blue'>Definici√≥n 3.17 ‚Äì Matriz (semi-)definida positiva (negativa):</font>** Sea $\\mathbf{A}\\in \\mathbb{R}^{n\\times n}$ una matriz sim√©trica. Diremos que:\n",
    "\n",
    "- **(D1):** $\\mathbf{A}$ es **definida positiva** si, para todo $\\mathbf{x}\\in \\mathbb{R}^{n}$, tal que $\\mathbf{x}\\neq \\mathbf{0}$, se tiene que $\\mathbf{x}^{\\top}\\mathbf{A}\\mathbf{x} > 0$.\n",
    "- **(D2):** $\\mathbf{A}$ es **semi-definida positiva** si, para todo $\\mathbf{x}\\in \\mathbb{R}^{n}$, tal que $\\mathbf{x}\\neq \\mathbf{0}$, se tiene que $\\mathbf{x}^{\\top}\\mathbf{A}\\mathbf{x} \\geq 0$.\n",
    "- **(D3):** $\\mathbf{A}$ es **definida negativa** si, para todo $\\mathbf{x}\\in \\mathbb{R}^{n}$, tal que $\\mathbf{x}\\neq \\mathbf{0}$, se tiene que $\\mathbf{x}^{\\top}\\mathbf{A}\\mathbf{x} < 0$.\n",
    "- **(D4):** $\\mathbf{A}$ es **semi-definida negativa** si, para todo $\\mathbf{x}\\in \\mathbb{R}^{n}$, tal que $\\mathbf{x}\\neq \\mathbf{0}$, se tiene que $\\mathbf{x}^{\\top}\\mathbf{A}\\mathbf{x} \\leq 0$.\n",
    "\n",
    "**<font color='crimson'>Teorema 3.13:</font>** *Sea $V$ un $\\mathbb{R}$-espacio vectorial y $q$ una forma cuadr√°tica sobre $V$. Entonces existe ua base ortonormal $\\alpha$ de $V$ tal que*\n",
    "\n",
    "$$q\\left( u\\right)  =\\sum^{n}_{i=1} \\lambda_{i} u^{2}_{i}\\  ;\\  \\left[ u\\right]_{\\alpha }  =\\left( u_{1},...,u_{n}\\right)^{\\top }$$\n",
    "<p style=\"text-align: right;\">$(3.96)$</p>\n",
    "\n",
    "*para $u\\in V$.* ‚óÜ\n",
    "\n",
    "**Ejemplo 3.15:** Si definimos la forma cuadr√°tica $q:\\mathbb{R}^{2}\\longrightarrow \\mathbb{R}$ tal que $q(x,y)=x^{2}-2xy-y^{2}$, vamos a intentar expresar $q$ de la forma $q\\left( x,y\\right)  =a_{1}\\left( b_{1}x+c_{1}y\\right)^{2}  +a_{2}\\left( b_{2}x+c_{2}y\\right)^{2}$ (llamada **forma can√≥nica**).\n",
    "\n",
    "Partimos, r√°pidamente, expresando $q$ en su forma matricial como sigue\n",
    "\n",
    "$$q\\left( x,y\\right)  =x^{2}-2xy-y^{2}=\\underbrace{\\left[ \\left( x,y\\right)  \\right]^{\\intercal }_{\\mathbf{e} \\left( 2\\right)  }  }_{\\mathbf{x}^{\\intercal } } \\overbrace{\\left[ q\\right]^{\\mathbf{e} \\left( 2\\right)  }_{\\mathbf{e} \\left( 2\\right)  }  }^{\\mathbf{A} } \\underbrace{\\left[ \\left( x,y\\right)  \\right]_{\\mathbf{e} \\left( 2\\right)  }  }_{\\mathbf{x} } =\\left( x,y\\right)  \\left( \\begin{matrix}1&-1\\\\ -1&-1\\end{matrix} \\right)  \\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.97)$</p>\n",
    "\n",
    "Observemos que la ecuaci√≥n (3.97) se escribe en la base can√≥nica de $\\mathbb{R}^{2}$, la que es ortonormal respecto del producto interno usual. Procedemos ahora a diagonalizar la matriz $\\mathbf{A}=\\left[ q\\right]^{\\mathbf{e} \\left( 2\\right)  }_{\\mathbf{e} \\left( 2\\right)  }$. Para ello, construimos el correspondiente polinomio caracter√≠stico $P_{\\mathbf{A}}(\\lambda)$ como sigue,\n",
    "\n",
    "$$\\begin{array}{lll}P_{\\mathbf{A} }\\left( \\lambda \\right)  &=&\\det \\left( \\begin{matrix}1-\\lambda &-1\\\\ -1&-1-\\lambda \\end{matrix} \\right)  \\\\ &=&-\\left( 1-\\lambda \\right)  \\left( 1-\\lambda \\right)  -1\\\\ &=&\\lambda^{2} -1\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.98)$</p>\n",
    "\n",
    "As√≠ que los autovalores de $\\mathbf{A}$ son $\\lambda_{1}=\\sqrt{2}\\wedge \\lambda_{2}=-\\sqrt{2}$. Ahora determinamos los autoespacios respectivos,\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{v} \\in \\left( \\mathbb{R}^{2\\times 1} \\right)_{\\lambda }  &\\Longleftrightarrow &\\mathbf{v} \\in \\mathbb{R}^{2\\times 1} \\wedge \\mathbf{A} \\mathbf{v} =\\lambda \\mathbf{v} \\\\ &\\Longleftrightarrow &\\mathbf{v} =\\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  \\wedge \\left( \\begin{matrix}1&-1\\\\ -1&-1\\end{matrix} \\right)  \\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  =\\lambda \\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  \\\\ &\\Longleftrightarrow &\\mathbf{v} =\\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  \\wedge \\begin{cases}\\begin{array}{lll}x-y&=&\\lambda x\\\\ -x-y&=&\\lambda y\\end{array} &\\end{cases} \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.99)$</p>\n",
    "\n",
    "Tenemos pues dos casos a estudiar. El primero es $\\lambda_{1}=\\sqrt{2}$, en cuyo caso se tiene que\n",
    "\n",
    "$$\\begin{array}{lll}x-y&=&\\sqrt{2} x\\\\ -x-y&=&\\sqrt{2} y\\end{array} \\  \\Longleftrightarrow \\  y=\\left( 1-\\sqrt{2} \\right)  x$$\n",
    "<p style=\"text-align: right;\">$(3.100)$</p>\n",
    "\n",
    "Luego,\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{v} \\in \\left( \\mathbb{R}^{2\\times 1} \\right)_{\\lambda_{1} =\\sqrt{2} }  &\\Longleftrightarrow &\\mathbf{v} \\in \\mathbb{R}^{2\\times 1} \\wedge \\mathbf{v} =\\left( \\begin{matrix}x\\\\ \\left( 1-\\sqrt{2} \\right)  x\\end{matrix} \\right)  \\\\ &\\Longleftrightarrow &\\mathbf{v} \\in \\mathbb{R}^{2\\times 1} \\wedge \\mathbf{v} =x\\left( \\begin{matrix}1\\\\ 1-\\sqrt{2} \\end{matrix} \\right)  ;x\\in \\mathbb{R} \\\\ &\\Longleftrightarrow &\\left( \\mathbb{R}^{2\\times 1} \\right)_{\\lambda_{1} =\\sqrt{2} }  =\\left< \\left\\{ \\left( \\begin{matrix}1\\\\ 1-\\sqrt{2} \\end{matrix} \\right)  \\right\\}  \\right>  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.101)$</p>\n",
    "\n",
    "Por otro lado, para $\\lambda_{2}=-\\sqrt{2}$, tenemos que\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{v} \\in \\left( \\mathbb{R}^{2\\times 1} \\right)_{\\lambda_{2} =-\\sqrt{2} }  &\\Longleftrightarrow &\\mathbf{v} \\in \\mathbb{R}^{2\\times 1} \\wedge \\mathbf{v} =\\left( \\begin{matrix}x\\\\ \\left( 1+\\sqrt{2} \\right)  x\\end{matrix} \\right)  \\\\ &\\Longleftrightarrow &\\mathbf{v} \\in \\mathbb{R}^{2\\times 1} \\wedge \\mathbf{v} =x\\left( \\begin{matrix}1\\\\ 1+\\sqrt{2} \\end{matrix} \\right)  ;x\\in \\mathbb{R} \\\\ &\\Longleftrightarrow &\\left( \\mathbb{R}^{2\\times 1} \\right)_{\\lambda_{2} =-\\sqrt{2} }  =\\left< \\left\\{ \\left( \\begin{matrix}1\\\\ 1+\\sqrt{2} \\end{matrix} \\right)  \\right\\}  \\right>  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.101)$</p>\n",
    "\n",
    "Construiremos ahora una base ortonormal de autovectores a partir de la base $\\beta$ definida como\n",
    "\n",
    "$$\\beta =\\left\\{ \\left( \\begin{matrix}1\\\\ 1-\\sqrt{2} \\end{matrix} \\right)  ,\\left( \\begin{matrix}1\\\\ 1+\\sqrt{2} \\end{matrix} \\right)  \\right\\}$$\n",
    "<p style=\"text-align: right;\">$(3.102)$</p>\n",
    "\n",
    "Notemos que $\\beta$ ya es ortogonal, as√≠ que, para construir la base ortonormal, basta con dividir cada vector por la norma respectiva. De este modo,\n",
    "\n",
    "$$\\alpha =\\left\\{ \\left( \\begin{matrix}\\displaystyle \\frac{1}{\\sqrt{4-2\\sqrt{2} } } \\\\ \\displaystyle \\frac{1-\\sqrt{2} }{\\sqrt{4-2\\sqrt{2} } } \\end{matrix} \\right)  ,\\left( \\begin{matrix}\\displaystyle \\frac{1}{\\sqrt{4+2\\sqrt{2} } } \\\\ \\displaystyle \\frac{1+\\sqrt{2} }{\\sqrt{4+2\\sqrt{2} } } \\end{matrix} \\right)  \\right\\}$$\n",
    "<p style=\"text-align: right;\">$(3.103)$</p>\n",
    "\n",
    "Ahora construiremos $[v]_{\\alpha}$. Por tanto,\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{v} &=&\\left< \\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  ,\\left( \\begin{matrix}\\displaystyle \\frac{1}{\\sqrt{4-2\\sqrt{2} } } \\\\ \\displaystyle \\frac{1-\\sqrt{2} }{\\sqrt{4-2\\sqrt{2} } } \\end{matrix} \\right)  \\right>  \\left( \\begin{matrix}\\displaystyle \\frac{1}{\\sqrt{4-2\\sqrt{2} } } \\\\ \\displaystyle \\frac{1-\\sqrt{2} }{\\sqrt{4-2\\sqrt{2} } } \\end{matrix} \\right)  +\\left< \\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  ,\\left( \\begin{matrix}\\displaystyle \\frac{1}{\\sqrt{4+2\\sqrt{2} } } \\\\ \\displaystyle \\frac{1+\\sqrt{2} }{\\sqrt{4+2\\sqrt{2} } } \\end{matrix} \\right)  \\right>  \\left( \\begin{matrix}\\displaystyle \\frac{1}{\\sqrt{4+2\\sqrt{2} } } \\\\ \\displaystyle \\frac{1+\\sqrt{2} }{\\sqrt{4+2\\sqrt{2} } } \\end{matrix} \\right)  \\\\ &=&\\left( \\displaystyle \\frac{x}{\\sqrt{4-2\\sqrt{2} } } +\\displaystyle \\frac{y\\left( 1-\\sqrt{2} \\right)  }{\\sqrt{4-2\\sqrt{2} } } \\right)  \\left( \\begin{matrix}\\displaystyle \\frac{1}{\\sqrt{4-2\\sqrt{2} } } \\\\ \\displaystyle \\frac{1-\\sqrt{2} }{\\sqrt{4-2\\sqrt{2} } } \\end{matrix} \\right)  +\\left( \\displaystyle \\frac{x}{\\sqrt{4+2\\sqrt{2} } } +\\displaystyle \\frac{y\\left( 1+\\sqrt{2} \\right)  }{\\sqrt{4+2\\sqrt{2} } } \\right)  \\left( \\begin{matrix}\\displaystyle \\frac{1}{\\sqrt{4+2\\sqrt{2} } } \\\\ \\displaystyle \\frac{1+\\sqrt{2} }{\\sqrt{4+2\\sqrt{2} } } \\end{matrix} \\right)  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.104)$</p>\n",
    "\n",
    "As√≠ que, finalmente,\n",
    "\n",
    "$$q\\left( x,y\\right)  =\\sqrt{2} \\left( \\frac{x}{\\sqrt{4-2\\sqrt{2} } } +\\frac{y\\left( 1-\\sqrt{2} \\right)  }{\\sqrt{4-2\\sqrt{2} } } \\right)^{2}  -\\sqrt{2} \\left( \\frac{x}{\\sqrt{4+2\\sqrt{2} } } +\\frac{y\\left( 1+\\sqrt{2} \\right)  }{\\sqrt{4+2\\sqrt{2} } } \\right)^{2}$$\n",
    "<p style=\"text-align: right;\">$(3.105)$</p>\n",
    "\n",
    "es la forma can√≥nica buscada. ‚óºÔ∏é"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631d9f15",
   "metadata": {},
   "source": [
    "### Aplicaciones a la geometr√≠a anal√≠tica.\n",
    "Lo estudiado en relaci√≥n a las formas y, en particular, las formas cuadr√°ticas (y sus formas normales), nos permiten representar de forma compacta y muy eficiente a cualquier **secci√≥n c√≥nica**. De los cursos de C√°lculo sabemos que las secciones c√≥nicas son regiones en $\\mathbb{R}^{2}$ que corresponden a la intersecci√≥n de un cono circular recto con un plano cuya orientaci√≥n puede ser de tres tipos: Paralelo a la generatriz del cono (**par√°bola**), paralelo al eje vertical del cono (**hip√©rbola**) y oblicua respecto de la generatriz y el eje del cono sin pasar por su v√©rtice (**elipse**). La **circunferencia** tambi√©n se considera una secci√≥n c√≥nica, pero es el caso particular de la elipse (cuando sus ejes son exactamente iguales), aunque es posible obtener como resultado de un plano que es perpendicular al eje del cono. La Fig. (3.3) muestra un esquema que permite entender como se generan las secciones c√≥nicas y los planos previamente mencionados.\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"figures/fig_3_3.png\" width=\"300\"></p>\n",
    "<p style=\"text-align: center;\">Fig. (3.3): Un esquema que ilustra c√≥mo se generan las secciones c√≥nicas\n",
    "\n",
    "Recordemos que las secciones c√≥nicas tienen sus correspondientes **ecuaciones** para el caso en el cual su orientaci√≥n es tal que sus ejes de simetr√≠a son paralelos, respectivamente, a los ejes X e Y en el plano $\\mathbb{R}^{2}$. En el caso de la elipse, si √©sta tiene su centro en el punto $(x_{0},y_{0})$ y sus semiejes mayor y menor tienen longitudes $a$ y $b$, respectivamente, entonces su ecuaci√≥n es\n",
    "\n",
    "$$\\frac{\\left( x-x_{0}\\right)^{2}  }{a^{2}} +\\frac{\\left( y-y_{0}\\right)^{2}  }{b^{2}} =1$$\n",
    "<p style=\"text-align: right;\">$(3.106)$</p>\n",
    "\n",
    "La elipse puede verse en la Fig. (3.4a).\n",
    "\n",
    "Una hip√©rbola de semiejes mayor y menor de longitudes $a$ y $b$, respectivamente, con centro en el punto $(x_{0},y_{0})$, tiene por ecuaci√≥n\n",
    "\n",
    "$$\\frac{\\left( x-x_{0}\\right)^{2}  }{a^{2}} -\\frac{\\left( y-y_{0}\\right)^{2}  }{b^{2}} =1$$\n",
    "<p style=\"text-align: right;\">$(3.107)$</p>\n",
    "\n",
    "la hip√©rbola se ilustra en la Fig. (3.4b).\n",
    "\n",
    "Finalmente, una par√°bola con foco $F$ en el punto $(x_{0},y_{0})$ y con directriz cuya ecuaci√≥n es $x=x_{0}+p$, tiene por ecuaci√≥n\n",
    "\n",
    "$$\\left( y-y_{0}\\right)^{2}  =4p\\left( x-x_{0}\\right)$$\n",
    "<p style=\"text-align: right;\">$(3.108)$</p>\n",
    "\n",
    "Igualmente, la par√°bola se ilustra en la Fig. (3.4c).\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"figures/fig_3_4.png\" width=\"1000\"></p>\n",
    "<p style=\"text-align: center;\">Fig. (3.4): (a) Una elipse centrada en el punto $(x_{0},y_{0})$, con semiejes mayor y menor iguales de longitud $a$ y $b$; (b) Una hip√©rbola centrada en el punto $(x_{0},y_{0})$, con semiejes mayor y menor iguales de longitud $a$ y $b$; (c) Una par√°bola con foco en el punto $(x_{0},y_{0})$ y distancia focal igual a $p$\n",
    "    \n",
    "Las ecuaciones de las secciones c√≥nicas son todas casos particulares de una ecuaci√≥n denominada **ecuaci√≥n general de segundo grado**, y que puede escribirse como $ax^{2}+bxy+cy^{2}+dx+ey+f=0$, donde todos los coeficientes pertenecen a $\\mathbb{R}$. De esta manera, es posible definir una secci√≥n c√≥nica como el conjunto de todos los puntos $(x,y)\\in \\mathbb{R}^{2}$ que satisfacen la ecuaci√≥n general de segundo grado; es decir, pertenecen al conjunto $C=\\left\\{ \\left( x,y\\right)  \\in \\mathbb{R}^{2} :ax^{2}+bxy+cy^{2}+dx+ey+f=0\\right\\}$. Equivalentemente, podemos reescribir esta ecuaci√≥n como la **suma de una forma cuadr√°tica y una forma lineal**, ya que\n",
    "\n",
    "$$\\underbrace{ax^{2}+bxy+cy^{2}}_{=q\\left( x,y\\right)  } +\\underbrace{dx+ey}_{=L\\left( x,y\\right)  } +f=0$$\n",
    "<p style=\"text-align: right;\">$(3.109)$</p>\n",
    "\n",
    "Donde $q(x,y)$ es la forma cuadr√°tica y $L(x,y)$ es la forma lineal. En t√©rminos matriciales, podemos entonces reescribir la ecuaci√≥n general de segundo grado como sigue,\n",
    "\n",
    "$$\\left( x,y\\right)  \\left( \\begin{matrix}a&b/2\\\\ b/2&c\\end{matrix} \\right)  \\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  +\\left( d,e\\right)  \\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  +f=0$$\n",
    "<p style=\"text-align: right;\">$(3.110)$</p>\n",
    "\n",
    "Aplicando el teorema (3.13) a la ecuaci√≥n (3.110), obtenemos\n",
    "\n",
    "$$\\left( x_{1},y_{1}\\right)  \\left( \\begin{matrix}\\lambda_{1} &0\\\\ 0&\\lambda_{2} \\end{matrix} \\right)  \\left( \\begin{matrix}x_{1}\\\\ y_{1}\\end{matrix} \\right)  +\\left( d,e\\right)  \\underbrace{\\left( \\begin{matrix}\\left< \\mathbf{v}_{1} ,\\mathbf{e}_{1} \\right>  &\\left< \\mathbf{v}_{2} ,\\mathbf{e}_{1} \\right>  \\\\ \\left< \\mathbf{v}_{1} ,\\mathbf{e}_{2} \\right>  &\\left< \\mathbf{v}_{2} ,\\mathbf{e}_{2} \\right>  \\end{matrix} \\right)  }_{=\\left[ I\\right]^{\\mathbf{e} \\left( 2\\right)  }_{\\alpha }  } \\left( \\begin{matrix}x_{1}\\\\ y_{1}\\end{matrix} \\right)  +f=0$$\n",
    "<p style=\"text-align: right;\">$(3.111)$</p>\n",
    "\n",
    "Donde $\\lambda_{1}$ y $\\lambda_{2}$ son los autovalores de la forma matricial de $q$, $\\alpha =\\left\\{ \\mathbf{v}_{1} ,\\mathbf{v}_{2} \\right\\}  \\subset \\mathbb{R}^{2}$ una base ortonormal de autovectores y\n",
    "\n",
    "$$\\left( \\begin{matrix}x_{1}\\\\ y_{1}\\end{matrix} \\right)  =\\underbrace{\\left( \\begin{matrix}\\left< \\mathbf{v}_{1} ,\\mathbf{e}_{1} \\right>  &\\left< \\mathbf{v}_{2} ,\\mathbf{e}_{1} \\right>  \\\\ \\left< \\mathbf{v}_{1} ,\\mathbf{e}_{2} \\right>  &\\left< \\mathbf{v}_{2} ,\\mathbf{e}_{2} \\right>  \\end{matrix} \\right)  }_{=\\left[ I\\right]^{\\mathbf{e} \\left( 2\\right)  }_{\\alpha }  } \\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.112)$</p>\n",
    "\n",
    "As√≠ que, despu√©s de las transformaciones hechas en la ecuaci√≥n general de segundo grado, tenemos la **ecuaci√≥n reducida**\n",
    "\n",
    "$$\\lambda_{1} x^{2}_{1}+\\lambda_{2} y^{2}_{1}+Dx_{1}+Ey_{1}+f=0\\  ;\\  \\left( D,E\\right)  =\\left( d,e\\right)  \\left( \\begin{matrix}\\left< \\mathbf{v}_{1} ,\\mathbf{e}_{1} \\right>  &\\left< \\mathbf{v}_{2} ,\\mathbf{e}_{1} \\right>  \\\\ \\left< \\mathbf{v}_{1} ,\\mathbf{e}_{2} \\right>  &\\left< \\mathbf{v}_{2} ,\\mathbf{e}_{2} \\right>  \\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.113)$</p>\n",
    "\n",
    "Por lo tanto, se desprenden varios casos de inter√©s conforme los valores que toman $\\lambda_{1}$ y $\\lambda_{2}$.\n",
    "\n",
    "**<font color='forestgreen'>Caso 1 ‚Äì $\\lambda_{1}\\lambda_{2}\\neq 0$:</font>** En este caso, completamos cuadrados en la ecuaci√≥n (3.113), obteniendo\n",
    "\n",
    "$$\\lambda_{1} \\left( x_{1}+\\frac{D}{2\\lambda_{1} } \\right)^{2}  -\\frac{D^{2}}{4\\lambda_{1} } +\\lambda_{2} \\left( y_{1}+\\frac{D}{2\\lambda_{2} } \\right)^{2}  -\\frac{E}{4\\lambda_{2} } +f=0$$\n",
    "<p style=\"text-align: right;\">$(3.114)$</p>\n",
    "\n",
    "Hagamos el cambio de variables $x_{2}=x_{1}+\\frac{D}{2\\lambda_{1} }$, $y_{2}=y_{1}+\\frac{D}{2\\lambda_{2} }$ y $F=f-\\frac{D^{2}}{4\\lambda_{1} } -\\frac{E^{2}}{4\\lambda_{2} }$. Luego reescribimos la ecuaci√≥n (3.114) como\n",
    "\n",
    "$$\\lambda_{1} x^{2}_{2}+\\lambda_{2} y^{2}_{2}+F=0$$\n",
    "<p style=\"text-align: right;\">$(3.115)$</p>\n",
    "\n",
    "<font color='forestgreen'>Caso 1.1 ‚Äì $\\lambda_{1}>0, \\lambda_{2}> 0$:</font> En este caso, tenemos:\n",
    "\n",
    "- $F>0$: Por lo tanto, ning√∫n punto $(x_{2},y_{2})\\in \\mathbb{R}^{2}$ satisface la ecuaci√≥n (3.115) y, por lo tanto, $C=\\emptyset$.\n",
    "- $F=0$: Entonces, para todo punto $(x,y)\\in \\mathbb{R}^{2}$, se tiene que $C$ representa un **punto** en el sistema $(x_{2},y_{2})$, y que podemos escribir como\n",
    "\n",
    "$$C:\\left( -\\frac{D}{2\\lambda_{1} } ,-\\frac{E}{2\\lambda_{2} } \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.116)$</p>\n",
    "\n",
    "- $F<0$: Entonces los puntos $(x_{2},y_{2})\\in \\mathbb{R}^{2}$ que satisfacen la ecuaci√≥n (3.115) trazan una **elipse** en el sistema $(x_{2},y_{2})$, cuya ecuaci√≥n es\n",
    "\n",
    "$$C:\\left\\{ \\left( x_{2},y_{2}\\right)  \\in \\mathbb{R}^{2} :\\frac{x^{2}_{2}}{-\\frac{F}{\\lambda_{1} } } +\\frac{y^{2}_{2}}{-\\frac{E}{\\lambda_{2} } } =1\\right\\}$$\n",
    "<p style=\"text-align: right;\">$(3.117)$</p>\n",
    "\n",
    "<font color='forestgreen'>Caso 1.2 ‚Äì $\\lambda_{1}>0, \\lambda_{2}< 0$:</font> En este caso, tenemos:\n",
    "\n",
    "- $F=0$: Entonces los puntos $(x_{2},y_{2})\\in \\mathbb{R}^{2}$ que satisfacen la ecuaci√≥n (3.115) trazan un **par de rectas concurrentes** en dicho sistema, cuya ecuaci√≥n es\n",
    "\n",
    "$$C:\\left\\{ \\left( x_{2},y_{2}\\right)  \\in \\mathbb{R}^{2} :y_{2}=\\pm \\sqrt{-\\frac{\\lambda_{1} }{\\lambda_{2} } } x_{2}\\right\\}$$\n",
    "<p style=\"text-align: right;\">$(3.118)$</p>\n",
    "\n",
    "- $F\\neq 0$: Entonces los puntos $(x_{2},y_{2})\\in \\mathbb{R}^{2}$ que satisfacen la ecuaci√≥n (3.115) trazan una **hip√©rbola** en dicho sistema, cuya ecuaci√≥n es\n",
    "\n",
    "$$C:\\left\\{ \\left( x_{2},y_{2}\\right)  \\in \\mathbb{R}^{2} :\\frac{x^{2}_{2}}{-\\frac{F}{\\lambda_{1} } } +\\frac{y^{2}_{2}}{-\\frac{E}{\\lambda_{2} } } =1\\right\\}$$\n",
    "<p style=\"text-align: right;\">$(3.119)$</p>\n",
    "\n",
    "En conclusi√≥n, para $\\lambda_{1}$ y $\\lambda_{2}$ no nulos, tenemos que\n",
    "\n",
    "$$\\lambda_{1} \\neq 0\\wedge \\lambda_{2} \\neq 0\\Longrightarrow \\begin{cases}\\left( \\mathrm{i} \\right)  &\\lambda_{1} \\lambda_{2} >0\\Longrightarrow C:\\begin{cases}\\mathrm{Vacio} :&\\emptyset \\\\ \\mathrm{Punto} :&\\left( -\\displaystyle \\frac{D}{2\\lambda_{1} } ,-\\displaystyle \\frac{E}{2\\lambda_{2} } \\right)  \\\\ \\mathrm{Elipse} :&\\displaystyle \\frac{x^{2}_{2}}{-F/\\lambda_{1} } +\\displaystyle \\frac{y^{2}_{2}}{-E/\\lambda_{2} } =1\\end{cases} \\\\ \\left( \\mathrm{i} \\mathrm{i} \\right)  &\\lambda_{1} \\lambda_{2} <0\\Longrightarrow C:\\begin{cases}\\mathrm{Par} \\  \\mathrm{de} \\  \\mathrm{rectas} \\  \\mathrm{concurrentes} :&y_{2}=\\pm \\sqrt{-\\frac{\\lambda_{1} }{\\lambda_{2} } } x_{2}\\\\ \\mathrm{Hiperbola} :&\\displaystyle \\frac{x^{2}_{2}}{-F/\\lambda_{1} } +\\frac{y^{2}_{2}}{-E/\\lambda_{2} } =1\\end{cases} \\end{cases}$$\n",
    "<p style=\"text-align: right;\">$(3.120)$</p>\n",
    "\n",
    "**<font color='forestgreen'>Caso 2 ‚Äì $\\lambda_{1}\\lambda_{2}= 0$:</font>** Nuevamente tenemos dos subcasos distintos.\n",
    "\n",
    "<font color='forestgreen'>Caso 2.1 ‚Äì $\\lambda_{1}=\\lambda_{2}= 0$:</font> En este caso tenemos que la ecuaci√≥n (3.115) se reduce a la descripci√≥n de una **recta**:\n",
    "\n",
    "$$C:\\left\\{ \\left( x_{1},y_{1}\\right)  \\in \\mathbb{R}^{2} :Dx_{1}+Ey_{1}+f=0\\right\\}$$\n",
    "<p style=\"text-align: right;\">$(3.121)$</p>\n",
    "\n",
    "<font color='forestgreen'>Caso 2.2 ‚Äì $\\lambda_{1}=0,\\lambda_{2}\\neq 0$:</font> En este caso, hacemos el cambio de variables $y_{2}=y_{1}+\\frac{E}{2\\lambda_{2} }$ y $F=f-\\frac{E^{2}}{4\\lambda_{2} }$. De esta manera, la ecuaci√≥n (3.115) se reduce a una **par√°bola** o sus degeneraciones:\n",
    "\n",
    "$$C:\\left\\{ \\left( x_{2},y_{2}\\right)  \\in \\mathbb{R}^{2} :\\lambda_{2} y^{2}_{2}+Dx_{1}+F=0\\right\\}$$\n",
    "<p style=\"text-align: right;\">$(3.122)$</p>\n",
    "\n",
    "En conclusi√≥n, para $\\lambda_{1}\\lambda_{2}=0$, tenemos que\n",
    "\n",
    "$$\\lambda_{1} \\lambda_{2} =0\\Longrightarrow C:\\begin{cases}\\mathrm{Vacio} :&\\emptyset \\\\ \\mathrm{Una} \\  \\mathrm{recta} :&Dx_{1}+Ey_{1}+f=0\\\\ \\mathrm{Parabola} \\  \\mathrm{o} \\  \\mathrm{par} \\  \\mathrm{de} \\  \\mathrm{rectas} \\  \\mathrm{paralelas} :&\\lambda_{2} y^{2}_{2}+Dx_{1}+F=0\\end{cases}$$\n",
    "<p style=\"text-align: right;\">$(3.123)$</p>\n",
    "\n",
    "Finalmente, observemos que $\\left[ q\\right]^{\\mathbf{e} \\left( 2\\right)  }_{\\mathbf{e} \\left( 2\\right)  }  =\\left[ I\\right]^{\\mathbf{e} \\left( 2\\right)  }_{\\alpha }  \\left[ q\\right]^{\\alpha }_{\\alpha }  \\left[ I\\right]^{\\alpha }_{\\mathbf{e} \\left( 2\\right)  }  =\\mathbf{x}^{\\top } \\mathbf{A} \\mathbf{x}$, de donde tenemos que $\\det \\left( \\mathbf{A} \\right)  =\\det \\left( \\left[ q\\right]^{\\mathbf{e} \\left( 2\\right)  }_{\\mathbf{e} \\left( 2\\right)  }  \\right)  =\\det \\left( \\left[ q\\right]^{\\alpha }_{\\alpha }  \\right)$. Por lo tanto, podemos escribir el producto de los autovalores $\\lambda_{1}$ y $\\lambda_{2}$ asociados a la ecuaci√≥n general de segundo grado como $\\lambda_{1}\\lambda_{2}=-(b^{2}-4ac)/4$. Usando el numerador de este producto (llamado **discriminante**), podemos clasificar las secciones c√≥nicas resultantes de la ecuaci√≥n (3.109) como\n",
    "\n",
    "$$C:ax^{2}+bxy+cy^{2}+dx+ey+f=0\\Longrightarrow \\begin{cases}b^{2}-4ac>0\\Longrightarrow C:&\\begin{cases}\\mathrm{Hiperbola} &\\\\ \\mathrm{Par} \\  \\mathrm{de} \\  \\mathrm{rectas} \\  \\mathrm{concurrentes} &\\end{cases} \\\\ b^{2}-4ac<0\\Longrightarrow C:&\\begin{cases}\\emptyset &\\\\ \\mathrm{Punto} &\\\\ \\mathrm{Elipse} &\\end{cases} \\\\ b^{2}-4ac=0\\Longrightarrow C:&\\begin{cases}\\mathrm{Parabola} &\\\\ \\mathrm{Una} \\  \\mathrm{recta} &\\\\ \\mathrm{Par} \\  \\mathrm{de} \\  \\mathrm{rectas} \\  \\mathrm{concurrentes} &\\\\ \\emptyset &\\end{cases} \\end{cases}$$\n",
    "<p style=\"text-align: right;\">$(3.124)$</p>\n",
    "\n",
    "**Ejemplo 3.16:** Identificaremos y graficaremos la secci√≥n c√≥nica que representa el lugar geom√©trico de todos los puntos $(x,y)\\in \\mathbb{R}^{2}$ que satisfacen la ecuaci√≥n general\n",
    "\n",
    "$$4x^{2}+4y^{2}-8xy+\\frac{33}{2} \\sqrt{2} x-\\frac{31}{2} \\sqrt{2} y+35=0$$\n",
    "<p style=\"text-align: right;\">$(3.125)$</p>\n",
    "\n",
    "En efecto, si eliminamos los coeficientes racionales, podemos expresar este problema como\n",
    "\n",
    "$$C:\\left\\{ \\left( x,y\\right)  \\in \\mathbb{R}^{2} :8x^{2}+8y^{2}-16xy+33\\sqrt{2} x-31\\sqrt{2} y+70=0\\right\\}$$\n",
    "<p style=\"text-align: right;\">$(3.126)$</p>\n",
    "\n",
    "Identificaremos las alternativas posibles de c√≥nicas involucradas en la soluci√≥n de la ecuaci√≥n (3.125) mediante el criterio del discriminante. De esta manera, como $b^{2}-4ac=(-16)^{2}-4\\cdot 8\\cdot 8=256-256=0$, entonces tenemos que $C$ puede ser la descripci√≥n de una par√°bola o de alg√∫n caso degenerado (o bien, ning√∫n punto $(x,y)\\in \\mathbb{R}^{2}$ satisface la ecuacu√≥n (3.125)). Para identificar cu√°l de todas estas alternativas es la que describe a $C$, formulamos la ecuaci√≥n (3.125) en su forma matricial como\n",
    "\n",
    "$$\\left( x,y\\right)  \\left( \\begin{matrix}8&-8\\\\ -8&8\\end{matrix} \\right)  \\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  +\\left( 33\\sqrt{2} ,-32\\sqrt{2} \\right)  \\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  +70=0$$\n",
    "<p style=\"text-align: right;\">$(3.127)$</p>\n",
    "\n",
    "El primer t√©rmino matricial (la forma cuadr√°tica) ser√° designado como $\\mathbf{A}=q(x,y)$. El siguiente paso es, por tanto, determinar los autovalores asociados a $\\mathbf{A}$. De esta manera, construimos el polinomio caracter√≠stico asociado a $\\mathbf{A}$ como sigue\n",
    "\n",
    "$$P_{\\mathbf{A} }\\left( \\lambda \\right)  =\\det \\left( \\begin{matrix}\\lambda -8&-8\\\\ -8&\\lambda -8\\end{matrix} \\right)  =\\left( \\lambda -8\\right)^{2}  -64$$\n",
    "<p style=\"text-align: right;\">$(3.128)$</p>\n",
    "\n",
    "Luego, las ra√≠ces del polinomio caracter√≠stico $P_{\\mathbf{A} }\\left( \\lambda \\right)$ son $\\lambda_{1}=16$ y $\\lambda_{2}=0$. Por lo tanto, ahora debemos determinar los autoespacios asociados a cada uno de estos autovalores. En el caso general, tenemos que\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{u} \\in \\left( \\mathbb{R}^{2\\times 1} \\right)_{\\lambda }  &\\Longleftrightarrow &\\mathbf{u} \\in \\mathbb{R}^{2\\times 1} \\wedge \\mathbf{A} \\mathbf{u} =\\lambda \\mathbf{u} \\\\ &\\Longleftrightarrow &\\mathbf{u} =\\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  \\wedge \\left( \\begin{matrix}8&-8\\\\ -8&8\\end{matrix} \\right)  \\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  =\\lambda \\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  \\\\ &\\Longleftrightarrow &\\mathbf{u} =\\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  \\wedge \\begin{cases}\\begin{array}{rcl}8x-8y&=&\\lambda x\\\\ -8x+8y&=&\\lambda y\\end{array} &\\end{cases} \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.129)$</p>\n",
    "\n",
    "<font color='forestgreen'>Caso 1 ‚Äì $\\lambda_{1}=16$:</font> Continuamos el desarrollo de la ecuaci√≥n (3.129):\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{u} \\in \\left( \\mathbb{R}^{2\\times 1} \\right)_{\\lambda =16}  &\\Longleftrightarrow &\\mathbf{u} =\\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  \\wedge \\begin{cases}\\begin{array}{rcl}8x-8y&=&16x\\\\ -8x+8y&=&16y\\end{array} &\\end{cases} \\\\ &\\Longrightarrow &\\mathbf{u} =\\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  \\wedge -x=y\\\\ &\\Longrightarrow &\\mathbf{u} =\\left( \\begin{matrix}x\\\\ -x\\end{matrix} \\right)  =x\\left( \\begin{matrix}1\\\\ -1\\end{matrix} \\right)  \\wedge x\\in \\mathbb{R} \\\\ &\\Longrightarrow &\\left( \\mathbb{R}^{2\\times 1} \\right)_{\\lambda =16}  =\\left< \\left\\{ \\left( \\begin{matrix}1\\\\ -1\\end{matrix} \\right)  \\right\\}  \\right>  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.130)$</p>\n",
    "\n",
    "<font color='forestgreen'>Caso 2 ‚Äì $\\lambda_{2}=0$:</font> Continuamos nuevamente el desarrollo de la ecuaci√≥n (3.129):\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{u} \\in \\left( \\mathbb{R}^{2\\times 1} \\right)_{\\lambda =0}  &\\Longleftrightarrow &\\mathbf{u} =\\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  \\wedge \\begin{cases}\\begin{array}{rcl}8x-8y&=&0x\\\\ -8x+8y&=&0y\\end{array} &\\end{cases} \\\\ &\\Longrightarrow &\\mathbf{u} =\\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  \\wedge x=y\\\\ &\\Longrightarrow &\\mathbf{u} =\\left( \\begin{matrix}x\\\\ x\\end{matrix} \\right)  =x\\left( \\begin{matrix}1\\\\ 1\\end{matrix} \\right)  \\wedge x\\in \\mathbb{R} \\\\ &\\Longrightarrow &\\left( \\mathbb{R}^{2\\times 1} \\right)_{\\lambda =0}  =\\left< \\left\\{ \\left( \\begin{matrix}1\\\\ 1\\end{matrix} \\right)  \\right\\}  \\right>  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.131)$</p>\n",
    "\n",
    "Definimos la base ortonormal de autovectores como\n",
    "\n",
    "$$\\alpha =\\left\\{ \\frac{\\left( \\begin{matrix}1\\\\ 1\\end{matrix} \\right)  }{\\left\\Vert \\left( \\begin{matrix}1\\\\ 1\\end{matrix} \\right)  \\right\\Vert  } ,\\frac{\\left( \\begin{matrix}1\\\\ -1\\end{matrix} \\right)  }{\\left\\Vert \\left( \\begin{matrix}1\\\\ -1\\end{matrix} \\right)  \\right\\Vert  } \\right\\}  =\\left\\{ \\frac{1}{\\sqrt{2} } \\left( \\begin{matrix}1\\\\ 1\\end{matrix} \\right)  ,\\frac{1}{\\sqrt{2} } \\left( \\begin{matrix}1\\\\ -1\\end{matrix} \\right)  \\right\\}  =\\left\\{ \\left( \\begin{array}{rr}\\displaystyle \\frac{\\sqrt{2} }{2} \\\\ \\displaystyle \\frac{\\sqrt{2} }{2} \\end{array} \\right)  ,\\left( \\begin{array}{rr}\\displaystyle \\frac{\\sqrt{2} }{2} \\\\ -\\displaystyle \\frac{\\sqrt{2} }{2} \\end{array} \\right)  \\right\\}$$\n",
    "<p style=\"text-align: right;\">$(3.132)$</p>\n",
    "\n",
    "Con esta base en consideraci√≥n, llevamos la ecuaci√≥n (3.127) a su forma reducida. Para ello, primero expresamos la matriz de la forma cuadr√°tica $\\mathbf{A}$ en su forma diagonal $q(x,y)=\\mathbf{P}\\mathbf{D}\\mathbf{P}^{-1}=\\left[ I\\right]^{\\mathbf{e} \\left( 2\\right)  }_{\\alpha }  \\left[ q\\right]^{\\alpha }_{\\alpha }  \\left[ I\\right]^{\\alpha }_{\\mathbf{e} \\left( 2\\right)  }$, donde $\\mathbf{D}$ es la matriz diagonal cuyos elementos no nulos son los autovalores de $\\mathbf{A}$. Es decir,\n",
    "\n",
    "$$\\mathbf{D}=\\left[ q\\right]^{\\alpha }_{\\alpha }  =\\left( \\begin{matrix}0&0\\\\ 0&16\\end{matrix} \\right)  $$\n",
    "<p style=\"text-align: right;\">$(3.133)$</p>\n",
    "\n",
    "Por otro lado, la matriz $\\mathbf{P}$ se construye a partir de la base ortonormal de autovectores previamente construida. Es decir,\n",
    "\n",
    "$$\\mathbf{P}=\\left[ I\\right]^{\\mathbf{e} \\left( 2\\right)  }_{\\alpha }  =\\left( \\begin{array}{rr}\\displaystyle \\frac{\\sqrt{2} }{2} &\\displaystyle \\frac{\\sqrt{2} }{2} \\\\ \\displaystyle \\frac{\\sqrt{2} }{2} &-\\displaystyle \\frac{\\sqrt{2} }{2} \\end{array} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.134)$</p>\n",
    "\n",
    "Donde $\\left[ I\\right]^{\\mathbf{e} \\left( 2\\right)  }_{\\alpha }  =\\left[ I\\right]^{\\alpha }_{\\mathbf{e} \\left( 2\\right)  }$. As√≠ que la matriz $\\mathbf{A}$ de la forma cuadr√°tica puede escribirse como\n",
    "\n",
    "$$\\left( \\begin{matrix}8&-8\\\\ -8&8\\end{matrix} \\right)  =\\left( \\begin{array}{rr}\\displaystyle \\frac{\\sqrt{2} }{2} &\\displaystyle \\frac{\\sqrt{2} }{2} \\\\ \\displaystyle \\frac{\\sqrt{2} }{2} &-\\displaystyle \\frac{\\sqrt{2} }{2} \\end{array} \\right)  \\left( \\begin{matrix}0&0\\\\ 0&16\\end{matrix} \\right)  \\left( \\begin{array}{rr}\\displaystyle \\frac{\\sqrt{2} }{2} &\\displaystyle \\frac{\\sqrt{2} }{2} \\\\ \\displaystyle \\frac{\\sqrt{2} }{2} &-\\displaystyle \\frac{\\sqrt{2} }{2} \\end{array} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.135)$</p>\n",
    "\n",
    "Por lo tanto, la c√≥nica $C$ ahora toma la forma\n",
    "\n",
    "$$\\begin{array}{rcl}C&:&\\left( x,y\\right)  \\left( \\begin{matrix}8&-8\\\\ -8&8\\end{matrix} \\right)  \\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  +\\left( 33\\sqrt{2} ,-31\\sqrt{2} \\right)  \\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  +70=0\\\\ \\Longrightarrow C&:&\\left( x,y\\right)  \\left( \\begin{array}{rr}\\displaystyle \\frac{\\sqrt{2} }{2} &\\displaystyle \\frac{\\sqrt{2} }{2} \\\\ \\displaystyle \\frac{\\sqrt{2} }{2} &-\\displaystyle \\frac{\\sqrt{2} }{2} \\end{array} \\right)  \\left( \\begin{matrix}0&0\\\\ 0&16\\end{matrix} \\right)  \\left( \\begin{array}{rr}\\displaystyle \\frac{\\sqrt{2} }{2} &\\displaystyle \\frac{\\sqrt{2} }{2} \\\\ \\displaystyle \\frac{\\sqrt{2} }{2} &-\\displaystyle \\frac{\\sqrt{2} }{2} \\end{array} \\right)  \\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  +\\left( 33\\sqrt{2} ,-31\\sqrt{2} \\right)  \\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  +70=0\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.136)$</p>\n",
    "\n",
    "Notemos que, para el caso de la forma lineal, podemos escribir\n",
    "\n",
    "$$\\left( \\begin{array}{rr}\\displaystyle \\frac{\\sqrt{2} }{2} &\\displaystyle \\frac{\\sqrt{2} }{2} \\\\ \\displaystyle \\frac{\\sqrt{2} }{2} &-\\displaystyle \\frac{\\sqrt{2} }{2} \\end{array} \\right)  \\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  =\\left( \\begin{array}{r}\\displaystyle \\frac{\\sqrt{2} }{2} \\left( x+y\\right)  \\\\ \\displaystyle \\frac{\\sqrt{2} }{2} \\left( x-y\\right)  \\end{array} \\right)  \\Longleftrightarrow \\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  =\\left( \\begin{array}{rr}\\displaystyle \\frac{\\sqrt{2} }{2} &\\displaystyle \\frac{\\sqrt{2} }{2} \\\\ \\displaystyle \\frac{\\sqrt{2} }{2} &-\\displaystyle \\frac{\\sqrt{2} }{2} \\end{array} \\right)  \\left( \\begin{array}{r}\\displaystyle \\frac{\\sqrt{2} }{2} \\left( x+y\\right)  \\\\ \\displaystyle \\frac{\\sqrt{2} }{2} \\left( x-y\\right)  \\end{array} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.137)$</p>\n",
    "\n",
    "Aplicando propiedades de la transposici√≥n de matrices y los resultados previos, vemos que la Ec. (3.125) se transforma en\n",
    "\n",
    "$$\\left( \\left( \\begin{array}{rr}\\displaystyle \\frac{\\sqrt{2} }{2} &\\displaystyle \\frac{\\sqrt{2} }{2} \\\\ \\displaystyle \\frac{\\sqrt{2} }{2} &-\\displaystyle \\frac{\\sqrt{2} }{2} \\end{array} \\right)  \\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  \\right)^{\\top }  \\left( \\begin{matrix}0&0\\\\ 0&16\\end{matrix} \\right)  \\left( \\begin{array}{rr}\\displaystyle \\frac{\\sqrt{2} }{2} &\\displaystyle \\frac{\\sqrt{2} }{2} \\\\ \\displaystyle \\frac{\\sqrt{2} }{2} &-\\displaystyle \\frac{\\sqrt{2} }{2} \\end{array} \\right)  \\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  +\\left( 33\\sqrt{2} ,-31\\sqrt{2} \\right)  \\left( \\begin{array}{rr}\\displaystyle \\frac{\\sqrt{2} }{2} &\\displaystyle \\frac{\\sqrt{2} }{2} \\\\ \\displaystyle \\frac{\\sqrt{2} }{2} &-\\displaystyle \\frac{\\sqrt{2} }{2} \\end{array} \\right)  \\left( \\begin{array}{r}\\displaystyle \\frac{\\sqrt{2} }{2} \\left( x+y\\right)  \\\\ \\displaystyle \\frac{\\sqrt{2} }{2} \\left( x-y\\right)  \\end{array} \\right)  +70=0$$\n",
    "<p style=\"text-align: right;\">$(3.138)$</p>\n",
    "\n",
    "Equivalentemente, desarrollando esta expresi√≥n, obtenemos\n",
    "\n",
    "$$\\left( \\displaystyle \\frac{\\sqrt{2} }{2} \\left( x+y\\right)  ,\\displaystyle \\frac{\\sqrt{2} }{2} \\left( x-y\\right)  \\right)  \\left( \\begin{matrix}0&0\\\\ 0&16\\end{matrix} \\right)  \\left( \\begin{array}{r}\\displaystyle \\frac{\\sqrt{2} }{2} \\left( x+y\\right)  \\\\ \\displaystyle \\frac{\\sqrt{2} }{2} \\left( x-y\\right)  \\end{array} \\right)  +\\left( 2,64\\right)  \\left( \\begin{array}{r}\\displaystyle \\frac{\\sqrt{2} }{2} \\left( x+y\\right)  \\\\ \\displaystyle \\frac{\\sqrt{2} }{2} \\left( x-y\\right)  \\end{array} \\right)  +70=0$$\n",
    "<p style=\"text-align: right;\">$(3.139)$</p>\n",
    "\n",
    "Multiplicando las matrices involucradas en la descomposici√≥n por formas anterior, transformamos la ecuaci√≥n (3.139) en\n",
    "\n",
    "$$16\\left( \\frac{\\sqrt{2} }{2} \\left( x-y\\right)  \\right)^{2}  +\\frac{2\\sqrt{2} }{2} \\left( x+y\\right)  +\\frac{64\\sqrt{2} }{2} \\left( x-y\\right)  +70=0$$\n",
    "<p style=\"text-align: right;\">$(3.140)$</p>\n",
    "\n",
    "Ahora generamos los cambios de variables respectivos. Notemos que el efecto principal de estos cambios, conforme lo visto en la [clase 1.2](https://github.com/rquezadac/udd_data_science_lectures/blob/main/PARTE%20I%20-%20Fundamentos%20matem%C3%A1ticos%20elementales/clase_1_2.ipynb) (y que hace patente la existencia del t√©rmino $bxy$ en la ecuaci√≥n general de segundo grado (3.109)), es la **rotaci√≥n** de los ejes del plano $\\mathbb{R}^{2}$ en una determinada direcci√≥n, donde las formas cuadr√°ticas asociadas nos entregan la informaci√≥n relativa a dicha rotaci√≥n. Los cambios de variables asociados a este ejercicio (o a cualquier otro caso general) tienen por objetivo expresar la c√≥nica en un nuevo sistema de coordenadas donde no existe tal rotaci√≥n, a fin de poder construir su correspondiente gr√°fico. Pongamos entonces $x_{1}=\\frac{\\sqrt{2} }{2} \\left( x+y\\right)  \\wedge y_{1}=\\frac{\\sqrt{2} }{2} \\left( x-y\\right)$. Reemplazando en la ecuaci√≥n (3.140) obtenemos\n",
    "\n",
    "$$16y^{2}_{1}+2x_{1}+64y_{1}+70=0\\  \\Longleftrightarrow \\  8y^{2}_{1}+x_{1}+32y_{1}+35=0$$\n",
    "<p style=\"text-align: right;\">$(3.141)$</p>\n",
    "\n",
    "Completando cuadrados, obtenemos\n",
    "\n",
    "$$\\begin{array}{lll}8y^{2}_{1}+x_{1}+32y_{1}+35=0&\\Longleftrightarrow &8\\left( y^{2}_{1}+4y_{1}\\right)  +x_{1}+35=0\\\\ &\\Longleftrightarrow &8\\left( y^{2}_{1}+4y_{1}+2^{2}-2^{2}\\right)  +x_{1}+35=0\\\\ &\\Longleftrightarrow &8\\left[ \\left( y_{1}+2\\right)^{2}  -4\\right]  +x_{1}-35=0\\\\ &\\Longleftrightarrow &8\\left( y_{1}+2\\right)^{2}  -32+x_{1}+35\\\\ &\\Longleftrightarrow &8\\left( y_{1}+2\\right)^{2}  =-x_{1}-3\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.142)$</p>\n",
    "\n",
    "La ecuaci√≥n de la c√≥nica (3.125) en el sistema $(x_{1},y_{1})$ es finalmente\n",
    "\n",
    "$$\\left( y_{1}-\\left( -2\\right)  \\right)^{2}  =-\\frac{1}{8} \\left( x_{1}-\\left( -3\\right)  \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.143)$</p>\n",
    "\n",
    "Esta ecuaci√≥n representa una **par√°bola** con foco en el punto $(x_{1},y_{1})=(-3,-2)$ y distancia focal $p=\\frac{1}{8}$. Dicha par√°bola est√° rotada en -45¬∫ con respecto al origen del sistema original $(x,y)$ (ya que $\\cos(-45¬∫)=\\sqrt{2}/2$). De esta manera, volviendo a las variables originales, tenemos que\n",
    "\n",
    "$$\\left( x_{1},y_{1}\\right)  =\\left( \\frac{\\sqrt{2} }{2} \\left( x+y\\right)  ,\\frac{\\sqrt{2} }{2} \\left( x-y\\right)  \\right)  \\  \\Longleftrightarrow \\  \\begin{array}{lll}x_{1}+y_{1}&=&\\sqrt{2} x\\\\ x_{1}-y_{1}&=&\\sqrt{2} y\\end{array} \\  \\Longrightarrow \\  \\begin{array}{l}x=\\frac{x_{1}+y_{1}}{\\sqrt{2} } \\\\ y=\\frac{x_{1}-y_{1}}{\\sqrt{2} } \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.144)$</p>\n",
    "\n",
    "Por lo que, en el sistema original, el foco est√° en el punto $F=\\left( \\frac{1}{\\sqrt{2} } \\left( -3-2\\right)  ,\\frac{1}{\\sqrt{2} } \\left( -3-\\left( -2\\right)  \\right)  \\right)  =\\left( -\\frac{5}{\\sqrt{2} } ,-\\frac{1}{\\sqrt{2} } \\right)$. La par√°bola en cuesti√≥n se ilustra en la Fig. (3.5).\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"figures/fig_3_5.png\" width=\"450\"></p>\n",
    "<p style=\"text-align: center;\">Fig. (3.5): Gr√°fico de la par√°bola descrita por la ecuaci√≥n (3.125)\n",
    "‚óºÔ∏é\n",
    "\n",
    "**Ejemplo 3.17:** Consideremos la ecuaci√≥n\n",
    "    \n",
    "$$5x^{2}+5y^{2}+2axy+8\\sqrt{2} x=0$$\n",
    "<p style=\"text-align: right;\">$(3.145)$</p>\n",
    "\n",
    "donde $a\\in \\mathbb{R}$ es un par√°metro. Vamos a encontrar todos los valores de $a$ para los cuales la ecuaci√≥n (3.145) describe una circunferencia, una elipse, una par√°bola y una hip√©rbola. Adem√°s, bosquejaremos el gr√°fico de la c√≥nica resultante para $a=3$.\n",
    "\n",
    "En efecto, los t√©rminos cuadr√°ticos de la ecuaci√≥n (3.145) pueden expresarse conforme la forma cuadr√°tica\n",
    "\n",
    "$$q\\left( x,y\\right)  =5x^{2}+5y^{2}+2axy=\\left( x,y\\right)  \\left( \\begin{matrix}5&a\\\\ a&5\\end{matrix} \\right)  \\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.146)$</p>\n",
    "\n",
    "Definimos la matriz asociada a $q$ como\n",
    "\n",
    "$$\\left[ q\\right]^{\\mathbf{e} \\left( 2\\right)  }_{\\mathbf{e} \\left( 2\\right)  }  =\\mathbf{A} =\\left( \\begin{matrix}5&a\\\\ a&5\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.147)$</p>\n",
    "\n",
    "Construimos el polinomio caracter√≠stico de $\\mathbf{A}$ como sigue,\n",
    "\n",
    "$$P_{\\mathbf{A} }\\left( \\lambda \\right)  =\\det \\left( \\mathbf{A} -\\lambda \\mathbf{I}_{2} \\right)  =\\det \\left( \\begin{matrix}5-\\lambda &a\\\\ a&5-\\lambda \\end{matrix} \\right)  =\\left( 5-\\lambda \\right)^{2}  -a^{2}=\\left( 5-\\lambda +a\\right)  \\left( 5-\\lambda -a\\right)$$\n",
    "<p style=\"text-align: right;\">$(3.148)$</p>\n",
    "\n",
    "Luego los autovalores de $\\mathbf{A}$ que se corresponden con las ra√≠ces del polinomio caracteristico $P_{\\mathbf{A} }\\left( \\lambda \\right)$ son $\\lambda_{1}=5+a$ y $\\lambda_{2}=5-a$. Notemos que, para $a=0$, la matriz $\\mathbf{A}$ es diagonal y el desarrollo de la c√≥nica es sencillo, ya que al reemplazar en la ecuaci√≥n (3.145), obtenemos\n",
    "\n",
    "$$x^{2}+y^{2}+\\frac{8\\sqrt{2} }{5} x=0$$\n",
    "<p style=\"text-align: right;\">$(3.149)$</p>\n",
    "\n",
    "Completando cuadrados,\n",
    "\n",
    "$$\\left( x+\\frac{4}{5} \\sqrt{2} \\right)^{2}  +y^{2}=\\frac{32}{25}$$\n",
    "<p style=\"text-align: right;\">$(3.150)$</p>\n",
    "\n",
    "Por lo tanto, para $a=0$, la ecuaci√≥n (3.145) representa una circunferencia con centro en el punto $\\left( 0,\\frac{4\\sqrt{2} }{5} \\right)$ y radio $r=\\frac{4\\sqrt{2} }{5}$.\n",
    "\n",
    "Prosigamos ahora con el desarrollo para $a\\neq 0$. Comenzaremos pues con la determinaci√≥n de los autoespacios correspondientes a los autovalores de $\\mathbf{A}$. Luego tenemos,\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{u} \\in \\left( \\mathbb{R}^{2\\times 1} \\right)_{\\lambda }  &\\Longleftrightarrow &\\mathbf{u} \\in \\mathbb{R}^{2\\times 1} \\wedge \\mathbf{A} \\mathbf{u} =\\lambda \\mathbf{u} \\\\ &\\Longleftrightarrow &\\mathbf{u} =\\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  \\wedge \\left( \\begin{matrix}5&a\\\\ a&5\\end{matrix} \\right)  \\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  =\\lambda \\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  \\\\ &\\Longleftrightarrow &\\mathbf{u} =\\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  \\wedge \\begin{cases}\\begin{array}{lll}5x+ay&=&\\lambda x\\\\ ax+5y&=&\\lambda y\\end{array} &\\end{cases} \\end{array} $$\n",
    "<p style=\"text-align: right;\">$(3.151)$</p>\n",
    "\n",
    "<font color='forestgreen'>Caso 1 ‚Äì $\\lambda_{1}=5-a$:</font> Continuamos con el desarrollo anterior,\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{u} \\in \\left( \\mathbb{R}^{2\\times 1} \\right)_{\\lambda =5-a}  &\\Longleftrightarrow &\\mathbf{u} =\\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  \\wedge \\begin{cases}\\begin{array}{lll}5x+ay&=&\\left( 5-a\\right)  x\\\\ ax+5y&=&\\left( 5-a\\right)  y\\end{array} &\\end{cases} \\\\ &\\Longrightarrow &\\mathbf{u} =\\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  \\wedge x=-y\\\\ &\\Longrightarrow &\\mathbf{u} =\\left( \\begin{matrix}x\\\\ -x\\end{matrix} \\right)  =x\\left( \\begin{matrix}1\\\\ -1\\end{matrix} \\right)  \\wedge x\\in \\mathbb{R} \\\\ &\\Longrightarrow &\\left( \\mathbb{R}^{2\\times 1} \\right)_{\\lambda =5-a}  =\\left< \\left\\{ \\left( \\begin{matrix}1\\\\ -1\\end{matrix} \\right)  \\right\\}  \\right>  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.152)$</p>\n",
    "\n",
    "<font color='forestgreen'>Caso 2 ‚Äì $\\lambda_{1}=5+a$:</font> Tenemos,\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{u} \\in \\left( \\mathbb{R}^{2\\times 1} \\right)_{\\lambda =5+a}  &\\Longleftrightarrow &\\mathbf{u} =\\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  \\wedge \\begin{cases}\\begin{array}{lll}5x+ay&=&\\left( 5+a\\right)  x\\\\ ax+5y&=&\\left( 5+a\\right)  y\\end{array} &\\end{cases} \\\\ &\\Longrightarrow &\\mathbf{u} =\\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  \\wedge x=y\\\\ &\\Longrightarrow &\\mathbf{u} =\\left( \\begin{matrix}x\\\\ x\\end{matrix} \\right)  =x\\left( \\begin{matrix}1\\\\ 1\\end{matrix} \\right)  \\wedge x\\in \\mathbb{R} \\\\ &\\Longrightarrow &\\left( \\mathbb{R}^{2\\times 1} \\right)_{\\lambda =5+a}  =\\left< \\left\\{ \\left( \\begin{matrix}1\\\\ 1\\end{matrix} \\right)  \\right\\}  \\right>  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.153)$</p>\n",
    "\n",
    "De esta manera, tenemos la siguiente base ortonormal de autovectores,\n",
    "\n",
    "$$\\alpha =\\left\\{ \\frac{\\left( \\begin{matrix}1\\\\ 1\\end{matrix} \\right)  }{\\left\\Vert \\left( \\begin{matrix}1\\\\ 1\\end{matrix} \\right)  \\right\\Vert  } ,\\frac{\\left( \\begin{matrix}1\\\\ -1\\end{matrix} \\right)  }{\\left\\Vert \\left( \\begin{matrix}1\\\\ -1\\end{matrix} \\right)  \\right\\Vert  } \\right\\}  =\\left\\{ \\frac{1}{\\sqrt{2} } \\left( \\begin{matrix}1\\\\ 1\\end{matrix} \\right)  ,\\frac{1}{\\sqrt{2} } \\left( \\begin{matrix}1\\\\ -1\\end{matrix} \\right)  \\right\\}  =\\left\\{ \\left( \\begin{array}{r}\\displaystyle \\frac{\\sqrt{2} }{2} \\\\ \\displaystyle \\frac{\\sqrt{2} }{2} \\end{array} \\right)  ,\\left( \\begin{array}{r}\\displaystyle \\frac{\\sqrt{2} }{2} \\\\ -\\displaystyle \\frac{\\sqrt{2} }{2} \\end{array} \\right)  \\right\\}$$\n",
    "<p style=\"text-align: right;\">$(3.154)$</p>\n",
    "\n",
    "Con esta base en consideraci√≥n, llevamos la c√≥nica $C$ a su forma reducida. Para ello, procedemos a diagonalizar la matriz de la forma quadr√°tica $\\mathbf{A}$ como sigue,\n",
    "\n",
    "$$\\left[ q\\right]^{\\mathbf{e} \\left( 2\\right)  }_{\\mathbf{e} \\left( 2\\right)  }  =\\mathbf{A} =\\mathbf{P} \\mathbf{D} \\mathbf{P}^{-1} =\\left[ I\\right]^{\\mathbf{e} \\left( 2\\right)  }_{\\alpha }  \\left[ q\\right]^{\\alpha }_{a}  \\left[ I\\right]^{\\alpha }_{\\mathbf{e} \\left( 2\\right)  }$$\n",
    "<p style=\"text-align: right;\">$(3.155)$</p>\n",
    "\n",
    "Donde,\n",
    "\n",
    "$$\\mathbf{P} =\\left[ I\\right]^{\\mathbf{e} \\left( 2\\right)  }_{\\alpha }  =\\left( \\begin{array}{rr}\\displaystyle \\frac{\\sqrt{2} }{2} &\\displaystyle \\frac{\\sqrt{2} }{2} \\\\ \\displaystyle \\frac{\\sqrt{2} }{2} &-\\displaystyle \\frac{\\sqrt{2} }{2} \\end{array} \\right)  \\wedge \\mathbf{A} =\\left[ q\\right]^{\\alpha }_{\\alpha }  =\\left( \\begin{matrix}5+a&0\\\\ 0&5-a\\end{matrix} \\right)  $$\n",
    "<p style=\"text-align: right;\">$(3.156)$</p>\n",
    "\n",
    "As√≠ que,\n",
    "\n",
    "$$\\left( \\begin{matrix}5&a\\\\ a&5\\end{matrix} \\right)  =\\left( \\begin{array}{rr}\\displaystyle \\frac{\\sqrt{2} }{2} &\\displaystyle \\frac{\\sqrt{2} }{2} \\\\ \\displaystyle \\frac{\\sqrt{2} }{2} &-\\displaystyle \\frac{\\sqrt{2} }{2} \\end{array} \\right)  \\left( \\begin{matrix}5+a&0\\\\ 0&5-a\\end{matrix} \\right)  \\left( \\begin{array}{rr}\\displaystyle \\frac{\\sqrt{2} }{2} &\\displaystyle \\frac{\\sqrt{2} }{2} \\\\ \\displaystyle \\frac{\\sqrt{2} }{2} &-\\displaystyle \\frac{\\sqrt{2} }{2} \\end{array} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.157)$</p>\n",
    "\n",
    "Reemplazando en la ecuaci√≥n (3.145),\n",
    "\n",
    "$$\\begin{array}{ll}&\\left( x,y\\right)  \\left( \\begin{matrix}5&a\\\\ a&5\\end{matrix} \\right)  \\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  +8\\sqrt{2} x=0\\\\ \\Longrightarrow &\\left( x,y\\right)  \\left( \\begin{array}{rr}\\displaystyle \\frac{\\sqrt{2} }{2} &\\displaystyle \\frac{\\sqrt{2} }{2} \\\\ \\displaystyle \\frac{\\sqrt{2} }{2} &-\\displaystyle \\frac{\\sqrt{2} }{2} \\end{array} \\right)  \\left( \\begin{matrix}5+a&0\\\\ 0&5-a\\end{matrix} \\right)  \\left( \\begin{array}{rr}\\displaystyle \\frac{\\sqrt{2} }{2} &\\displaystyle \\frac{\\sqrt{2} }{2} \\\\ \\displaystyle \\frac{\\sqrt{2} }{2} &-\\displaystyle \\frac{\\sqrt{2} }{2} \\end{array} \\right)  \\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  +8\\sqrt{2} x=0\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.158)$</p>\n",
    "\n",
    "Hacemos ahora el cambio de variables\n",
    "\n",
    "$$\\left( \\begin{matrix}u\\\\ v\\end{matrix} \\right)  =\\left[ I\\right]^{{}\\alpha }_{\\mathbf{e} \\left( 2\\right)  }  \\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.159)$</p>\n",
    "\n",
    "Luego tenemos,\n",
    "\n",
    "$$\\left( x,y\\right)  \\mathbf{A} \\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  =\\left( x,y\\right)  \\left[ I\\right]^{\\mathbf{e} \\left( 2\\right)  }_{\\alpha }  \\left[ q\\right]^{\\alpha }_{\\alpha }  \\left[ I\\right]^{\\alpha }_{\\mathbf{e} \\left( 2\\right)  }  \\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  =\\left( \\left[ I\\right]^{\\mathbf{e} \\left( 2\\right)  }_{\\alpha }  \\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  \\right)^{\\top }  \\left[ q\\right]^{\\alpha }_{\\alpha }  \\left[ I\\right]^{\\alpha }_{\\mathbf{e} \\left( 2\\right)  }  \\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  =\\left( \\begin{matrix}u\\\\ v\\end{matrix} \\right)^{\\top }  \\left[ q\\right]^{\\alpha }_{\\alpha }  \\left( \\begin{matrix}u\\\\ v\\end{matrix} \\right)  =\\left( u,v\\right)  \\left[ q\\right]^{\\alpha }_{\\alpha }  \\left( \\begin{matrix}u\\\\ v\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.160)$</p>\n",
    "\n",
    "As√≠ que,\n",
    "\n",
    "$$\\left( u,v\\right)  \\left[ q\\right]^{\\alpha }_{\\alpha }  \\left( \\begin{matrix}u\\\\ v\\end{matrix} \\right)  =\\left( u,v\\right)  \\left( \\begin{matrix}5+a&0\\\\ 0&5-a\\end{matrix} \\right)  \\left( \\begin{matrix}u\\\\ v\\end{matrix} \\right)  =\\left( u\\left( 5+a\\right)  ,v\\left( 5-a\\right)  \\right)  =u^{2}\\left( 5+a\\right)  +v^{2}\\left( 5-a\\right)$$\n",
    "<p style=\"text-align: right;\">$(3.161)$</p>\n",
    "\n",
    "Por lo tanto, en el sistema $(u,v)$, los t√©rminos cuadr√°ticos de la ecuaci√≥n (3.145) se transforman en $q(u,v)=u^{2}\\left( 5+a\\right)  +v^{2}\\left( 5-a\\right)$. Para transformar el resto de la ecuaci√≥n, notamos que\n",
    "\n",
    "$$\\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  =\\left[ I\\right]^{\\mathbf{e} \\left( 2\\right)  }_{\\alpha }  \\left( \\begin{matrix}u\\\\ v\\end{matrix} \\right)  =\\left( \\begin{array}{rr}\\displaystyle \\frac{\\sqrt{2} }{2} &\\displaystyle \\frac{\\sqrt{2} }{2} \\\\ \\displaystyle \\frac{\\sqrt{2} }{2} &-\\displaystyle \\frac{\\sqrt{2} }{2} \\end{array} \\right)  \\left( \\begin{matrix}u\\\\ v\\end{matrix} \\right)  =\\displaystyle \\frac{\\sqrt{2} }{2} \\left( \\begin{matrix}1&1\\\\ 1&-1\\end{matrix} \\right)  \\left( \\begin{matrix}u\\\\ v\\end{matrix} \\right)  =\\displaystyle \\frac{\\sqrt{2} }{2} \\left( \\begin{matrix}u+v\\\\ u-v\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.162)$</p>\n",
    "\n",
    "As√≠ que $x=\\frac{\\sqrt{2} }{2} \\left( u+v\\right)  \\wedge y=\\frac{\\sqrt{2} }{2} \\left( u-v\\right)$. Reemplazando en el t√©rmino de primer grado de la ecuaci√≥n (3.145), obtenemos\n",
    "\n",
    "$$8\\sqrt{2} x=8\\sqrt{2} \\frac{\\sqrt{2} }{2} \\left( u+v\\right)  =8\\left( u+v\\right)$$\n",
    "<p style=\"text-align: right;\">$(3.163)$</p>\n",
    "\n",
    "Por lo tanto, en el sistema $(u,v)$, la c√≥nica se expresa como\n",
    "\n",
    "$$\\left( 5+a\\right)  u^{2}+\\left( 5-a\\right)  v^{2}+8u+8v=0$$\n",
    "<p style=\"text-align: right;\">$(3.164)$</p>\n",
    "\n",
    "Tenemos entonces varios sub-casos que debemos considerar para cada valor que puede tomar el par√°metro $a$ (recordemos que ya revisamos lo que ocurre cuando $a=0$).\n",
    "\n",
    "**(1)** Si $a=5$, la ecuaci√≥n (3.164) se reduce a\n",
    "\n",
    "$$10u^{2}+8u+8v=0$$\n",
    "<p style=\"text-align: right;\">$(3.165)$</p>\n",
    "\n",
    "Completando cuadrados,\n",
    "\n",
    "$$\\begin{array}{lll}10u^{2}+8u+8v=0&\\Longleftrightarrow &u^{2}+\\displaystyle \\frac{4}{5} u+\\displaystyle \\frac{4}{5} v=0\\\\ &\\Longleftrightarrow &u^{2}+\\frac{4}{5} u+\\left( \\displaystyle \\frac{2}{5} \\right)^{2}  +\\displaystyle \\frac{4}{5} v-\\left( \\displaystyle \\frac{2}{5} \\right)^{2}  =0\\\\ &\\Longleftrightarrow &\\left( u+\\displaystyle \\frac{2}{5} \\right)^{2}  =-\\displaystyle \\frac{4}{5} \\left( v-\\displaystyle \\frac{2}{5} \\right)  \\\\ &\\Longleftrightarrow &\\left( u-\\left( -\\displaystyle \\frac{2}{5} \\right)  \\right)^{2}  =-\\displaystyle \\frac{4}{5} \\left( v-\\frac{2}{5} \\right)  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.166)$</p>\n",
    "\n",
    "Luego, para $a=5$, la ecuaci√≥n (3.164) describe, en el sistema $(u,v)$, una par√°bola con foco en $F=(-\\frac{2}{5},\\frac{2}{5})$.\n",
    "\n",
    "**(2)** Si $a=-5$, la ecuaci√≥n (3.164) se reduce a\n",
    "\n",
    "$$-10u^{2}+8u+8v=0$$\n",
    "<p style=\"text-align: right;\">$(3.168)$</p>\n",
    "\n",
    "De manera an√°loga al caso $a=5$, completando cuadrados, obtenemos que\n",
    "\n",
    "$$\\left( v-\\frac{2}{5} \\right)^{2}  =\\frac{4}{5} \\left( u-\\left( -\\frac{2}{5} \\right)  \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.169)$</p>\n",
    "\n",
    "Luego, para $a=-5$, la ecuaci√≥n (3.164) describe, en el sistema $(u,v)$, una par√°bola con foco en $F=(\\frac{2}{5},-\\frac{2}{5})$, con orientaci√≥n opuesta al caso $a=5$.\n",
    "\n",
    "**(3)** Si $-5<a<5$, entonces $5+a>0$ y $5-a>0$. Por lo tanto, desarrollamos la ecuaci√≥n (3.164) completando cuadrados como sigue,\n",
    "\n",
    "$$\\begin{array}{lll}\\left( 5+a\\right)  u^{2}+\\left( 5-a\\right)  v^{2}+8u+8v=0&\\Longleftrightarrow &\\left( 5+a\\right)  \\left( u^{2}+\\displaystyle \\frac{8}{5+a} u\\right)  +\\left( 5-a\\right)  \\left( v^{2}+\\displaystyle \\frac{8}{5-a} v\\right)  =0\\\\ &\\Longleftrightarrow &\\left( 5+a\\right)  \\left( u+\\displaystyle \\frac{4}{5+a} \\right)^{2}  +\\left( 5-a\\right)  \\left( v+\\displaystyle \\frac{4}{5-a} \\right)^{2}  =16\\left( \\displaystyle \\frac{1}{5+a} +\\displaystyle \\frac{1}{5-a} \\right)  \\\\ &\\Longleftrightarrow &\\underbrace{\\left( 5+a\\right)  }_{>0} \\left( u+\\displaystyle \\frac{4}{5+a} \\right)^{2}  +\\underbrace{\\left( 5-a\\right)  }_{>0} \\left( v+\\displaystyle \\frac{4}{5-a} \\right)^{2}  =\\underbrace{\\displaystyle \\frac{160}{25-a^{2}} }_{>0} \\\\ &\\Longleftrightarrow &\\displaystyle \\frac{\\left( 5+a\\right)^{2}  \\left( 5-a\\right)  }{160} \\left( u+\\displaystyle \\frac{4}{5+a} \\right)^{2}  +\\displaystyle \\frac{\\left( 5+a\\right)^{2}  \\left( 5-a\\right)  }{160} \\left( v+\\displaystyle \\frac{4}{5-a} \\right)^{2}  =1\\\\ &\\Longleftrightarrow &\\displaystyle \\frac{\\left( u+\\frac{4}{5+a} \\right)^{2}  }{\\frac{160}{\\left( 5+a\\right)^{2}  \\left( 5-a\\right)  } } +\\frac{\\left( v+\\frac{4}{5-a} \\right)^{2}  }{\\frac{160}{\\left( 5-a\\right)^{2}  \\left( 5+a\\right)  } } =1\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.170)$</p>\n",
    "\n",
    "Por lo tanto, para $-5<a<5$, en el sistema $(u,v)$, la ecuaci√≥n (3.164) describe una elipse con centro en $P=\\left( -\\frac{4}{5+a} ,-\\frac{4}{5-a} \\right)$ y semiejes $\\ell_{1} =\\frac{4\\sqrt{10} }{\\left( 5+a\\right)  \\sqrt{5-a} } \\wedge \\ell_{2} =\\frac{4\\sqrt{10} }{\\left( 5-a\\right)  \\sqrt{5+a} }$.\n",
    "\n",
    "**(4)** Si $a<-5$, entonces uno de los autovalores de la matriz $\\mathbf{A}$ asociada a la forma cuadr√°tica $q(x,y)$ es positivo, mientras que el otro es negativo. Por lo tanto, la √∫nica diferencia con el caso (3) es que los sumandos involucrados en la ecuaci√≥n (3.170) tienen signos opuestos. Por lo tanto, podemos escribir la ecuaci√≥n de la c√≥nica, en el sistema $(u,v)$, como\n",
    "\n",
    "$$\\frac{\\left( u+\\frac{4}{5+a} \\right)^{2}  }{\\frac{160}{\\left( 5+a\\right)^{2}  \\left( 5-a\\right)  } } -\\frac{\\left( v+\\frac{4}{5-a} \\right)^{2}  }{\\frac{160}{\\left( 5-a\\right)^{2}  \\left( 5+a\\right)  } } =1$$\n",
    "<p style=\"text-align: right;\">$(3.171)$</p>\n",
    "\n",
    "De este modo, para $a<-5$, la c√≥nica resulta ser una hip√©rbola con centro en $P=\\left( -\\frac{4}{5+a} ,-\\frac{4}{5-a} \\right)$ y semiejes $\\ell_{1} =\\frac{4\\sqrt{10} }{\\left( 5+a\\right)  \\sqrt{5-a} } \\wedge \\ell_{2} =\\frac{4\\sqrt{10} }{\\left( 5-a\\right)  \\sqrt{5+a} }$.\n",
    "\n",
    "Para $a=3$, sabemos, por la ecuaci√≥n (3.170), que la c√≥nica es una elipse. Reemplazando en dicha ecuaci√≥n $a=3$, obtenemos\n",
    "\n",
    "$$\\frac{\\left( u+\\frac{1}{2} \\right)^{2}  }{160/\\left( 64\\cdot 2\\right)  } +\\frac{\\left( v+2\\right)^{2}  }{160/\\left( 4\\cdot 8\\right)  } =1\\  \\Longleftrightarrow \\  \\frac{\\left( u+\\frac{1}{2} \\right)^{2}  }{5/4} +\\frac{\\left( v+2\\right)^{2}  }{5} =1$$\n",
    "<p style=\"text-align: right;\">$(3.172)$</p>\n",
    "\n",
    "La ecuaci√≥n anterior describe, en el sistema $(u,v)$, una elipse con centro en el punto $P=\\left( -\\frac{1}{2} ,-2\\right)$, con semiejes $\\ell_{1} =\\frac{\\sqrt{5} }{2} \\wedge \\ell_{2} =\\sqrt{5}$. La elipse se encuentra rotada 45¬∫ con respecto al origen del sistema original $(x,y)$. Por lo tanto, en dicho sistema, el centro de la elipse se localiza en el punto $P=\\left( \\sqrt{2} \\left( -\\frac{1}{2} -2\\right)  ,\\sqrt{2} \\left( -\\frac{1}{2} +2\\right)  \\right)  =\\left( -\\frac{5}{2} \\sqrt{2} ,\\frac{3}{2} \\sqrt{2} \\right)$. El bosquejo de esta elipse se observa en la Fig. (3.6). ‚óºÔ∏é\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"figures/fig_3_6.png\" width=\"450\"></p>\n",
    "<p style=\"text-align: center;\">Fig. (3.6): Gr√°fico de la elipse descrita por la ecuaci√≥n (3.172)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0828a320",
   "metadata": {},
   "source": [
    "## Descomposici√≥n de Cholesky.\n",
    "Como ya hemos revisado en las subsecciones anteriores, existen varias formas de descomponer una matriz. Puntualmente, la diagonalizaci√≥n y la descomposici√≥n por formas nos han permitido reformular matrices que cumplen con ciertas condiciones en t√©rminos de productos de tres matrices.\n",
    "\n",
    "En esta subsecci√≥n, vamos a estudiar una nueva descomposici√≥n conocida como **factorizaci√≥n de Cholesky**, la que nos permite descomponer una matriz $\\mathbf{A}$ que cumple igualmente algunas condiciones en el producto $\\mathbf{A}=\\mathbf{L}\\mathbf{L}^{\\top}$, donde $\\mathbf{L}$ es una matriz triangular inferior cuya diagonal contiene n√∫meros positivos. Sin embargo, para ello, es necesario que las matrices que queremos descomponer sean **definidas positivas**. Este concepto lo revisamos previamente, pero lo refinaremos a fin de considerar matrices definidas sobre cualquier cuerpo ($\\mathbb{R}$ o $\\mathbb{C}$).\n",
    "\n",
    "**<font color='blue'>Definici√≥n 3.18 ‚Äì Matriz definida positiva (otra vez...):</font>** Sea $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$ una matriz herm√≠tica. Diremos que $\\mathbf{A}$ es una **matriz definida positiva** si cumple con una (y, por tanto, con todas) de las siguientes formulaciones equivalentes:\n",
    "\n",
    "- **(F1):** Para todos los vectores no nulos $\\mathbf{u}\\in \\mathbb{K}^{n}$, se tiene que $\\mathbf{u}^{\\ast}\\mathbf{A}\\mathbf{u}$, donde $\\mathbf{u}^{\\ast}$ es la matriz transpuesta conjugada de $\\mathbf{u}$.\n",
    "- **(F2):** Todos los autovalores de $\\mathbf{A}$ son positivos.\n",
    "- **(F3):** La funci√≥n $\\left< \\mathbf{u} ,\\mathbf{v} \\right>  =\\mathbf{v}^{\\ast } \\mathbf{A} \\mathbf{u}$ define un producto interno en $\\mathbb{K}^{n}$.\n",
    "\n",
    "Observamos, de las condiciones anteriores, que si $\\mathbf{A}$ es definida positiva, entonces $\\det(\\mathbf{A})>0$ y su inversa $\\mathbf{A}^{-1}$ es tambi√©n definida positiva.\n",
    "\n",
    "Estamos pues en condiciones de formular el siguiente teorema.\n",
    "\n",
    "**<font color='crimson'>Teorema 3.14 ‚Äì Descomposici√≥n de Cholesky:</font>** *Sea $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$ una matriz sim√©trica y definida positiva. Entonces $\\mathbf{A}$ puede ser expresada como $\\mathbf{A}=\\mathbf{L}\\mathbf{L}^{\\top}$, donde $\\mathbf{L}$ es una matriz triangular inferior con elementos positivos en su diagonal principal. Es decir,*\n",
    "\n",
    "$$\\mathbf{A} =\\left( \\begin{matrix}a_{11}&a_{12}&\\cdots &a_{1n}\\\\ a_{21}&a_{22}&\\cdots &a_{2n}\\\\ \\vdots &\\vdots &\\ddots &\\vdots \\\\ a_{n1}&a_{n2}&\\cdots &a_{nn}\\end{matrix} \\right)  =\\left( \\begin{matrix}l_{11}&0&\\cdots &0\\\\ l_{21}&l_{22}&\\cdots &0\\\\ \\vdots &\\vdots &\\ddots &\\vdots \\\\ l_{n1}&l_{n2}&\\cdots &l_{nn}\\end{matrix} \\right)  \\left( \\begin{matrix}l_{11}&l_{21}&\\cdots &l_{n1}\\\\ 0&l_{22}&\\cdots &l_{n2}\\\\ \\vdots &\\vdots &\\ddots &\\vdots \\\\ 0&0&\\cdots &l_{nn}\\end{matrix} \\right)  =\\mathbf{L} \\mathbf{L}^{\\top }$$\n",
    "<p style=\"text-align: right;\">$(3.173)$</p>\n",
    "\n",
    "*La matriz $\\mathbf{L}$ se denomina **factor de Cholesky** de $\\mathbf{A}$, y es √∫nico.* ‚óÜ\n",
    "\n",
    "**Ejemplo 3.18:** Consideremos una matriz sim√©trica y definida positiva $\\mathbf{A}\\in \\mathbb{R}^{3\\times 3}$, definida como $\\mathbf{A} =\\left\\{ a_{ij}\\right\\}$, tal que\n",
    "\n",
    "$$\\mathbf{A} =\\left( \\begin{matrix}a_{11}&a_{12}&a_{13}\\\\ a_{21}&a_{22}&a_{23}\\\\ a_{31}&a_{32}&a_{33}\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.174)$</p>\n",
    "\n",
    "Vamos a buscar una f√≥rmula para determinar expl√≠citamente los elementos del factor de Cholesky $\\mathbf{L}$ de $\\mathbf{A}$. En efecto, $\\mathbf{A}=\\mathbf{L}\\mathbf{L}^{\\top}$, donde\n",
    "\n",
    "$$\\mathbf{L} \\mathbf{L}^{\\top } =\\left( \\begin{matrix}l_{11}&0&0\\\\ l_{21}&l_{22}&0\\\\ l_{31}&l_{32}&l_{33}\\end{matrix} \\right)  \\left( \\begin{matrix}l_{11}&l_{21}&l_{32}\\\\ 0&l_{22}&l_{32}\\\\ 0&0&l_{33}\\end{matrix} \\right)  =\\left( \\begin{matrix}l^{2}_{11}&l_{21}l_{11}&l_{31}l_{11}\\\\ l_{21}l_{31}&l^{2}_{21}+l^{2}_{22}&l_{31}l_{21}+l_{32}l_{22}\\\\ l_{31}l_{11}&l_{31}l_{21}+l_{32}l_{22}&l^{2}_{31}+l^{2}_{32}+l^{2}_{33}\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.175)$</p>\n",
    "\n",
    "Comparando el lado derecho de la ecuaci√≥n (3.174) con el lado derecho de la ecuaci√≥n (3.175), podemos observar que existe un sencillo patr√≥n en los elementos de la diagonal principal de $\\mathbf{L}$. En efecto,\n",
    "\n",
    "$$l_{11}=\\sqrt{a_{11}} \\  ;\\  l_{22}=\\sqrt{a_{22}-l^{2}_{21}} \\  ;\\  l_{33}=\\sqrt{a_{33}-\\left( l^{2}_{31}+l^{2}_{32}\\right)  }$$\n",
    "<p style=\"text-align: right;\">$(3.176)$</p>\n",
    "\n",
    "Similarmente, para los elementos ubicados debajo de la diagonal principal de $\\mathbf{L}$ ($l_{ij}$, donde $i>j$), tambi√©n se observa un patr√≥n,\n",
    "\n",
    "$$l_{21}=\\frac{1}{l_{11}} a_{21}\\  ;\\  l_{31}=\\frac{1}{l_{11}} a_{31}\\  ;\\  l_{32}=\\frac{1}{l_{22}} \\left( a_{32}-l_{31}l_{21}\\right)$$\n",
    "<p style=\"text-align: right;\">$(3.177)$</p>\n",
    "\n",
    "De esta manera, en t√©rminos m√°s generales, para una matriz $\\mathbf{A}\\in \\mathbb{R}^{n\\times n}$, podemos probar (por inducci√≥n) que:\n",
    "\n",
    "- Para los elementos de la diagonal principal: $l_{ii}=a_{ii}-\\sum^{i-1}_{k=1} l^{2}_{ik}$.\n",
    "- Para el resto de los elementos: $l_{ij}=\\frac{1}{l_{jj}} \\left( a_{ij}-\\sum^{j-1}_{k=1} l_{ik}l_{jk}\\right)  ;\\forall i>j$.\n",
    "‚óºÔ∏é\n",
    "\n",
    "**Ejemplo 3.19:** En <font color='purple'>Numpy</font> es posible obtener una descomposici√≥n de Cholesky f√°cilmente por medio de la funci√≥n `cholesky()`, cuya dependencia es igualmente el m√≥dulo `numpy.linalg`. Esta funci√≥n simplemente requiere, como argumento, un arreglo bidimensional que represente a una matriz definida positiva. Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e361f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos nuestra matriz A.\n",
    "A = np.array([\n",
    "    [2, 2, 1],\n",
    "    [2, 3, 0],\n",
    "    [1, 0, 2],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48b5291d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 2, 1],\n",
       "       [2, 3, 0],\n",
       "       [1, 0, 2]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mostramos en pantalla esta matriz.\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c210d1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos su factor de Cholesky.\n",
    "L = linalg.cholesky(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51bf0431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.41421356,  0.        ,  0.        ],\n",
       "       [ 1.41421356,  1.        ,  0.        ],\n",
       "       [ 0.70710678, -1.        ,  0.70710678]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mostramos la matriz L en pantalla.\n",
    "L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e5a765",
   "metadata": {},
   "source": [
    "Es posible verificar que la descompisici√≥n obtenida efectivamente genera la matriz original `A`, realizando la operaci√≥n $\\mathbf{L}\\mathbf{L}^{\\top}$. En efecto,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85bda3ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verificamos que la descomposici√≥n efectivamente genera la matriz A.\n",
    "np.allclose(A, L @ L.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa103af3",
   "metadata": {},
   "source": [
    "Por supuesto, la factorizaci√≥n de Cholesky exige que las matrices a descomponer sean definidas positivas. En <font color='purple'>Numpy</font> esta no es una excepci√≥n, y se levantar√° un error de tipo `LinAlgError` si esta condici√≥n no se cumple al intentar implementar una decomposici√≥n de Cholesky:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e816fb25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3,  2,  1,  1],\n",
       "       [-2,  5, -5, -2],\n",
       "       [-5, -2,  5, -2],\n",
       "       [ 4, -1,  2, -1]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definimos una matriz arbitraria.\n",
    "B = rng.integers(low=-6, high=6, size=(4, 4))\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6bfc5d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix is not positive definite\n"
     ]
    }
   ],
   "source": [
    "# Si intentamos calcular su descomposici√≥n de Cholesky, se levantar√° un error, ya que no es\n",
    "# definida positiva.\n",
    "try:\n",
    "    L = linalg.cholesky(B)\n",
    "except linalg.LinAlgError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1b4a83",
   "metadata": {},
   "source": [
    "‚óºÔ∏é\n",
    "\n",
    "La descomposici√≥n de Cholesky es una herramienta importante en el c√°lculo num√©rico subyacente a los algoritmos de aprendizaje. Las matrices sim√©tricas y definidas positivas, con frecuencia, requieren de intensivas manipulaciones para llegar a valores de inter√©s en muchos problemas estad√≠sticos. Por ejemplo, la *matriz de covarianza* asociada a un *vector de variables aleatorias distribuidas normalmente* cumple con estas condiciones. La descomposici√≥n de Cholesky de esta matriz de covarianza nos permite generar muestreos desde una distribuci√≥n *Gaussiana*. Tambi√©n nos permite construir transformaciones lineales de variables aleatorias, lo que se explota enormemente al calcular gradientes en modelos estoc√°sticos de gran profundidad, tales como los *auto-enconders* de tipo variacionales (Jimenez Rezende et al., 2014; Kingma & Weilling., 2014). \n",
    "\n",
    "La descomposici√≥n de Cholesky tambi√©n nos permite calcular determinantes de manera eficiente. Dada la factorizaci√≥n $\\mathbf{A}=\\mathbf{L}\\mathbf{L}^{\\top}$, sabemos que $\\det(\\mathbf{A})=\\det(\\mathbf{L}) \\det(\\mathbf{L}^{\\top})=\\det(\\mathbf{L})^{2}$. Ya que $\\mathbf{L}$ es una matriz triangular, el determinante de $\\mathbf{L}$ es simplemente el producto de los elementos de la diagonal principal. Por esta raz√≥n, muchos paquetes computacionales (y, en varios casos, librer√≠as de Python) utilizan la descomposici√≥n de Cholesky para facilitar c√°lculos que involucran matrices (siempre que sean definidas positivas)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61825a1",
   "metadata": {},
   "source": [
    "## Descomposici√≥n QR.\n",
    "Otro tipo de descomposici√≥n muy com√∫n en √°lgebra lineal y, puntualmente, en machine learning (sobretodo por el aumento en la eficiencia de ciertos c√°lculos matriciales), corresponde a la **descomposici√≥n QR**. Tal descomposici√≥n es v√°lida para cualquier matriz $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$, y es tal que $\\mathbf{A}=\\mathbf{Q}\\mathbf{R}$, donde $\\mathbf{Q}$ es una matriz ortogonal o unitaria (recordemos que toda matriz $\\mathbf{Q}\\in \\mathbb{K}^{n\\times n}$ que satisface la expresi√≥n $\\mathbf{Q}^{\\top}\\mathbf{Q}=\\mathbf{I}_{n}$ se denomina ortogonal o unitaria) y $\\mathbf{R}$ es una matriz triangular superior. Si $\\mathbf{A}$ es, adem√°s, no singular (y, por tanto, invertible), entonces la descomposici√≥n QR es √∫nica siempre que los elementos de la diagonal principal de $\\mathbf{R}$ sean todos positivos.\n",
    "\n",
    "La construcci√≥n de la factorizaci√≥n QR puede hacerse de varias formas, siendo la m√°s usual el uso de proyecciones ortogonales y del proceso de ortogonalizaci√≥n de Gram-Schmidt. Para ello, consideremos este √∫ltimo proceso (que fue desarollado en el teorema (2.2)) aplicado a las columnas de una matriz no singular, digamos $\\mathbf{A}=(\\mathbf{a}_{1},...,\\mathbf{a}_{n})$, donde $\\mathbf{a}_{j} =\\left\\{ a_{ij}\\right\\}$ para $i=1,...,n$, y definimos el producto interno usual $\\left< \\mathbf{v} ,\\mathbf{w} \\right>  =\\mathbf{v}^{\\ast } \\mathbf{w}$. Definimos adem√°s la proyecci√≥n ortogonal\n",
    "\n",
    "$$\\pi_{\\mathbf{u} } \\left( \\mathbf{a} \\right)  =\\frac{\\left< \\mathbf{u} ,\\mathbf{a} \\right>  }{\\left< \\mathbf{u} ,\\mathbf{u} \\right>  } \\mathbf{u} =\\frac{\\left< \\mathbf{u} ,\\mathbf{a} \\right>  }{\\left\\Vert \\mathbf{u} \\right\\Vert^{2}  } \\mathbf{u}$$\n",
    "<p style=\"text-align: right;\">$(3.178)$</p>\n",
    "\n",
    "Entonces, usando el m√©todo de Gram-Schmidt, obtenemos\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{u}_{1} &=&\\mathbf{a}_{1} \\  ;\\  \\mathbf{e}_{1} =\\frac{\\mathbf{u}_{1} }{\\left\\Vert \\mathbf{u}_{1} \\right\\Vert^{2}  } \\\\ \\mathbf{u}_{2} &=&\\mathbf{a}_{2} -\\pi_{\\mathbf{u}_{1} } \\left( \\mathbf{a}_{2} \\right)  \\  ;\\  \\mathbf{e}_{2} =\\frac{\\mathbf{u}_{2} }{\\left\\Vert \\mathbf{u}_{2} \\right\\Vert^{2}  } \\\\ \\mathbf{u}_{3} &=&\\mathbf{a}_{3} -\\pi_{\\mathbf{u}_{1} } \\left( \\mathbf{a}_{3} \\right)  -\\pi_{\\mathbf{u}_{2} } \\left( \\mathbf{a}_{3} \\right)  \\  ;\\  \\mathbf{e}_{3} =\\frac{\\mathbf{u}_{3} }{\\left\\Vert \\mathbf{u}_{3} \\right\\Vert^{2}  } \\\\ \\vdots &\\vdots &\\vdots \\\\ \\mathbf{u}_{k} &=&\\mathbf{a}_{k} -\\sum^{k-1}_{j=1} \\pi_{\\mathbf{u}_{j} } \\left( \\mathbf{a}_{k} \\right)  \\  ;\\  \\mathbf{e}_{k} =\\frac{\\mathbf{u}_{k} }{\\left\\Vert \\mathbf{u}_{k} \\right\\Vert^{2}  } \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.179)$</p>\n",
    "\n",
    "Podemos expresar las columnas $\\mathbf{a}_{k}$ sobre nuestra nueva base ortonormal, reci√©n calculada, como\n",
    "\n",
    "$$\\mathbf{R} =\\left( \\begin{array}{rrrrr}\\left< \\mathbf{e}_{1} ,\\mathbf{a}_{1} \\right>  &\\left< \\mathbf{e}_{1} ,\\mathbf{a}_{2} \\right>  &\\left< \\mathbf{e}_{1} ,\\mathbf{a}_{3} \\right>  &\\cdots &\\left< \\mathbf{e}_{1} ,\\mathbf{a}_{n} \\right>  \\\\ 0&\\left< \\mathbf{e}_{2} ,\\mathbf{a}_{2} \\right>  &\\left< \\mathbf{e}_{2} ,\\mathbf{a}_{3} \\right>  &\\cdots &\\left< \\mathbf{e}_{2} ,\\mathbf{a}_{n} \\right>  \\\\ 0&0&\\left< \\mathbf{e}_{3} ,\\mathbf{a}_{3} \\right>  &\\cdots &\\left< \\mathbf{e}_{3} ,\\mathbf{a}_{n} \\right>  \\\\ \\vdots &\\vdots &\\vdots &\\ddots &\\vdots \\\\ 0&0&0&\\cdots &\\left< \\mathbf{e}_{n} ,\\mathbf{a}_{n} \\right>  \\end{array} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.180)$</p>\n",
    "\n",
    "Donde $\\left< \\mathbf{e}_{i} ,\\mathbf{a}_{k} \\right>  =\\left\\Vert \\mathbf{u}_{i} \\right\\Vert$. Podemos escribir el desarrollo (3.180) en forma matricial como $\\mathbf{A}=\\mathbf{Q}\\mathbf{R}$, donde $\\mathbf{Q}=(\\mathbf{e}_{1},...,\\mathbf{e}_{n})$ y\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{a}_{1} &=&\\left< \\mathbf{e}_{1} ,\\mathbf{a}_{1} \\right>  \\mathbf{e}_{1} \\\\ \\mathbf{a}_{2} &=&\\left< \\mathbf{e}_{1} ,\\mathbf{a}_{2} \\right>  \\mathbf{e}_{1} +\\left< \\mathbf{e}_{2} ,\\mathbf{a}_{2} \\right>  \\mathbf{e}_{2} \\\\ \\mathbf{a}_{3} &=&\\left< \\mathbf{e}_{1} ,\\mathbf{a}_{3} \\right>  \\mathbf{e}_{1} +\\left< \\mathbf{e}_{2} ,\\mathbf{a}_{2} \\right>  \\mathbf{e}_{2} +\\left< \\mathbf{e}_{3} ,\\mathbf{a}_{3} \\right>  \\mathbf{e}_{3} \\\\ \\vdots &\\vdots &\\vdots \\\\ \\mathbf{a}_{k} &=&\\sum^{k}_{j=1} \\left< \\mathbf{e}_{j} ,\\mathbf{a}_{k} \\right>  \\mathbf{e}_{j} \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.181)$</p>\n",
    "\n",
    "**Ejemplo 3.20:** Consideremos la matriz\n",
    "\n",
    "$$\\mathbf{A} =\\left( \\begin{matrix}1&1&0\\\\ 1&0&1\\\\ 0&1&1\\end{matrix} \\right)  \\in \\mathbb{R}^{3\\times 3}$$\n",
    "<p style=\"text-align: right;\">$(3.182)$</p>\n",
    "\n",
    "Notemos que podemos reescribir la matriz $\\mathbf{A}$ con un arreglo de vectores columna del tipo $\\mathbf{A}=(\\mathbf{a}_{1},\\mathbf{a}_{2},\\mathbf{a}_{3})$, donde\n",
    "\n",
    "$$\\mathbf{a}_{1} =\\left( \\begin{matrix}1\\\\ 1\\\\ 0\\end{matrix} \\right)  \\  ;\\  \\mathbf{a}_{2} =\\left( \\begin{matrix}1\\\\ 0\\\\ 1\\end{matrix} \\right)  \\  ;\\  \\mathbf{a}_{3} =\\left( \\begin{matrix}0\\\\ 1\\\\ 1\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.183)$</p>\n",
    "\n",
    "Dado que estos vectores son linealmente independientes en $\\mathbb{R}^{3}$, podemos construir un conjunto ortonormal de los mismos mediante el proceso de Gram-Schmidt. De esta manera tenemos que\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{u}_{1} &=&\\left( \\begin{matrix}1\\\\ 1\\\\ 0\\end{matrix} \\right)  \\\\ \\mathbf{e}_{1} &=&\\displaystyle \\frac{\\left( \\begin{matrix}1\\\\ 1\\\\ 0\\end{matrix} \\right)  }{\\left\\Vert \\left( \\begin{matrix}1\\\\ 1\\\\ 0\\end{matrix} \\right)  \\right\\Vert  } =\\displaystyle \\frac{1}{\\sqrt{2} } \\left( \\begin{matrix}1\\\\ 1\\\\ 0\\end{matrix} \\right)  =\\left( \\begin{array}{r}\\displaystyle \\frac{1}{\\sqrt{2} } \\\\ \\displaystyle \\frac{1}{\\sqrt{2} } \\\\ 0\\end{array} \\right)  \\\\ \\mathbf{u}_{2} &=&\\left( \\begin{matrix}1\\\\ 0\\\\ 1\\end{matrix} \\right)  -\\displaystyle \\frac{1}{\\sqrt{2} } \\left( \\begin{array}{r}\\displaystyle \\frac{1}{\\sqrt{2} } \\\\ \\displaystyle \\frac{1}{\\sqrt{2} } \\\\ 0\\end{array} \\right)  =\\left( \\begin{array}{r}\\displaystyle \\frac{1}{2} \\\\ -\\displaystyle \\frac{1}{2} \\\\ 1\\end{array} \\right)  \\\\ \\mathbf{e}_{2} &=&\\displaystyle \\frac{\\left( \\begin{array}{r}\\displaystyle \\frac{1}{2} \\\\ -\\displaystyle \\frac{1}{2} \\\\ 1\\end{array} \\right)  }{\\left\\Vert \\left( \\begin{array}{r}\\displaystyle \\frac{1}{2} \\\\ -\\displaystyle \\frac{1}{2} \\\\ 1\\end{array} \\right)  \\right\\Vert  } =\\displaystyle \\frac{1}{\\sqrt{3/2} } \\left( \\begin{array}{r}\\displaystyle \\frac{1}{2} \\\\ -\\displaystyle \\frac{1}{2} \\\\ 1\\end{array} \\right)  =\\left( \\begin{array}{r}\\displaystyle \\frac{1}{\\sqrt{6} } \\\\ -\\displaystyle \\frac{1}{\\sqrt{6} } \\\\ \\displaystyle \\frac{2}{\\sqrt{6} } \\end{array} \\right)  \\\\ \\mathbf{u}_{3} &=&\\left( \\begin{matrix}1\\\\ 0\\\\ 1\\end{matrix} \\right)  -\\displaystyle \\frac{1}{\\sqrt{2} } \\left( \\begin{array}{r}\\displaystyle \\frac{1}{\\sqrt{2} } \\\\ \\displaystyle \\frac{1}{\\sqrt{2} } \\\\ 0\\end{array} \\right)  -\\displaystyle \\frac{1}{\\sqrt{6} } \\left( \\begin{array}{r}\\displaystyle \\frac{1}{\\sqrt{6} } \\\\ -\\displaystyle \\frac{1}{\\sqrt{6} } \\\\ \\displaystyle \\frac{2}{\\sqrt{6} } \\end{array} \\right)  =\\left( \\begin{array}{r}-\\displaystyle \\frac{1}{\\sqrt{3} } \\\\ \\displaystyle \\frac{1}{\\sqrt{3} } \\\\ \\displaystyle \\frac{1}{\\sqrt{3} } \\end{array} \\right)  \\\\ \\mathbf{e}_{3} &=&\\displaystyle \\frac{\\left( \\begin{array}{r}-\\displaystyle \\frac{1}{\\sqrt{3} } \\\\ \\displaystyle \\frac{1}{\\sqrt{3} } \\\\ \\displaystyle \\frac{1}{\\sqrt{3} } \\end{array} \\right)  }{\\left\\Vert \\left( \\begin{array}{r}-\\displaystyle \\frac{1}{\\sqrt{3} } \\\\ \\displaystyle \\frac{1}{\\sqrt{3} } \\\\ \\displaystyle \\frac{1}{\\sqrt{3} } \\end{array} \\right)  \\right\\Vert  } =\\left( \\begin{array}{r}-\\displaystyle \\frac{1}{\\sqrt{3} } \\\\ \\displaystyle \\frac{1}{\\sqrt{3} } \\\\ \\displaystyle \\frac{1}{\\sqrt{3} } \\end{array} \\right)  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.184)$</p>\n",
    "\n",
    "De esta manera,\n",
    "\n",
    "$$\\mathbf{Q} =\\left( \\mathbf{e}_{1} ,\\mathbf{e}_{2} ,\\mathbf{e}_{2} \\right)  =\\left( \\begin{array}{rrr}\\frac{1}{\\sqrt{2} } &\\frac{1}{\\sqrt{6} } &-\\frac{1}{\\sqrt{3} } \\\\ \\frac{1}{\\sqrt{2} } &-\\frac{1}{\\sqrt{6} } &\\frac{1}{\\sqrt{3} } \\\\ 0&\\frac{2}{\\sqrt{6} } &\\frac{1}{\\sqrt{3} } \\end{array} \\right)  \\wedge \\mathbf{R} =\\left( \\begin{matrix}\\left< \\mathbf{u}_{1} ,\\mathbf{e}_{1} \\right>  &\\left< \\mathbf{u}_{2} ,\\mathbf{e}_{1} \\right>  &\\left< \\mathbf{u}_{3} ,\\mathbf{e}_{1} \\right>  \\\\ 0&\\left< \\mathbf{u}_{2} ,\\mathbf{e}_{2} \\right>  &\\left< \\mathbf{u}_{3} ,\\mathbf{e}_{2} \\right>  \\\\ 0&0&\\left< \\mathbf{u}_{3} ,\\mathbf{e}_{3} \\right>  \\end{matrix} \\right)  =\\left( \\begin{array}{rrr}\\frac{2}{\\sqrt{2} } &\\frac{1}{\\sqrt{2} } &\\frac{1}{\\sqrt{2} } \\\\ 0&\\frac{3}{\\sqrt{6} } &\\frac{1}{\\sqrt{6} } \\\\ 0&0&\\frac{2}{\\sqrt{3} } \\end{array} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.185)$</p>\n",
    "‚óºÔ∏é\n",
    "\n",
    "**Ejemplo 3.21:** En <font color='purple'>Numpy</font> existe igualmente una implementaci√≥n de la descomposici√≥n QR mediante el uso de la funci√≥n `qr()`, tambi√©n dependiente del m√≥dulo `numpy.linalg`. S√≥lo se requiere como entrada la matriz de inter√©s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d71c0ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos una matriz arbitraria de 4x4.\n",
    "A = rng.integers(low=-5, high=5, size=(4, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec40e0c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3,  2,  4, -3],\n",
       "       [ 2, -3,  2,  2],\n",
       "       [-1,  2, -3, -5],\n",
       "       [-5, -1,  4, -4]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mostramos nuestra matriz en pantalla.\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a8c3e032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos la descomposici√≥n QR de la matriz A.\n",
    "Q, R = linalg.qr(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "535fc5f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.48038446,  0.32771179,  0.7764457 , -0.24283292],\n",
       "       [ 0.32025631, -0.63614643,  0.26310283, -0.65079223],\n",
       "       [-0.16012815,  0.44337478, -0.51100417, -0.71878544],\n",
       "       [-0.80064077, -0.5397606 , -0.25842545,  0.02913995]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mostramos en pantalla la matriz Q.\n",
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee904bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.244998  , -1.44115338, -4.00320385,  6.08486984],\n",
       "       [ 0.        ,  3.99037303, -3.45061243, -2.31325973],\n",
       "       [ 0.        ,  0.        ,  4.13129917,  1.78559124],\n",
       "       [ 0.        ,  0.        ,  0.        ,  2.90428173]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mostramos en pantalla la matriz R.\n",
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "270d1bd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comprobamos que la descomposici√≥n as√≠ definida efectivamente genera la matriz A.\n",
    "np.allclose(A, Q @ R)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a195aeb7",
   "metadata": {},
   "source": [
    "‚óºÔ∏é\n",
    "\n",
    "## Descomposici√≥n en valores singulares.\n",
    "Procedemos ahora a revisar el √∫ltimo t√≥pico relativo a la descomposici√≥n matricial con uno de los resultados m√°s importantes del √°lgebra lineal, conocido como **descomposici√≥n en valores singulares** (com√∫nmente llamada SVD, del ingl√©s *singular value decomposition*). Corresponde a una generalizaci√≥n de la descomposici√≥n por autovalores y autovectores (diagonalizaci√≥n) y permite factorizar cualquier matriz $\\mathbf{A}\\in \\mathbb{K}^{m\\times n}$, incluso aunque $\\mathbf{A}$ no sea una matriz cuadrada ($m\\neq n$).\n",
    "\n",
    "Espec√≠ficamente, la descomposici√≥n SVD de una matriz $\\mathbf{A}\\in \\mathbb{K}^{m\\times n}$ es una factorizaci√≥n de la forma $\\mathbf{A}=\\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^{\\ast}$, donde $\\mathbf{U}$ es una matriz ortogonal de $m\\times m$ (es decir, $\\mathbf{U}^{\\ast}=\\mathbf{U}^{-1}$) con elementos no negativos en su diagonal, $\\mathbf{V} $ es una matriz ortogonal de $n\\times n$ y $\\mathbf{V}^{\\ast}$ es la matriz transpuesta conjugada de $\\mathbf{V}$. Si $\\mathbf{A}$ tiene solo elementos en $\\mathbb{R}$, la descomposici√≥n SVD se reduce a una del tipo $\\mathbf{A}=\\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^{\\top}$.\n",
    "\n",
    "Los elementos diagonales de $\\Sigma$, que denotamos como $\\sigma_{i} =\\left\\{ \\psi_{ij} \\right\\}$, son determinadas un√≠vocamente por la matriz $\\mathbf{A}$ y son llamados **valores singulares** de $\\mathbf{A}$. El n√∫mero de valores singulares no nulos es igual al rango de la matriz $\\mathbf{A}$. Las columnas de $\\mathbf{U}$ y las columnas de $\\mathbf{V}$ son llamadas **vectores singulares por la izquierda (siniestrales)** y **vectores singulares por la derecha (dextrales)**, respectivamente, de $\\mathbf{A}$. Entre ambos constituyen dos conjuntos de bases ortonormales $\\left\\{ \\mathbf{u}_{1} ,...,\\mathbf{u}_{m} \\right\\}  \\wedge \\left\\{ \\mathbf{v}_{1} ,...,\\mathbf{v}_{m} \\right\\}$, lo que permite que la descomposici√≥n SVD pueda escribirse de forma expl√≠cita como\n",
    "\n",
    "$$\\mathbf{A} =\\sum^{r}_{i=1} \\sigma_{i} \\mathbf{u}_{i} \\mathbf{v}^{\\ast }_{i} \\  ;\\  r\\leq \\rho \\left( \\mathbf{A} \\right)  =\\min \\left\\{ m,n\\right\\}$$\n",
    "<p style=\"text-align: right;\">$(3.186)$</p>\n",
    "\n",
    "donde $\\rho(\\mathbf{A})$ es el rango de $\\mathbf{A}$.\n",
    "\n",
    "La descomposici√≥n SVD no es √∫nica. Sin embargo, siempre es posible escoger una tal que los valores singulares $\\sigma_{i}$ se presenten en un orden decreciente. En este caso, la matriz $\\mathbf{\\Sigma}$ s√≠ es √∫nica y es determinada completamente por $\\mathbf{A}$ (pero no $\\mathbf{U}$ ni $\\mathbf{V}$).\n",
    "\n",
    "Antes de comenzar a estudiar en detalle esta descomposici√≥n, vamos a definirla por medio de un teorema (aunque, para efectos pr√°cticos, trabajaremos con matrices cuyos elementos son n√∫meros reales).\n",
    "\n",
    "**<font color='crimson'>Teorema 3.15 ‚Äì Descomposici√≥n en valores singulares:</font>** *Sea $\\mathbf{A}\\in \\mathbb{R}^{m\\times n}$ una matriz rectangular de rango $\\rho(\\mathbf{A})\\in [0, \\min \\left\\{ m,n\\right\\}]$. La descomposici√≥n en valores singulares de $\\mathbf{A}$ tiene la forma (esquem√°tica)*\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"figures/svd_illustration.png\" width=\"450\"></p>\n",
    "<p style=\"text-align: right;\">$(3.187)$</p>\n",
    "\n",
    "*Donde $\\mathbf{U}\\in \\mathbb{R}^{m\\times m}$ es una matriz ortogonal con columnas $\\mathbf{u}_{i}, i=1,...,m$, y $\\mathbf{V}\\in \\mathbb{R}^{n\\times n}$ es una matriz ortogonal con columnas $\\mathbf{v}_{j}, j=1,...,n$. La matriz de valores singulares $\\mathbf{\\Sigma } =\\left\\{ \\psi_{ij} \\right\\}  \\in \\mathbb{R}^{m\\times n}$ tiene las mismas dimensiones que $\\mathbf{A}$, y es tal que $psi_{ij}=0$ para $i\\neq j$.* ‚óÜ\n",
    "\n",
    "Como comentamos en un principio, la matriz de valores singulares $\\mathbf{\\Sigma}$, bajo ciertas condiciones, es √∫nica, pero requiere cierto nivel de atenci√≥n. Observemos que $\\mathbf{\\Sigma}$ tiene la misma dimensi√≥n que $\\mathbf{A}$. Esto significa que $\\mathbf{\\Sigma}$ contiene una submatriz diagonal con los valores singulares de $\\mathbf{A}$ con un relleno de ceros en las filas o columnas siguientes (lo que, en computaci√≥n cient√≠fica, se conoce como **zero padding**). Espec√≠ficamente, si $m>n$, entonces la matriz $\\mathbf{\\Sigma}$ tiene una estructura diagonal hasta la fila $n$ y luego consiste de vectores fila del tipo $\\mathbf{0}^{\\top}$ hasta la fila $m$. Es decir,\n",
    "\n",
    "$$\\mathbf{\\Sigma } =\\left( \\begin{matrix}\\sigma_{1} &0&\\cdots &0\\\\ 0&\\sigma_{2} &\\cdots &0\\\\ \\vdots &\\vdots &\\ddots &\\vdots \\\\ 0&0&\\cdots &\\sigma_{n} \\\\ \\vdots &\\vdots &\\ddots &\\vdots \\\\ 0&0&\\cdots &0\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.188)$</p>\n",
    "\n",
    "Por otro lado, si $m<n$, entonces la matriz $\\mathbf{\\Sigma}$ tiene una estructura diagonal hasta la columna $m$ y nula hasta la posici√≥n $n$. Es decir,\n",
    "\n",
    "$$\\mathbf{\\Sigma } =\\left( \\begin{matrix}\\sigma_{1} &0&\\cdots &0&\\cdots &0\\\\ 0&\\sigma_{2} &\\cdots &0&\\cdots &0\\\\ \\vdots &\\vdots &\\ddots &\\vdots &\\ddots &\\vdots \\\\ 0&0&\\cdots &\\sigma_{m} &\\cdots &0\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.189)$</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b03a2b71",
   "metadata": {},
   "source": [
    "### Una primera explicaci√≥n geom√©trica.\n",
    "La descomposici√≥n SVD nos ofrece una interpretaci√≥n geom√©trica intuitiva y muy interesante que nos permite describir este procedimiento antes de entrar en el detalle matem√°tico del mismo. En lo que sigue, discutiremos esta descomposici√≥n en base a una secuencia de transformaciones lineales aplicadas sobre ciertas bases del conjunto de matrices de inter√©s. Luego, por medio de un ejemplo, aplicaremos estas transformaciones a un conjunto de vectores en ‚Ñù^2, lo que nos permitir√° observar el efecto de cada una, en t√©rminos geom√©tricos, de manera m√°s clara.\n",
    "\n",
    "La descomposici√≥n SVD de una matriz puede ser interpretada como una que involucra a tres transformaciones lineales asociadas a las correspondientes matrices, digamos del tipo $T:\\mathbb{R}^{n}\\longrightarrow \\mathbb{R}^{n}$, como se muestra en la Fig. (3.7). En t√©rminos muy generales, la descomposici√≥n SVD realiza un cambio de base mediante el uso de la matriz $\\mathbf{V}^{\\ast}$, seguido de un escalamiento y aumento (o reducci√≥n) de su dimensi√≥n mediante el uso de la matriz de valores singulares $\\mathbf{\\Sigma}$. Finalmente, se realiza un segundo cambio de base por medio de la matriz $\\mathbf{U}$. Por supuesto, esta secuencia de operaciones guarda mucho m√°s detalles matem√°ticos que revisaremos m√°s adelante. Pero antes, refinaremos esta interpretaci√≥n geom√©trica poni√©ndole nombre a nuestras operaciones.\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"figures/fig_3_7.png\" width=\"600\"></p>\n",
    "<p style=\"text-align: center;\">Fig. (3.7): Ilustraci√≥n de la descomposici√≥n de valores singulares $\\mathbf{A}=\\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^{\\ast}$ de una matriz de $2\\times 2$ con elementos en $\\mathbb{R}$\n",
    "\n",
    "Asumamos que disponemos de una matriz de cambio de base asociada a una transformaci√≥n lineal $T:\\mathbb{R}^{n}\\longrightarrow \\mathbb{R}^{m}$ con respecto a las bases can√≥nicas $\\mathbf{e}(n)$ y $\\mathbf{e}(m)$, respectivamente. Adem√°s, consideremos dos bases m√°s, digamos $\\alpha$ y $\\beta$, para $\\mathbb{R}^{n}$ y $\\mathbb{R}^{m}$, respectivamente. Entonces:\n",
    "    \n",
    "- **(P1):** La matriz $\\mathbf{V}$ realiza un cambio de base en el dominio $\\mathbb{R}^{n}$ desde la base $\\alpha$ (representada por los vectores rojos $\\mathbf{v}_{1}$ y $\\mathbf{v}_{2}$ en la Fig. (3.7), panel superior izquierdo) a la base can√≥nica $\\mathbf{e}(n)$. Por lo tanto, podemos escribir $\\mathbf{V}^{\\ast}=[I]_{\\alpha}^{\\mathbf{e}(n)}$, lo que implica que el resultado de esta operaci√≥n es el alineamiento de estos vectores con los ejes del sistema $\\mathbb{R}^{m}$ (lo que se observa en el panel inferior izquierdo de la Fig. (3.7)).\n",
    "    \n",
    "- **(P2):** Habiendo cambiado la base de nuestras coordenadas a la referencia can√≥nica de $\\mathbb{R}^{m}$, la matriz $\\mathbf{\\Sigma}$ escala las nuevas coordenadas con respecto a los valores singulares $\\sigma_{i}$ (y a√±ade o quita dimensiones); es decir, $\\mathbf{\\Sigma}$ es la matriz de cambio de base de $T$ con respecto a $\\alpha$ y $\\beta$ (es decir, $\\mathbf{\\Sigma}=[I]_{\\alpha}^{\\beta}$), representada por los vectores rojos ubicados en el sistema $(\\mathbf{e}_{1},\\mathbf{e}_{2})$, los que adem√°s son modificados en su longitud. En sistema completo $(\\mathbf{e}_{1},\\mathbf{e}_{2})$ ahora se encuentra inmerso en otro de mayor dimensi√≥n, como se observa en el panel inferior derecho de la Fig. (3.7).\n",
    "    \n",
    "- **(P3):** La matriz $\\mathbf{U}$ realiza un cambio de base en codominio $\\mathbb{R}^{m}$ desde la base $\\beta$ a la base can√≥nica $\\mathbf{e}(m)$ (es decir, $\\mathbf{U}=[I]_{\\beta}^{\\mathbf{e}(m)}$), y que se representa por medio de una rotaci√≥n de los vectores rojos en el gr√°fico del panel superior derecho de la Fig. (3.8) fuera del sistema $(\\mathbf{e}_{1},\\mathbf{e}_{2})$.\n",
    "    \n",
    "La descomposici√≥n SVD representa un cambio de base en ambos, dominio y codominio de la transformaci√≥n lineal $T$. Esto es distinto a lo que ocurre en la descomposici√≥n por autovalores y autovectores (diagonalizaci√≥n), porque √©sta opera siempre en el mismo espacio vectorial, donde se aplica siempre un cambio de base que luego se deshace mediante un cambio de base inverso. Lo que hace especial a la descomposici√≥n SVD, es que estas dos bases distintas est√°n enlazadas simult√°neamente a la matriz de valores singulares $\\mathbf{\\Sigma}$.\n",
    "    \n",
    "**Ejemplo 3.22:** Consideremos una transformaci√≥n lineal que aplica una grilla cuadrada de vectores $\\mathcal{X}\\in \\mathbb{R}^{2}$ que caben en una caja de tama√±o $2\\times 2$ centrada en el origen. Usando la base can√≥nica de $\\mathbb{R}^{2}$, operamos sobre estos vectores usando las siguientes identidades:\n",
    "    \n",
    "$$\\mathbf{A} =\\left( \\begin{matrix}1&-0.8\\\\ 0&1\\\\ 1&0\\end{matrix} \\right)  =\\mathbf{U} \\mathbf{\\Sigma } \\mathbf{V}^{\\top } =\\left( \\begin{matrix}-0.79&0&-0.62\\\\ 0.38&-0.78&-0.49\\\\ -0.48&-0.62&0.62\\end{matrix} \\right)  \\left( \\begin{matrix}1.62&0\\\\ 0&1\\\\ 0&0\\end{matrix} \\right)  \\left( \\begin{matrix}-0.78&0.62\\\\ -0.62&-0.78\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.190)$</p>\n",
    "\n",
    "Partimos con un conjunto de vectores $\\mathcal{X}$ que son representados, en el panel superior izquierdo de la Fig. (3.8), como puntos coloreados (desde magenta a azul) arreglados en una grilla. Luego aplicamos $\\mathbf{V}^{\\top}\\in \\mathbb{R}^{2\\times 2}$, lo que genera una rotaci√≥n de $\\mathcal{X}$, lo que se muestra en el panel inferior izquierdo de la Fig. (3.8). Ahora transformamos estos vectores por medio de la matriz de valores singulares $\\mathbf{\\Sigma}$ desde el dominio $\\mathbb{R}^{2}$ al codominio $\\mathbb{R}^{3}$, como se observa en el panel inferior derecho de la Fig. (3.8). Notemos que todos los vectores $\\mathcal{X}$ se ubican en el plano $(x_{1},x_{2})$. La tercera coordenada es siempre igual a cero, y los vectores en el plano $(x_{1},x_{2})$ son escalados por los valores singulares presentes en la diagonal de $\\mathbf{\\Sigma}$.\n",
    "\n",
    "La transformaci√≥n de los vectores $\\mathcal{X}$ por medio de $\\mathbf{A}$ al codominio $\\mathbb{R}^{3}$ se iguala a la transformaci√≥n de $\\mathcal{X}$ por medio de la descomposici√≥n $\\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^{\\top}$, donde $\\mathbf{U}$ aplica una rotaci√≥n en el codominio $\\mathbb{R}^{3}$, de manera tal que los vectores sobre los cuales operamos mediante esta transformaci√≥n ya no est√°n restringidos al sistema $(x_{1},x_{2})$, aunque siguen residiendo en un plano, como se observa en el panel superior derecho de la Fig. (3.8).\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"figures/fig_3_8.png\" width=\"800\"></p>\n",
    "<p style=\"text-align: center;\">Fig. (3.8): Esquema geom√©trico que interpreta la aplicaci√≥n de la descomposici√≥n de valores singulares en el ejemplo (3.22)\n",
    "\n",
    "‚óºÔ∏é"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "759e5e2d",
   "metadata": {},
   "source": [
    "### Construcci√≥n de la descomposici√≥n SVD.\n",
    "A continuaci√≥n, discutiremos las **condiciones de existencia** de la descomposici√≥n en valores singulares y mostraremos c√≥mo calcularla en detalle. Esta descomposici√≥n comparte ciertas similitudes con la **diagonalizaci√≥n de matrices herm√≠ticas**, ya que\n",
    "\n",
    "$$\\mathbf{S} =\\mathbf{S}^{\\top } =\\mathbf{P} \\mathbf{D} \\mathbf{P}^{-1}$$\n",
    "<p style=\"text-align: right;\">$(3.191)$</p>\n",
    "\n",
    "Donde $\\mathbf{S}\\in \\mathbb{R}^{n\\times n}$ es una matriz herm√≠tica. La correspondiente descomposici√≥n en valores singulares es\n",
    "\n",
    "$$\\mathbf{S} =\\mathbf{U} \\mathbf{\\Sigma } \\mathbf{V}^{\\ast }$$\n",
    "<p style=\"text-align: right;\">$(3.192)$</p>\n",
    "\n",
    "Si ponemos $\\mathbf{U}=\\mathbf{P}=\\mathbf{V}$ y $\\mathbf{D}=\\mathbf{\\Sigma}$, podemos concluir que la descomposici√≥n en valores singulares de matrices herm√≠ticas es equivalente a su descomposici√≥n diagonal.\n",
    "\n",
    "A continuaci√≥n, discutiremos las condiciones bajo las cuales el Teorema 3.15 se cumple y c√≥mo construimos la descomposici√≥n SVD. El c√°lculo de esta descomposici√≥n para una matriz $\\mathbf{A}\\in \\mathbb{R}^{m\\times n}$ es equivalente a encontrar dos conjuntos de bases ortonormales $\\alpha =\\left\\{ \\mathbf{u}_{1} ,...,\\mathbf{u}_{m} \\right\\}  \\wedge \\beta =\\left\\{ \\mathbf{v}_{1} ,...,\\mathbf{v}_{n} \\right\\}$ relativos al dominio $\\mathbb{R}^{m}$ y al codominio $\\mathbb{R}^{n}$, respectivamente. A partir de estas bases, construimos las matrices $\\mathbf{U}$ y $\\mathbf{V}$. Para ello, ser√° necesario que establezcamos primero un teorema esencial en la construcci√≥n de la descomposici√≥n SVD.\n",
    "\n",
    "**<font color='crimson'>Teorema 3.16:</font>** *Sea $\\mathbf{A}\\in \\mathbb{R}^{m\\times n}$ una matriz no singular. Entonces siempre podremos definir una matriz sim√©trica y semi-definida positiva $\\mathbf{S}\\in \\mathbb{R}^{n\\times n}$, a partir de $\\mathbf{A}$, como*\n",
    "\n",
    "$$\\mathbf{S} :=\\mathbf{A}^{\\top } \\mathbf{A}$$\n",
    "<p style=\"text-align: right;\">$(3.193)$</p>\n",
    "‚óÜ\n",
    "\n",
    "Nuestro plan es comenzar construyendo el conjunto ortonormal de **vectores singulares por la derecha** $\\beta =\\left\\{ \\mathbf{v}_{1} ,...,\\mathbf{v}_{n} \\right\\}  \\in \\mathbb{R}^{n}$. Luego construiremos el conjunto de **vectores singulares por la izquierda** $\\alpha =\\left\\{ \\mathbf{u}_{1} ,...,\\mathbf{u}_{m} \\right\\}  \\in \\mathbb{R}^{m}$. A continuaci√≥n, enlazaremos ambos conjuntos con el requerimiento de **la ortogonalidad de $\\beta$ se preserve conforme la transformaci√≥n de $\\mathbf{A}$**. Esto √∫ltimo es importante porque sabemos de antemano que las im√°genes $\\mathbf{A}\\mathbf{v}_{i}$ conforman un conjunto ortogonal de vectores. Luego normalizaremos estas im√°genes por medio de factores de escalamiento, los que, por supuesto, resultar√°n ser los **valores singulares** de $\\mathbf{A}$.\n",
    "\n",
    "Partimos entonces construyendo los vectores singulares por la derecha. El teorema (3.10) nos dice que los autovectores de una matriz sim√©trica conforman una base ortonormal, lo que implica, por supuesto, que dicha matriz es diagonalizable. Adem√°s, del teorema (3.16), sabemos que siempre podemos construir una matriz sim√©trica y semi-definida positiva $\\mathbf{A}^{\\top}\\mathbf{A}\\in \\mathbb{R}^{n\\times n}$ a partir de cualquier matriz rectangular. De esta manera, podemos diagonalizar $\\mathbf{A}^{\\top}\\mathbf{A}$ y obtener\n",
    "\n",
    "$$\\mathbf{A}^{\\top } \\mathbf{A} =\\mathbf{P} \\mathbf{D} \\mathbf{P}^{\\top } =\\mathbf{P} \\left( \\begin{matrix}\\lambda_{1} &\\cdots &0\\\\ \\vdots &\\ddots &\\vdots \\\\ 0&\\cdots &\\lambda_{n} \\end{matrix} \\right)  \\mathbf{P}^{\\top }$$\n",
    "<p style=\"text-align: right;\">$(3.194)$</p>\n",
    "\n",
    "Donde $\\mathbf{P}$ es una matriz ortogonal que est√° compuesta por una base ortonormal de autovectores. Los escalares $\\lambda_{1}, ..., \\lambda_{n}$ son los autovalores de $\\mathbf{A}^{\\top}\\mathbf{A}$. Asumamos entonces que la descomposici√≥n en valores singulares de $\\mathbf{A}$ existe y apliquemos el teorema (3.15) a la ecuaci√≥n (3.194). De esta manera, obtenemos\n",
    "\n",
    "$$\\mathbf{A}^{\\top } \\mathbf{A} =\\left( \\mathbf{U} \\mathbf{\\Sigma } \\mathbf{V}^{\\top } \\right)^{\\top }  \\left( \\mathbf{U} \\mathbf{\\Sigma } \\mathbf{V}^{\\top } \\right)  =\\mathbf{V} \\mathbf{\\Sigma }^{\\top } \\mathbf{U}^{\\top } \\mathbf{U} \\mathbf{\\Sigma } \\mathbf{V}^{\\top }$$\n",
    "<p style=\"text-align: right;\">$(3.195)$</p>\n",
    "\n",
    "Como $\\mathbf{U}$ y $\\mathbf{V}$ son matrices ortogonales, tenemos que $\\mathbf{U}^{\\top}\\mathbf{U}=\\mathbf{I}_{m}$. Por lo tanto,\n",
    "\n",
    "$$\\mathbf{A}^{\\top } \\mathbf{A} =\\mathbf{V} \\mathbf{\\Sigma }^{\\top } \\mathbf{\\Sigma } \\mathbf{V}^{\\top } =\\mathbf{V} \\left( \\begin{matrix}\\sigma^{2}_{1} &\\cdots &0\\\\ \\vdots &\\ddots &\\vdots \\\\ 0&\\cdots &\\sigma^{2}_{n} \\end{matrix} \\right)  \\mathbf{V}^{\\top }$$\n",
    "<p style=\"text-align: right;\">$(3.196)$</p>\n",
    "\n",
    "Si comparamos las ecuaciones (3.196) y (3.194), podemos darnos cuenta que\n",
    "\n",
    "$$\\mathbf{V}^{\\top } =\\mathbf{P}^{\\top } \\wedge \\sigma^{2}_{i} =\\lambda_{i}$$\n",
    "<p style=\"text-align: right;\">$(3.196)$</p>\n",
    "\n",
    "Por lo tanto, los autovectores de $\\mathbf{A}^{\\top}\\mathbf{A}$ que componen $\\mathbf{P}$ son los vectores singulares derechos $\\mathbf{V}$ de $\\mathbf{A}$, mientras que los autovalores de $\\mathbf{A}^{\\top}\\mathbf{A}$ son iguales al cuadrado de los valores singulares de $\\mathbf{A}$ (y que se corresponden con los elementos no nulos de la matriz $\\mathbf{\\Sigma}$. Para obtener los vectores singulares por la izquierda que componen $\\mathbf{U}$, seguimos un procedimiento similar. Partimos calculando la descomposici√≥n en valores singulares de la matriz sim√©trica $\\mathbf{A}\\mathbf{A}^{\\top}\\in \\mathbb{R}^{m\\times m}$. En este caso, obtenemos\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{A} \\mathbf{A}^{\\top } &=&\\left( \\mathbf{U} \\mathbf{\\Sigma } \\mathbf{V}^{\\top } \\right)  \\left( \\mathbf{U} \\mathbf{\\Sigma } \\mathbf{V}^{\\top } \\right)^{\\top }  \\\\ &=&\\mathbf{U} \\mathbf{\\Sigma } \\mathbf{V}^{\\top } \\mathbf{V} \\mathbf{\\Sigma }^{\\top } \\mathbf{U}^{\\top } \\\\ &=&\\mathbf{U} \\left( \\begin{matrix}\\sigma^{2}_{1} &\\cdots &0\\\\ \\vdots &\\ddots &\\vdots \\\\ 0&\\cdots &\\sigma^{2}_{n} \\end{matrix} \\right)  \\mathbf{U}^{\\top } \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.197)$</p>\n",
    "\n",
    "El teorema (3.10) nos dice que $\\mathbf{A}\\mathbf{A}^{\\top} = \\mathbf{S}\\mathbf{D}\\mathbf{S}^{\\top}$ es diagonalizable y que podemos encontrar una base ortonormal de autovectores de $\\mathbf{A}\\mathbf{A}^{\\top}$, los que est√°n contenidos en $\\mathbf{S}$. Los autovectores ortonormales de $\\mathbf{A}\\mathbf{A}^{\\top}$ son los vectores singulares por la izquierda que constituyen $\\mathbf{U}$ y conforman una base ortonormal en el codominio de la descomposici√≥ SVD.\n",
    "\n",
    "Lo anterior deja abierta la pregunta relativa a la estructura de la matriz $\\mathbf{\\Sigma}$. Dado que $\\mathbf{A}\\mathbf{A}^{\\top}$ y $\\mathbf{A}^{\\top}\\mathbf{A}$ tienen los mismos autovalores no nulos, las entradas no nulas de las matrices $\\mathbf{\\Sigma}$ en ambas descomposiciones SVD (para $\\mathbf{A}\\mathbf{A}^{\\top}$ y $\\mathbf{A}\\mathbf{A}^{\\top}$ deben ser las mismas.\n",
    "\n",
    "El √∫ltimo paso es unir todo lo que hemos hecho hasta ahora. Tenemos un conjunto ortonormal de vectores singulares por la derecha en la matriz $\\mathbf{V}$ y, para finalizar la construcci√≥n de la factorizaci√≥n SVD, debemos unirlos a los vectores singulares por la izquierda que componen la matriz $\\mathbf{U}$. Para lograr esto, usamos el hecho de que las im√°genes de los vectores $\\mathbf{v}_{1},...,\\mathbf{v}_{n}$ bajo $\\mathbf{A}$ tienen que ser, tambi√©n, ortogonales. En ese caso, necesitamos que el producto interno entre $\\mathbf{A}\\mathbf{v}_{i}$ y $\\mathbf{A}\\mathbf{v}_{j}$ sea nulo para $i\\neq j$. De este modo, tenemos que\n",
    "\n",
    "$$\\left( \\mathbf{A} \\mathbf{v}_{i} \\right)^{\\top }  \\left( \\mathbf{A} \\mathbf{v}_{j} \\right)  =\\mathbf{v}^{\\top }_{i} \\left( \\mathbf{A}^{\\top } \\mathbf{A} \\right)  \\mathbf{v}_{j} =\\mathbf{v}^{\\top }_{i} \\left( \\lambda_{j} \\mathbf{v}_{j} \\right)  =\\lambda_{j} \\mathbf{v}^{\\top }_{i} \\mathbf{v}_{j} =0$$\n",
    "<p style=\"text-align: right;\">$(3.198)$</p>\n",
    "\n",
    "Para el caso $m\\geq r$, se tiene que $\\left\\{ \\mathbf{A} \\mathbf{v}_{1} ,...,\\mathbf{A} \\mathbf{v}_{r} \\right\\}$ es una base para un subespacio $r$-dimensional de $\\mathbb{R}^{m}$.\n",
    "\n",
    "Para completar la construcci√≥n de la descomposici√≥n SVD, necesitamos vectores singulares por la izquierda que sean ortonormales. Por lo tanto, normalizamos las im√°genes de los vectores singulares por la derecha $\\mathbf{A} \\mathbf{v}_{i}$, obteniendo\n",
    "\n",
    "$$\\mathbf{u}_{i} :=\\frac{\\mathbf{A} \\mathbf{v}_{i} }{\\left\\Vert \\mathbf{A} \\mathbf{v}_{i} \\right\\Vert  } =\\frac{1}{\\sqrt{\\lambda_{i} } } \\mathbf{A} \\mathbf{v}_{i} =\\frac{1}{\\sigma_{i} } \\mathbf{A} \\mathbf{v}_{i}$$\n",
    "<p style=\"text-align: right;\">$(3.199)$</p>\n",
    "\n",
    "Por lo tanto, los autovectores de $\\mathbf{A}^{\\top}\\mathbf{A}$, que ya sabemos que son los vectores singulares por la derecha $\\mathbf{v}_{i}$, y sus im√°genes normalizadas bajo $\\mathbf{A}$, los vectores singulares por la izquierda $\\mathbf{u}_{i}$, conforman dos bases ortonormales consistentes que est√°n conectadas por la matriz de valores singulares $\\mathbf{\\Sigma}$.\n",
    "\n",
    "Vamos a reorganizar la ecuaci√≥n (3.199) para obtener la **ecuaci√≥n de valores singulares**\n",
    "\n",
    "$$\\mathbf{A} \\mathbf{v}_{i} =\\sigma_{i} \\mathbf{u}_{i} \\  ;\\  i=1,...,r$$\n",
    "<p style=\"text-align: right;\">$(3.200)$</p>\n",
    "\n",
    "Para $n<m$, la ecuaci√≥n (3.200) se cumple √∫nicamente para $i\\leq n$, pero no nos dice nada acerca de los vectores $\\mathbf{u}_{i}$ para $i>n$. Sin embargo, por construcci√≥n, sabemos que dichos vectores son ortonormales. Por el contrario, para $m<n$, la ecuaci√≥n (3.200) se cumple √∫nicamente para $i\\geq m$. Para $i>m$, tenemos que $\\mathbf{A}\\mathbf{v}_{i}=0$ y a√∫n sabemos que los vectores $\\mathbf{v}_{i}$ conforman un conjunto ortonormal. Esto significa que la descomposici√≥n SVD tambi√©n nos provee de una base ortornomal para $\\ker(\\mathbf{A})$ (es decir, el conjunto de vectores $\\mathbf{x}$ tales que $\\mathbf{A}\\mathbf{x}=\\mathbf{0}$).\n",
    "\n",
    "Uniendo los vectores $\\mathbf{v}_{i}$ como columnas de $\\mathbf{V}$ y los vectores $\\mathbf{u}_{i}$ como columnas de $\\mathbf{U}$, obtenemos\n",
    "\n",
    "$$\\mathbf{A} \\mathbf{v} =\\mathbf{U} \\mathbf{\\Sigma } $$\n",
    "<p style=\"text-align: right;\">$(3.201)$</p>\n",
    "\n",
    "Donde $\\mathbf{\\Sigma }$ tiene la misma dimensi√≥n que $\\mathbf{A}$ y una estructura diagonal para las filas 1 a $r$. Por lo tanto, multiplicando esta expresi√≥n por la derecha por $\\mathbf{V}^{\\top}$, obtenemos $\\mathbf{A}=\\mathbf{U}\\mathbf{\\Sigma }\\mathbf{V}^{\\top}$, que resulta ser la descomposici√≥n SVD de $\\mathbf{A}$.\n",
    "\n",
    "**Ejemplo 3.23:** Vamos a determinar la descomposici√≥n en valores singulares de la matriz $\\mathbf{A}$ definida como\n",
    "\n",
    "$$\\mathbf{A} =\\left( \\begin{matrix}4&11&14\\\\ 8&7&-2\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.202)$</p>\n",
    "\n",
    "Siguiendo el proceso de construcci√≥n de la descomposici√≥n SVD visto previamente, sabemos que el primer paso es determinar los autovalores del producto $\\mathbf{A}^{\\top}\\mathbf{A}$. En este caso, como\n",
    "\n",
    "$$\\mathbf{A}^{\\top } \\mathbf{A} =\\left( \\begin{matrix}80&100&40\\\\ 100&170&140\\\\ 40&140&200\\end{matrix} \\right)  $$\n",
    "<p style=\"text-align: right;\">$(3.203)$</p>\n",
    "\n",
    "y se tiene que el polinomio caracter√≠stico de esta matriz es $P_{\\mathbf{A}^{\\top } \\mathbf{A} }\\left( \\lambda \\right)  =\\det \\left( \\mathbf{A}^{\\top } \\mathbf{A} -\\lambda \\mathbf{I}_{3} \\right)  =\\lambda \\left( \\lambda -360\\right)  \\left( \\lambda -90\\right)$, lo que implica que sus autovalores son $\\lambda_{1} =360,\\lambda_{2} =90$ y $\\lambda_{3}=0$. Por lo tanto, los valores singulares de $\\mathbf{A}$ son $\\sigma_{1} =6\\sqrt{10} ,\\sigma_{2} =3\\sqrt{10}$ y $\\sigma_{3}=0$. Buscaremos ahora una base ortonormal de autovectores para $\\mathbf{A}^{\\top } \\mathbf{A}$. Para ello, determinamos los autoespacios respectivos como sigue,\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{u} \\in \\left( \\mathbb{R}^{3\\times 1} \\right)_{\\lambda }  &\\Longleftrightarrow &\\mathbf{u} \\in \\mathbb{R}^{3\\times 1} \\wedge \\left( \\mathbf{A}^{\\top } \\mathbf{A} \\right)  \\mathbf{u} =\\lambda \\mathbf{u} \\\\ &\\Longleftrightarrow &\\mathbf{u} =\\left( \\begin{matrix}x\\\\ y\\\\ z\\end{matrix} \\right)  \\wedge \\left( \\begin{matrix}80&100&40\\\\ 100&170&140\\\\ 40&140&200\\end{matrix} \\right)  \\left( \\begin{matrix}x\\\\ y\\\\ z\\end{matrix} \\right)  =\\left( \\begin{matrix}\\lambda x\\\\ \\lambda y\\\\ \\lambda z\\end{matrix} \\right)  \\\\ &\\Longleftrightarrow &\\mathbf{u} =\\left( \\begin{matrix}x\\\\ y\\\\ z\\end{matrix} \\right)  \\wedge \\begin{cases}\\begin{array}{rcl}80x+100y+40z&=&\\lambda x\\\\ 100x+170y+140z&=&\\lambda y\\\\ 40x+140y+200z&=&\\lambda z\\end{array} &\\end{cases} \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.204)$</p>\n",
    "\n",
    "Para $\\lambda_{1}=360$:\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{u} \\in \\left( \\mathbb{R}^{3\\times 1} \\right)_{\\lambda =360}  &\\Longleftrightarrow &\\mathbf{u} =\\left( \\begin{matrix}x\\\\ y\\\\ z\\end{matrix} \\right)  \\wedge \\begin{cases}\\begin{array}{rcl}80x+100y+40z&=&360x\\\\ 100x+170y+140z&=&360y\\\\ 40x+140y+200z&=&360z\\end{array} &\\end{cases} \\\\ &\\Longrightarrow &\\mathbf{u} =\\left( \\begin{matrix}x\\\\ y\\\\ z\\end{matrix} \\right)  \\wedge y=2x\\wedge z=2x\\\\ &\\Longrightarrow &\\mathbf{u} =\\left( \\begin{matrix}x\\\\ 2x\\\\ 2x\\end{matrix} \\right)  =x\\left( \\begin{matrix}1\\\\ 2\\\\ 2\\end{matrix} \\right)  \\\\ &\\Longrightarrow &\\left( \\mathbb{R}^{3\\times 1} \\right)_{\\lambda =360}  =\\left< \\left\\{ \\left( \\begin{matrix}1\\\\ 2\\\\ 2\\end{matrix} \\right)  \\right\\}  \\right>  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.205)$</p>\n",
    "\n",
    "Para $\\lambda_{2}=90$:\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{u} \\in \\left( \\mathbb{R}^{3\\times 1} \\right)_{\\lambda =90}  &\\Longleftrightarrow &\\mathbf{u} =\\left( \\begin{matrix}x\\\\ y\\\\ z\\end{matrix} \\right)  \\wedge \\begin{cases}\\begin{array}{rcl}80x+100y+40z&=&90x\\\\ 100x+170y+140z&=&90y\\\\ 40x+140y+200z&=&90z\\end{array} &\\end{cases} \\\\ &\\Longrightarrow &\\mathbf{u} =\\left( \\begin{matrix}x\\\\ y\\\\ z\\end{matrix} \\right)  \\wedge 2z=-2x\\wedge 2z=-y\\\\ &\\Longrightarrow &\\mathbf{u} =\\left( \\begin{matrix}-2x\\\\ -x\\\\ 2x\\end{matrix} \\right)  =x\\left( \\begin{matrix}-2\\\\ -1\\\\ 2\\end{matrix} \\right)  \\\\ &\\Longrightarrow &\\left( \\mathbb{R}^{3\\times 1} \\right)_{\\lambda =90}  =\\left< \\left\\{ \\left( \\begin{matrix}-2\\\\ -1\\\\ 2\\end{matrix} \\right)  \\right\\}  \\right>  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.206)$</p>\n",
    "\n",
    "Finalmente, para $\\lambda_{3}=0$\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{u} \\in \\left( \\mathbb{R}^{3\\times 1} \\right)_{\\lambda =0}  &\\Longleftrightarrow &\\mathbf{u} =\\left( \\begin{matrix}x\\\\ y\\\\ z\\end{matrix} \\right)  \\wedge \\begin{cases}\\begin{array}{rcl}80x+100y+40z&=&0x\\\\ 100x+170y+140z&=&0y\\\\ 40x+140y+200z&=&0z\\end{array} &\\end{cases} \\\\ &\\Longrightarrow &\\mathbf{u} =\\left( \\begin{matrix}x\\\\ y\\\\ z\\end{matrix} \\right)  \\wedge z=2x\\wedge z=-2y\\\\ &\\Longrightarrow &\\mathbf{u} =\\left( \\begin{matrix}2x\\\\ -2x\\\\ x\\end{matrix} \\right)  =x\\left( \\begin{matrix}2\\\\ -2\\\\ 1\\end{matrix} \\right)  \\\\ &\\Longrightarrow &\\left( \\mathbb{R}^{3\\times 1} \\right)_{\\lambda =90}  =\\left< \\left\\{ \\left( \\begin{matrix}2\\\\ -2\\\\ 1\\end{matrix} \\right)  \\right\\}  \\right>  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.207)$</p>\n",
    "\n",
    "Por lo tanto, una base ortonormal de autovectores para $\\mathbf{A}^{\\top}\\mathbf{A}$ es\n",
    "\n",
    "$$\\alpha =\\left\\{ \\frac{\\left( \\begin{matrix}1\\\\ 2\\\\ 2\\end{matrix} \\right)  }{\\left\\Vert \\left( \\begin{matrix}1\\\\ 2\\\\ 2\\end{matrix} \\right)  \\right\\Vert  } ,\\frac{\\left( \\begin{matrix}-2\\\\ -1\\\\ 2\\end{matrix} \\right)  }{\\left\\Vert \\left( \\begin{matrix}-2\\\\ -1\\\\ 2\\end{matrix} \\right)  \\right\\Vert  } ,\\frac{\\left( \\begin{matrix}2\\\\ -2\\\\ 1\\end{matrix} \\right)  }{\\left\\Vert \\left( \\begin{matrix}2\\\\ -2\\\\ 1\\end{matrix} \\right)  \\right\\Vert  } \\right\\}  =\\left\\{ \\frac{1}{\\sqrt{9} } \\left( \\begin{matrix}1\\\\ 2\\\\ 2\\end{matrix} \\right)  ,\\frac{1}{\\sqrt{9} } \\left( \\begin{matrix}-2\\\\ -1\\\\ 2\\end{matrix} \\right)  ,\\frac{1}{\\sqrt{9} } \\left( \\begin{matrix}2\\\\ -2\\\\ 1\\end{matrix} \\right)  \\right\\}  =\\left\\{ \\frac{1}{3} \\left( \\begin{matrix}1\\\\ 2\\\\ 2\\end{matrix} \\right)  ,\\frac{1}{3} \\left( \\begin{matrix}-2\\\\ -1\\\\ 2\\end{matrix} \\right)  ,\\frac{1}{3} \\left( \\begin{matrix}2\\\\ -2\\\\ 1\\end{matrix} \\right)  \\right\\}  $$\n",
    "<p style=\"text-align: right;\">$(3.208)$</p>\n",
    "\n",
    "Calculamos ahora $\\mathbf{A}\\mathbf{v}_{i}$ para $i=1,2,3$ como sigue,\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{A} \\mathbf{v}_{1} &=&\\left( \\begin{matrix}4&11&14\\\\ 8&7&-2\\end{matrix} \\right)  \\left( \\begin{matrix}1/3\\\\ 2/3\\\\ 2/3\\end{matrix} \\right)  =\\left( \\begin{matrix}18\\\\ 6\\end{matrix} \\right)  \\\\ \\mathbf{A} \\mathbf{v}_{2} &=&\\left( \\begin{matrix}4&11&14\\\\ 8&7&-2\\end{matrix} \\right)  \\left( \\begin{matrix}-2/3\\\\ -1/3\\\\ 2/3\\end{matrix} \\right)  =\\left( \\begin{matrix}3\\\\ -9\\end{matrix} \\right)  \\\\ \\mathbf{A} \\mathbf{v}_{3} &=&\\left( \\begin{matrix}4&11&14\\\\ 8&7&-2\\end{matrix} \\right)  \\left( \\begin{matrix}2/3\\\\ -2/3\\\\ 1/3\\end{matrix} \\right)  =\\left( \\begin{matrix}0\\\\ 0\\end{matrix} \\right)  \\end{array} $$\n",
    "<p style=\"text-align: right;\">$(3.209)$</p>\n",
    "\n",
    "Como hay dos valores singulares no nulos, tenemos\n",
    "\n",
    "$$\\mathbf{u}_{1} =\\displaystyle \\frac{1}{\\sigma_{1} } \\mathbf{A} \\mathbf{v}_{1} =\\displaystyle \\frac{1}{6\\sqrt{10} } \\left( \\begin{matrix}18\\\\ 6\\end{matrix} \\right)  =\\left( \\begin{array}{r}\\displaystyle \\frac{3}{\\sqrt{10} } \\\\ \\displaystyle \\frac{1}{\\sqrt{10} } \\end{array} \\right)  \\  ;\\  \\mathbf{u}_{2} =\\displaystyle \\frac{1}{\\sigma_{2} } \\mathbf{A} \\mathbf{v}_{2} =\\displaystyle \\frac{1}{3\\sqrt{10} } \\left( \\begin{matrix}3\\\\ -9\\end{matrix} \\right)  =\\left( \\begin{array}{r}\\displaystyle \\frac{1}{\\sqrt{10} } \\\\ -\\displaystyle \\frac{3}{\\sqrt{10} } \\end{array} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.210)$</p>\n",
    "\n",
    "Notemos que $\\beta=\\left\\{ \\mathbf{u}_{1} ,\\mathbf{u}_{2} \\right\\}$ es una base ortonormal para $\\mathbb{R}^{2}$. De esta manera, la descomposici√≥n en valores singulares de $\\mathbf{A}$ es\n",
    "\n",
    "$$\\mathbf{A} =\\underbrace{\\left( \\begin{array}{rr}\\displaystyle \\frac{3}{\\sqrt{10} } &\\displaystyle \\frac{1}{\\sqrt{10} } \\\\ \\displaystyle \\frac{1}{\\sqrt{10} } &-\\displaystyle \\frac{3}{\\sqrt{10} } \\end{array} \\right)  }_{\\mathbf{U} } \\underbrace{\\left( \\begin{matrix}6\\sqrt{10} &0&0\\\\ 0&3\\sqrt{10} &0\\end{matrix} \\right)  }_{\\mathbf{\\Sigma } } \\underbrace{\\left( \\begin{array}{rrr}1/3&2/3&2/3\\\\ -2/3&-1/3&2/3\\\\ 2/3&-2/3&1/3\\end{array} \\right)  }_{\\mathbf{V}^{\\top } }$$\n",
    "<p style=\"text-align: right;\">$(3.211)$</p>\n",
    "‚óºÔ∏é\n",
    "\n",
    "**Ejemplo 3.24:** Vamos a encontrar la descomposici√≥n en valores singulares de la matriz\n",
    "\n",
    "$$\\mathbf{A} =\\left( \\begin{matrix}1&0&1\\\\ -2&1&0\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.212)$</p>\n",
    "\n",
    "En efecto, primero construimos la matriz $\\mathbf{A}^{\\top} \\mathbf{A}$ como sigue,\n",
    "\n",
    "$$\\mathbf{A}^{\\top } \\mathbf{A} =\\left( \\begin{matrix}1&-2\\\\ 0&1\\\\ 1&0\\end{matrix} \\right)  \\left( \\begin{matrix}1&0&1\\\\ -2&1&0\\end{matrix} \\right)  =\\left( \\begin{matrix}5&-2&1\\\\ -2&1&0\\\\ 1&0&1\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.213)$</p>\n",
    "\n",
    "Queremos pues diagonalizar $\\mathbf{A}^{\\top } \\mathbf{A}$. Partimos calculando su polinomio caracter√≠stico,\n",
    "\n",
    "$$P_{\\mathbf{A}^{\\top } \\mathbf{A} }\\left( \\lambda \\right)  =\\left( \\begin{matrix}5-\\lambda &-2&1\\\\ -2&1-\\lambda &0\\\\ 1&0&1-\\lambda \\end{matrix} \\right)  =\\lambda \\left( \\lambda -6\\right)  \\left( \\lambda -1\\right)$$\n",
    "<p style=\"text-align: right;\">$(3.214)$</p>\n",
    "\n",
    "Por lo que los autovalores de $\\mathbf{A}^{\\top } \\mathbf{A}$ son $\\lambda_{1}=6,\\lambda_{2}=1$ y $\\lambda_{3}=0$. De esta manera, los valores singulares de $\\mathbf{A}^{\\top } \\mathbf{A}$ son $\\sigma_{1}=\\sqrt{6}, \\sigma_{2}=1$ y $\\sigma_{3}=0$. Debemos buscar ahora una base ortonormal de autovectores para $\\mathbf{A}^{\\top } \\mathbf{A}$ a partir del c√°lculo de sus autoespacios. De este modo tenemos que\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{u} \\in \\left( \\mathbb{R}^{3\\times 1} \\right)_{\\lambda }  &\\Longleftrightarrow &\\mathbf{u} \\in \\mathbb{R}^{3\\times 1} \\wedge \\left( \\mathbf{A}^{\\top } \\mathbf{A} \\right)  \\mathbf{u} =\\lambda \\mathbf{u} \\\\ &\\Longleftrightarrow &\\mathbf{u} =\\left( \\begin{matrix}x\\\\ y\\\\ z\\end{matrix} \\right)  \\wedge \\left( \\begin{matrix}5&-2&1\\\\ -2&1&0\\\\ 1&0&1\\end{matrix} \\right)  \\left( \\begin{matrix}x\\\\ y\\\\ z\\end{matrix} \\right)  =\\left( \\begin{matrix}\\lambda x\\\\ \\lambda y\\\\ \\lambda z\\end{matrix} \\right)  \\\\ &\\Longleftrightarrow &\\mathbf{u} =\\left( \\begin{matrix}x\\\\ y\\\\ z\\end{matrix} \\right)  \\wedge \\begin{cases}\\begin{array}{rcl}5x-2y+z&=&\\lambda x\\\\ -2x+y&=&\\lambda y\\\\ x+z&=&\\lambda z\\end{array} &\\end{cases} \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.215)$</p>\n",
    "\n",
    "Luego, para $\\lambda_{1}=6$,\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{u} \\in \\left( \\mathbb{R}^{3\\times 1} \\right)_{\\lambda =6}  &\\Longrightarrow &\\mathbf{u} =\\left( \\begin{matrix}x\\\\ y\\\\ z\\end{matrix} \\right)  \\wedge \\begin{array}{rcl}5x-2y+z&=&6x\\\\ -2x+y&=&6y\\\\ x+z&=&6z\\end{array} \\\\ &\\Longrightarrow &\\mathbf{u} =\\left( \\begin{matrix}x\\\\ y\\\\ z\\end{matrix} \\right)  \\wedge x=5z\\wedge y=-2z\\\\ &\\Longrightarrow &\\mathbf{u} =\\left( \\begin{matrix}5z\\\\ -2z\\\\ z\\end{matrix} \\right)  =z\\left( \\begin{matrix}5\\\\ -2\\\\ 1\\end{matrix} \\right)  \\\\ &\\Longrightarrow &\\left( \\mathbb{R}^{3\\times 1} \\right)_{\\lambda =6}  =\\left< \\left\\{ \\left( \\begin{matrix}5\\\\ -2\\\\ 1\\end{matrix} \\right)  \\right\\}  \\right>  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.216)$</p>\n",
    "\n",
    "Adem√°s, para $\\lambda_{2}=1$\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{u} \\in \\left( \\mathbb{R}^{3\\times 1} \\right)_{\\lambda =1}  &\\Longrightarrow &\\mathbf{u} =\\left( \\begin{matrix}x\\\\ y\\\\ z\\end{matrix} \\right)  \\wedge \\begin{array}{rcl}5x-2y+z&=&x\\\\ -2x+y&=&y\\\\ x+z&=&z\\end{array} \\\\ &\\Longrightarrow &\\mathbf{u} =\\left( \\begin{matrix}x\\\\ y\\\\ z\\end{matrix} \\right)  \\wedge x=0\\wedge z=2y\\\\ &\\Longrightarrow &\\mathbf{u} =\\left( \\begin{matrix}0\\\\ y\\\\ 2y\\end{matrix} \\right)  =y\\left( \\begin{matrix}0\\\\ 1\\\\ 2\\end{matrix} \\right)  \\\\ &\\Longrightarrow &\\left( \\mathbb{R}^{3\\times 1} \\right)_{\\lambda =1}  =\\left< \\left\\{ \\left( \\begin{matrix}0\\\\ 1\\\\ 2\\end{matrix} \\right)  \\right\\}  \\right>  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.217)$</p>\n",
    "\n",
    "Finalmente, para $\\lambda_{3}=0$,\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{u} \\in \\left( \\mathbb{R}^{3\\times 1} \\right)_{\\lambda =0}  &\\Longrightarrow &\\mathbf{u} =\\left( \\begin{matrix}x\\\\ y\\\\ z\\end{matrix} \\right)  \\wedge \\begin{array}{rcl}5x-2y+z&=&0x\\\\ -2x+y&=&0y\\\\ x+z&=&0z\\end{array} \\\\ &\\Longrightarrow &\\mathbf{u} =\\left( \\begin{matrix}x\\\\ y\\\\ z\\end{matrix} \\right)  \\wedge y=-2z\\wedge x=-z\\\\ &\\Longrightarrow &\\mathbf{u} =\\left( \\begin{matrix}-z\\\\ -2z\\\\ z\\end{matrix} \\right)  =z\\left( \\begin{matrix}-1\\\\ -2\\\\ 1\\end{matrix} \\right)  \\\\ &\\Longrightarrow &\\left( \\mathbb{R}^{3\\times 1} \\right)_{\\lambda =0}  =\\left< \\left\\{ \\left( \\begin{matrix}-1\\\\ -2\\\\ 1\\end{matrix} \\right)  \\right\\}  \\right>  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.218)$</p>\n",
    "\n",
    "Por lo tanto, una base ortonormal de autovectores para $\\mathbf{A}^{\\top}\\mathbf{A}$ es\n",
    "\n",
    "$$\\alpha =\\left\\{ \\frac{\\left( \\begin{matrix}5\\\\ -2\\\\ 1\\end{matrix} \\right)  }{\\left\\Vert \\left( \\begin{matrix}5\\\\ -2\\\\ 1\\end{matrix} \\right)  \\right\\Vert  } ,\\frac{\\left( \\begin{matrix}0\\\\ 1\\\\ 2\\end{matrix} \\right)  }{\\left\\Vert \\left( \\begin{matrix}0\\\\ 1\\\\ 2\\end{matrix} \\right)  \\right\\Vert  } ,\\frac{\\left( \\begin{matrix}-1\\\\ -2\\\\ 1\\end{matrix} \\right)  }{\\left\\Vert \\left( \\begin{matrix}-1\\\\ -2\\\\ 1\\end{matrix} \\right)  \\right\\Vert  } \\right\\}  =\\left\\{ \\frac{1}{\\sqrt{30} } \\left( \\begin{matrix}5\\\\ -2\\\\ 1\\end{matrix} \\right)  ,\\frac{1}{\\sqrt{5} } \\left( \\begin{matrix}0\\\\ 1\\\\ 2\\end{matrix} \\right)  ,\\frac{1}{\\sqrt{6} } \\left( \\begin{matrix}-1\\\\ -2\\\\ 1\\end{matrix} \\right)  \\right\\}$$\n",
    "<p style=\"text-align: right;\">$(3.219)$</p>\n",
    "\n",
    "Calculamos ahora $\\mathbf{A}\\mathbf{v}_{i}$ para $i=1,2,3$:\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{A} \\mathbf{v}_{1} &=&\\frac{1}{\\sqrt{30} } \\left( \\begin{matrix}1&0&1\\\\ -2&1&0\\end{matrix} \\right)  \\left( \\begin{matrix}5\\\\ -2\\\\ 1\\end{matrix} \\right)  =\\frac{1}{\\sqrt{30} } \\left( \\begin{matrix}6\\\\ -12\\end{matrix} \\right)  =\\left( \\begin{matrix}6/\\sqrt{30} \\\\ -12/\\sqrt{30} \\end{matrix} \\right)  \\\\ \\mathbf{A} \\mathbf{v}_{2} &=&\\frac{1}{\\sqrt{5} } \\left( \\begin{matrix}1&0&1\\\\ -2&1&0\\end{matrix} \\right)  \\left( \\begin{matrix}0\\\\ 1\\\\ 2\\end{matrix} \\right)  =\\frac{1}{\\sqrt{5} } \\left( \\begin{matrix}2\\\\ 1\\end{matrix} \\right)  =\\left( \\begin{matrix}2/\\sqrt{5} \\\\ 1/\\sqrt{5} \\end{matrix} \\right)  \\\\ \\mathbf{A} \\mathbf{v}_{3} &=&\\frac{1}{\\sqrt{6} } \\left( \\begin{matrix}1&0&1\\\\ -2&1&0\\end{matrix} \\right)  \\left( \\begin{matrix}-1\\\\ -2\\\\ 1\\end{matrix} \\right)  =\\frac{1}{\\sqrt{6} } \\left( \\begin{matrix}0\\\\ 0\\end{matrix} \\right)  =\\left( \\begin{matrix}0\\\\ 0\\end{matrix} \\right)  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.220)$</p>\n",
    "\n",
    "Como hay dos valores singulares no nulos, calculamos\n",
    "\n",
    "$$\\mathbf{u}_{1} =\\frac{1}{\\sigma_{1} } \\mathbf{A} \\mathbf{v}_{1} =\\frac{1}{\\sqrt{6} } \\frac{1}{\\sqrt{30} } \\left( \\begin{matrix}1&0&1\\\\ -2&1&0\\end{matrix} \\right)  \\left( \\begin{matrix}5\\\\ -2\\\\ 1\\end{matrix} \\right)  =\\left( \\begin{matrix}1/\\sqrt{5} \\\\ -2/\\sqrt{5} \\end{matrix} \\right)  \\wedge \\mathbf{u}_{2} =\\frac{1}{\\sigma_{2} } \\mathbf{A} \\mathbf{v}_{2} =\\frac{1}{\\sqrt{5} } \\left( \\begin{matrix}1&0&1\\\\ -2&1&0\\end{matrix} \\right)  \\left( \\begin{matrix}0\\\\ 1\\\\ 2\\end{matrix} \\right)  =\\left( \\begin{matrix}2/\\sqrt{5} \\\\ 1/\\sqrt{5} \\end{matrix} \\right)  $$\n",
    "<p style=\"text-align: right;\">$(3.221)$</p>\n",
    "\n",
    "Notemos que $\\beta =\\left\\{ \\mathbf{u}_{1} ,\\mathbf{u}_{2} \\right\\}  =\\left\\{ \\left( \\begin{matrix}1/\\sqrt{5} \\\\ -2/\\sqrt{5} \\end{matrix} \\right)  ,\\left( \\begin{matrix}2/\\sqrt{5} \\\\ 1/\\sqrt{5} \\end{matrix} \\right)  \\right\\}$ es una base ortonormal de $\\mathbb{R}^{2}$. Por lo tanto,\n",
    "\n",
    "$$\\mathbf{A} =\\underbrace{\\left( \\begin{array}{rr}\\displaystyle \\frac{1}{\\sqrt{5} } &\\displaystyle \\frac{2}{\\sqrt{5} } \\\\ -\\displaystyle \\frac{2}{\\sqrt{5} } &\\displaystyle \\frac{1}{\\sqrt{5} } \\end{array} \\right)  }_{\\mathbf{U} } \\underbrace{\\left( \\begin{matrix}\\sqrt{6} &0&0\\\\ 0&1&0\\end{matrix} \\right)  }_{\\mathbf{\\Sigma } } \\underbrace{\\left( \\begin{array}{rrr}\\displaystyle \\frac{5}{\\sqrt{30} } &0&-\\displaystyle \\frac{1}{\\sqrt{6} } \\\\ -\\displaystyle \\frac{2}{\\sqrt{30} } &\\displaystyle \\frac{1}{\\sqrt{5} } &-\\displaystyle \\frac{2}{\\sqrt{6} } \\\\ \\displaystyle \\frac{1}{\\sqrt{30} } &\\displaystyle \\frac{2}{\\sqrt{5} } &\\displaystyle \\frac{1}{\\sqrt{6} } \\end{array} \\right)  }_{\\mathbf{V}^{\\top } } $$\n",
    "<p style=\"text-align: right;\">$(3.222)$</p>\n",
    "\n",
    "es la descomposici√≥n en valores singulares buscada. ‚óºÔ∏é\n",
    "\n",
    "**Ejemplo 3.25 ‚Äì Implementaci√≥n *high-level* de la descomposici√≥n SVD en <font color='purple'>Numpy</font>:** La librer√≠a <font color='purple'>Numpy</font> nos provee con una implementaci√≥n bastante eficiente de la descomposici√≥n en valores singulares por medio de su m√≥dulo `numpy.linalg`. Puntualmente, la funci√≥n `svd()` permite obtener tal descomposici√≥n sobre cualquier arreglo bidimensional que represente una matriz.\n",
    "\n",
    "Consideremos la matriz del ejemplo anterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "63a6078d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos la matriz A.\n",
    "A = np.array([\n",
    "    [1, 0, 1],\n",
    "    [-2, 1, 0],\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99aac20",
   "metadata": {},
   "source": [
    "La funci√≥n `svd()` requiere como √∫nico argumento la matriz sobre la cual deseamos operar. Como resultado, se retorna una tupla con las matrices `U`, `S` y `V_t`, que representan las componentes de la descomposici√≥n propiamente tal. No obstante, debemos notar que `S` no es la matriz de diagonal $\\mathbf{\\Sigma}$ que se usa en la descomposici√≥n, sino un arreglo unidimensional donde se almacenan √∫nicamente los valores singulares de `A`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7761e085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos la descomposici√≥n en valores singulares de A.\n",
    "U, S, V_t = linalg.svd(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4ba65752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.447,  0.894],\n",
       "       [-0.894,  0.447]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectores singulares por la izquierda.\n",
    "U.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "44ab57f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.913, -0.365,  0.183],\n",
       "       [-0.   ,  0.447,  0.894],\n",
       "       [-0.408, -0.816,  0.408]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectores singulares por la derecha.\n",
    "V_t.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0acaf69b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.449, 1.   ])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Valores singulares de A.\n",
    "S.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b8986167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de valores singulares.\n",
    "E = np.eye(M=V_t.shape[0], N=U.shape[0]) * S.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ca0ac9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recorstruimos la matriz A a partir de la descomposici√≥n SVD.\n",
    "A =(U @ E @ V_t).round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8cc597bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  1.],\n",
       "       [-2.,  1.,  0.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y la imprimimos en pantalla.\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c076d14",
   "metadata": {},
   "source": [
    "Y ah√≠ lo tenemos. Se trata de una implementaci√≥n eficiente y muy √∫til en la construcci√≥n de modelos de alta complejidad y que, por supuesto, nos ahorra el enorme trabajo manual que hab√≠amos hecho previamente. ‚óºÔ∏é"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "87298cce",
   "metadata": {},
   "source": [
    "## Aproximaci√≥n matricial.\n",
    "Hemos considerado la descomposici√≥n en valores singulares como una forma de factorizar una matriz $\\mathbf{A}\\in \\mathbb{R}^{m\\times n}$ en t√©rminos del producto entre tres matrices $\\mathbf{U}\\in \\mathbb{R}^{m\\times m}$, $\\mathbf{\\Sigma}\\in \\mathbb{R}^{m\\times n}$ y $\\mathbf{V}\\in \\mathbb{R}^{n\\times n}$, donde $\\mathbf{U}$ y $\\mathbf{V}$ son matrices ortogonales y $\\mathbf{\\Sigma}$ contiene los valores singulares de $\\mathbf{A}$ en su diagonal. En vez de realizar la descomposici√≥n SVD completa, ahora investigaremos c√≥mo dicha descomposici√≥n nos permite representar una matriz $\\mathbf{A}$ como una suma de matrices m√°s simples (siendo ‚Äúm√°s simples‚Äù por tener un rango menor que el rango de $\\mathbf{A}$), el cual tiende a conformar un esquema de aproximaci√≥n matricial que resulta mucho menos costoso de calcular que la descomposici√≥n SVD completa.\n",
    "\n",
    "Podemos construir una matriz de rango 1, $\\mathbf{A}_{i}\\in \\mathbb{R}^{m\\times n}$ como\n",
    "\n",
    "$$\\mathbf{A}_{i}=\\mathbf{u}_{i} \\mathbf{v}_{i}^{\\top}$$\n",
    "<p style=\"text-align: right;\">$(3.223)$</p>\n",
    "\n",
    "La cual est√° conformada por el producto exterior entre el $i$-√©simo vector columna de $\\mathbf{U}$ con $i$-√©simo vector columna de $\\mathbf{V}$. \n",
    "\n",
    "La Fig. (3.9) muestra una imagen de Stonehenge, la cual est√° constituida por una matriz $\\mathbf{A}\\in \\mathbb{R}^{1432\\times 1910}$ (bajo la etiqueta *\"original image\"*), donde cada elemento de la matriz representa un determinado pixel con una cierta intensidad (y que es, por supuesto, un n√∫mero real). Usaremos esta imagen como ejemplo de lo que podemos hacer, en t√©rminos de la **compresi√≥n de informaci√≥n** y **representatividad**, utilizando como base la aproximaci√≥n matricial aprovechando la descomposici√≥n en valores singulares.\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"figures/fig_3_9.png\" width=\"800\"></p>\n",
    "<p style=\"text-align: center;\">Fig. (3.9): Procesamiento de una imagen de Stonhenge por medio de una descomposici√≥n SVD; (a) La imagen original corresponde a una matriz $\\mathbf{A}$ de 1432$\\times$1910 pixeles, donde cada valor de la matriz es un n√∫mero real $a_{ij}$ tal que $0\\leq a_{ij}\\leq 1$, donde 0 representa un color totalmente negro, y 1 un color totalmente blanco. De (b) a (f) se muestran matrices de rango 1 a 5, $\\mathbf{A}_{1},...,\\mathbf{A}_{5}$, y sus correspondientes valores singulares $\\sigma_{1},...,\\sigma_{5}$. La estructura de tipo grilla de cada matriz de rango 1 en esta imagen queda definida por el producto exterior de los vectores singulares por la izquierda y por la derecha ($\\mathbf{U}$ y $\\mathbf{V}$) (imagen tomada del fant√°stico libro \"Mathematics for Machine Learning\", (Deinsenroth, M., Faisal, A. & Soon Ong, C.; 2021))\n",
    "\n",
    "Una matriz $\\mathbf{A}\\in \\mathbb{R}^{m\\times n}$ de rango $r$ puede ser escrita como una **suma de matrices de rango 1**, digamos $\\mathbf{A}_{i}$, de manera tal que\n",
    "\n",
    "$$\\mathbf{A} =\\sum^{r}_{i=1} \\sigma_{i} \\mathbf{u}_{i} \\mathbf{v}^{\\top }_{i} =\\sum^{r}_{i=1} \\sigma_{i} \\mathbf{A}_{i}$$\n",
    "<p style=\"text-align: right;\">$(3.224)$</p>\n",
    "\n",
    "Donde las matrices $\\mathbf{A}_{i}$ resultantes de los productos exteriores se ponderan por el $i$-√©simo valor singular $\\sigma_{i}$. Podemos observar por qu√© se cumple la ecuaci√≥n (3.224): La estructura diagonal de la matriz de valores singulares $\\mathbf{\\Sigma}$ multiplica √∫nicamente los correspondientes vectores $\\mathbf{u}_{i}\\mathbf{v}_{i}^{\\top}$ y los escala por los correspondientes valores singulares $\\sigma_{i}$. Todos los t√©rminos $\\sum\\nolimits_{ij} \\sigma_{i} \\mathbf{u}_{i} \\mathbf{v}^{\\top }_{i}$ se anulan para $i\\neq j$, porque $\\mathbf{\\Sigma}$ es una matriz diagonal. Cualquier t√©rmino para el cual $i>r$ tambi√©n se anula, porque los correspondientes valores singulares son tambi√©n nulos.\n",
    "\n",
    "En la ecuaci√≥n (3.223) introducimos las matrices de rango 1, denotadas como $\\mathbf{A}_{i}$. En este caso, sumamos hasta la $r$-√©sima matriz de rango 1 para obtener la matriz $\\mathbf{A}$ de rango $r$. Si la suma no se realiza sobre las $r$ matrices $\\mathbf{A}_{i}$, sino que hasta un valor $k<r$, obtenemos lo que llamamos una **aproximaci√≥n de rango $k$** para la matriz $\\mathbf{A}$, y que denotamos como $\\widehat{\\mathbf{A} } \\left( k\\right)$. Es decir,\n",
    "\n",
    "$$\\widehat{\\mathbf{A} } \\left( k\\right)  :=\\sum^{k}_{i=1} \\sigma_{i} \\mathbf{u}_{i} \\mathbf{v}^{\\top }_{i} =\\sum^{k}_{i=1} \\sigma_{i} \\mathbf{A}_{i}$$\n",
    "<p style=\"text-align: right;\">$(3.225)$</p>\n",
    "\n",
    "Donde $\\rho(\\mathbf{A})=k$. La Fig. (3.10) permite observar el efecto de las aproximaciones de bajo rango $\\widehat{\\mathbf{A} } \\left( k\\right)$ para la imagen original de Stonehenge representada, en este caso, por la matriz $\\mathbf{A}\\in \\mathbb{R}^{1432\\times 1910}$. La forma de estas rocas se vuelve incrementalmente visible y claramente reconocible en la aproximaci√≥n de rango $k=5$. Mientras que la imagen original requiere un total de 1432$\\times$1910 = 2735120 vectores para su representaci√≥n, la aproximaci√≥n de rango 5 requiere almacenar √∫nicamente 5 vectores singulares por la izquierda y por la derecha, respectivamente, conforme filas y columnas; es decir, un total de 5$\\times$(1432 + 1910 + 1) = 16715 n√∫meros. Aproximadamente un 0.6% del total de valores necesarios en la matriz original.\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"figures/fig_3_10.png\" width=\"800\"></p>\n",
    "<p style=\"text-align: center;\">Fig. (3.10): 5 aproxoimaciones de baja densidad para la matriz $\\mathbf{A}$ que representa la imagen de Stonehenge (imagen tomada del fant√°stico libro \"Mathematics for Machine Learning\", (Deinsenroth, M., Faisal, A. & Soon Ong, C.; 2021))\n",
    "    \n",
    "Tom√©monos un tiempo para entender lo importante que fue lo anterior: Usando un 0.6% de la data original, podemos representar la imagen a un nivel tal que es posible reconocer la formaci√≥n rocosa de Stonehenge sin mayores problemas. Con aproximaciones ligeramente de mayor densidad (digamos, un 10% de la data original), podemos *comprimir* la imagen sin perder demasiada informaci√≥n relevante, lo que es de extrema importancia en el contexto de los llamados **modelos de reducci√≥n de dimensionalidad**: Muchas veces, cuando disponemos de conjuntos de datos con demasiadas variables, muchas de ellas son **redundantes** o, en realidad, comunican **poca informaci√≥n**.\n",
    "    \n",
    "Para medir la diferencia (error) entre $\\mathbf{A}$ y su aproximaci√≥n de rango $k$, podemos utilizar un tipo particular de norma de matrices, que requiere de una definici√≥n particular.\n",
    "\n",
    "**<font color='blue'>Definici√≥n 3.19 ‚Äì Norma espectral de una matriz:</font>** Para $\\mathbf{x} \\in \\mathbb{R}^{n} -\\left\\{ \\mathbf{0} \\right\\}$, definimos la **norma espectral** de la matriz $\\mathbf{A}\\in \\mathbb{R}^{m\\times n}$ como\n",
    "    \n",
    "$$\\left\\Vert \\mathbf{A} \\right\\Vert_{2}  :=\\max_{\\mathbf{x} } \\left( \\frac{\\left\\Vert \\mathbf{A} \\mathbf{x} \\right\\Vert_{2}  }{\\left\\Vert \\mathbf{x} \\right\\Vert_{2}  } \\right)  =\\sqrt{\\max_{j} \\left( \\lambda_{j} \\right)  \\mathbf{A}^{\\top } \\mathbf{A} }$$\n",
    "<p style=\"text-align: right;\">$(3.225)$</p>\n",
    "\n",
    "Donde $\\max_{j} \\left( \\lambda_{j} \\right)$ hace referencia al mayor de los autovalores de $\\mathbf{A}$. De esta manera, la norma espectral de una matriz nos permite entender qu√© tan largo puede hacerse un vector, como m√°ximo, cuando lo multiplicamos por la matriz $\\mathbf{A}$.\n",
    "\n",
    "La definici√≥n (3.19) motiva los siguientes teoremas.\n",
    "\n",
    "**<font color='crimson'>Teorema 3.17:</font>** *La norma espectral de una matriz $\\mathbf{A}\\in \\mathbb{R}^{m\\times n}$ es igual a su valor singular de mayor magnitud.* ‚óÜ\n",
    "\n",
    "**<font color='crimson'>Teorema 3.18 ‚Äì Eckart-Young:</font>** *Consideremos la matriz $\\mathbf{A}\\in \\mathbb{R}^{m\\times n}$ de rango $\\rho(\\mathbf{A})=r$, y sea $\\mathbf{B}\\in \\mathbb{R}^{m\\times n}$ otra matriz de rango $\\rho(\\mathbf{B})=k$. Entonces, para todo $k\\neq r$, con $\\widehat{\\mathbf{A} } \\left( k\\right)  =\\sum\\nolimits^{k}_{i=1} \\sigma_{i} \\mathbf{u}_{i} \\mathbf{v}^{\\top }_{i}$, se tiene que*\n",
    "\n",
    "$$\\widehat{\\mathbf{A} } \\left( k\\right)  =\\mathrm{argmin}_{\\rho \\left( \\mathbf{B} \\right)  =k} \\left\\Vert \\mathbf{A} -\\mathbf{B} \\right\\Vert_{2}  \\  ;\\  \\left\\Vert \\mathbf{A} -\\widehat{\\mathbf{A} } \\left( k\\right)  \\right\\Vert_{2}  =\\sigma_{k+1}$$\n",
    "<p style=\"text-align: right;\">$(3.226)$</p>\n",
    "‚óÜ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
