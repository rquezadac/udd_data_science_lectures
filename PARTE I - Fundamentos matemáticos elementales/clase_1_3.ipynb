{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3775495",
   "metadata": {},
   "source": [
    "# CLASE 1.3: Descomposiciones matriciales\n",
    "\n",
    "## Introducci√≥n.\n",
    "En las secciones anteriores estudiamos algunas formas de manipular y obtener ciertas m√©tricas para los vectores, proyecciones de esos vectores con respecto a determinados subespacios vectoriales y transformaciones lineales. Las aplicaciones y transformaciones que permiten operar con vectores pueden ser convenientemente descritas por medio de matrices. Adem√°s, la mayor√≠a de los conjuntos de datos *bien comportados* que podemos encontrar en el mundo real vienen especificados en estructuras que pueden ser arregladas y/o representadas igualmente por medio de matrices. Por ejemplo, las filas de estos conjuntos de datos suelen representar **registros** u **observaciones** (como personas, fechas, unidades, entre otras) y las columnas suelen representar diferentes **atributos** para cada fila (como edad, altura, propiedades extensivas de alg√∫n fen√≥meno o el valor de alguna variable en el tiempo). En esta secci√≥n, presentaremos tres aspectos relativos a las matrices: C√≥mo **resumirlas**, como **descomponerlas** y como utilizar tales descomposiciones para construir **aproximaciones** para determinadas matrices.\n",
    "\n",
    "A diferencia de la secci√≥n anterior, en √©sta volveremos a escribir algo de c√≥digo, a fin de corresponder algunos conceptos esenciales con librer√≠as tales como <font color='purple'>Numpy</font> o <font color='purple'>Scipy</font>. No ser√° demasiado c√≥digo, pero s√≠ el suficiente para darle algo de sentido pr√°ctico a los conceptos que desarrollaremos desde la perspectiva computacional."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aff586d9",
   "metadata": {},
   "source": [
    "## Determinantes.\n",
    "\n",
    "### Un interludio previo.\n",
    "El concepto de determinante corresponde a otro de los elementos m√°s importantes del √°lgebra lineal. Corresponde a un objeto matem√°tico que es importante en el an√°lisis y soluci√≥n de sistemas de ecuaciones lineales (los mismos que vimos en el inicio de la [clase 1.1](https://github.com/rquezadac/udd_data_science_lectures/blob/main/PARTE%20I%20-%20Fundamentos%20matem%C3%A1ticos%20elementales/clase_1_1.ipynb)) y que puede expresarse por medio de una funci√≥n, denominada como **funci√≥n determinante**, y que permite aplicar cualquier matriz cuadrada de orden $n$ en el cuerpo $\\mathbb{K}$ donde sus elementos est√°n definidos. Dicha funci√≥n, para una matriz $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$, se denota como $\\det(\\mathbf{A})=|\\mathbf{A}|$, y es tal que $\\left| \\  \\cdot \\  \\right|  :\\mathbb{K}^{n} \\times \\mathbb{K}^{n} \\longrightarrow \\mathbb{K}$, pudi√©ndose escribir el determinante de la matriz $\\mathbf{A}$ como\n",
    "\n",
    "$$\\mathbf{A} =\\left\\{ a_{ij}\\in \\mathbb{K} \\right\\}  =\\left( \\begin{matrix}a_{11}&a_{12}&\\cdots &a_{1n}\\\\ a_{21}&a_{22}&\\cdots &a_{2n}\\\\ \\vdots &\\vdots &\\ddots &\\vdots \\\\ a_{n1}&a_{n2}&\\cdots &a_{nn}\\end{matrix} \\right)  \\in \\mathbb{K}^{n\\times n} \\wedge \\det \\left( \\mathbf{A} \\right)  =\\left| \\begin{matrix}a_{11}&a_{12}&\\cdots &a_{1n}\\\\ a_{21}&a_{22}&\\cdots &a_{2n}\\\\ \\vdots &\\vdots &\\ddots &\\vdots \\\\ a_{n1}&a_{n2}&\\cdots &a_{nn}\\end{matrix} \\right|  \\in \\mathbb{K}$$\n",
    "<p style=\"text-align: right;\">$(3.1)$</p>\n",
    "\n",
    "**Ejemplo 3.1:** Comenzaremos a motivar el estudio de los determinantes explorando la posibilidad de que una matriz cuadrada, digamos $\\mathbf{A}\\in \\mathbb{R}^{n\\times n}$, sea **invertible**. Para los casos de menor dimensi√≥n, ya conocemos los casos que aseguran que $\\mathbf{A}$ cumpla con esta condici√≥n. Por ejemplo, si $\\mathbf{A}\\in \\mathbb{R}^{1\\times 1}$ (es decir, $\\mathbf{A}$ es un escalar), sabemos que $\\mathbf{A}=a\\ \\Longrightarrow \\mathbf{A}^{-1}=1/a$, lo que implica que $\\mathbf{A}$ tiene una inversa siempre que $a\\neq 0$. Para el caso de matrices de $2\\times 2$, sabemos que la inversa $\\mathbf{A}^{-1}$ cumple con la condici√≥n de que $\\mathbf{A}\\mathbf{A}^{-1}=\\mathbf{I}_{2}$. De esta manera, podemos escribir\n",
    "\n",
    "$$\\mathbf{A} =\\left( \\begin{matrix}a_{11}&a_{12}\\\\ a_{21}&a_{22}\\end{matrix} \\right)  \\Longrightarrow \\mathbf{A} \\mathbf{A}^{-1} =\\mathbf{I}_{2} \\Longleftrightarrow \\mathbf{A}^{-1} =\\frac{1}{a_{11}a_{22}-a_{12}a_{21}} \\left( \\begin{matrix}a_{22}&-a_{12}\\\\ -a_{21}&a_{11}\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.2)$</p>\n",
    "\n",
    "Por lo tanto, $\\mathbf{A}$ es invertible si y s√≥lo si $a_{11}a_{22}-a_{12}a_{21}\\neq 0$. Para matrices de $2\\times 2$, dicha cantidad corresponde al **determinante** de la matriz respectiva. De esta manera, para la matriz $\\mathbf{A} =\\left\\{ a_{ij}\\right\\}  \\in \\mathbb{R}^{2\\times 2}$, su determinante se define como\n",
    "\n",
    "$$\\det \\left( \\mathbf{A} \\right)  =\\det \\left( \\begin{matrix}a_{11}&a_{12}\\\\ a_{21}&a_{22}\\end{matrix} \\right)  =\\left| \\begin{matrix}a_{11}&a_{12}\\\\ a_{21}&a_{22}\\end{matrix} \\right|  =a_{11}a_{22}-a_{12}a_{21}$$\n",
    "<p style=\"text-align: right;\">$(3.3)$</p>\n",
    "‚óºÔ∏é\n",
    "\n",
    "El ejemplo (3.1) permite establecer un hecho que puede ser generalizado a conjuntos de mayor dimensi√≥n: Una matriz es invertible siempre que su determinante no sea nulo. Formalicemos este hecho mediante un teorema.\n",
    "\n",
    "**<font color='crimson'>Teorema 3.1 ‚Äì Existencia de matriz inversa:</font>** *Sea $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$ una matriz cuadrada con elementos en el cuerpo $\\mathbb{K}$. Entonces $\\mathbf{A}$ se dir√° **invertible** o **no singular** (es decir, existe la matriz inversa $\\mathbf{A}^{-1}$) si y solo si $\\det(\\mathbf{A})\\neq 0$.*\n",
    "‚óÜ\n",
    "\n",
    "Ya disponemos de una expresi√≥n cerrada que permite calcular el determinante de cualquier matriz de dimensi√≥n $2\\times 2$. Sin embargo, no es tan sencillo generalizar dicho c√°lculo para dimensiones superiores. Por ejemplo, para el caso de matrices de $3\\times 3$, es com√∫n el c√°lculo de sus determinantes mediante la llamada regla de Sarrus:\n",
    "\n",
    "$$\\det \\left( \\begin{matrix}a_{11}&a_{12}&a_{13}\\\\ a_{21}&a_{22}&a_{23}\\\\ a_{31}&a_{32}&a_{33}\\end{matrix} \\right)  =a_{11}a_{22}a_{33}+a_{21}a_{32}a_{13}+a_{31}a_{12}a_{23}-a_{31}a_{22}a_{13}-a_{11}a_{32}a_{23}-a_{21}a_{12}a_{33}$$\n",
    "<p style=\"text-align: right;\">$(3.4)$</p>\n",
    "\n",
    "Si bien, en un principio, la regla de Sarrus parece una f√≥rmula complicada de entender, √©sta no es m√°s que un recurso mnemot√©cnico, ya que los triples productos involucrados en la f√≥rmula y sus signos guardan relaci√≥n con las diagonales (y subdiagonales) presentes en la matriz correspondiente, como se observa en el esquema de la Fig. (3.1)\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"figures/fig_3_1.png\" width=\"650\"></p>\n",
    "<p style=\"text-align: center;\">Fig. (3.1): Esquema que ilustra c√≥mo opera la regla de Sarrus</p>\n",
    "\n",
    "Recordemos que, al resolver sistemas de ecuaciones lineales, nuestro objetivo era transformar la matriz ampliada de un sistema en una tal que s√≥lo tuviera elementos no nulos en su regi√≥n superior derecha (matriz triangular superior), aunque tambi√©n es posible operar para llegar al caso opuesto, donde la matriz resultante tenga elementos no nulos en su regi√≥n inferior izquierda (matriz triangular inferior). Este tipo de matrices fueron formalizadas previamente en la definici√≥n (1.7).\n",
    "\n",
    "Para una matriz triangular, digamos $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$, definimos su determinante como\n",
    "\n",
    "$$\\det \\left( \\mathbf{A} \\right)  =\\prod^{n}_{i=1} a_{ii}$$\n",
    "<p style=\"text-align: right;\">$(3.5)$</p>\n",
    "\n",
    "Donde $a_{ii}$ es el correspondiente elemento relativo a la diagonal principal de la matriz $\\mathbf{A}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e254522",
   "metadata": {},
   "source": [
    "### Interpretaci√≥n geom√©trica del determinante.\n",
    "Si una matriz $\\mathbf{A}\\in \\mathbb{R}^{n\\times n}$ tiene elementos $a_{ij}\\in \\mathbb{R}$, entonces puede ser utilizada para representar dos transformaciones lineales. Una que aplica la base can√≥nica de $\\mathbb{R}^{n}$ a las filas de $\\mathbf{A}$, y otra que aplica la misma base a las columnas de $\\mathbf{A}$. Cualquiera sea el caso, si la matriz $\\mathbf{A}$ es de $2\\times 2$, las im√°genes de cada uno de los vectores de la base can√≥nica de $\\mathbb{R}^{2}$ forman un paralel√≥gramo que representa la imagen del cuadrado unitario bajo la transformaci√≥n lineal respectiva, y cuyos v√©rtices se corresponden con combinaciones de los elementos de $\\mathbf{A}$, como se observa en la Fig. (3.2a).\n",
    "\n",
    "Si $\\mathbf{A} =\\left\\{ {}a_{ij}\\right\\}  \\in \\mathbb{R}^{2\\times 2}$ es la matriz que conforma los v√©rtices del paralel√≥gramo en la Fig. (3.2a), se tendr√° que el √°rea encerrada por el mismo ser√° igual a $\\det \\left( \\mathbf{A} \\right)  =a_{11}a_{22}-a_{12}a_{21}$. Para mostrar este resultado, podemos considerar que los elementos de la matriz $\\mathbf{A}$ corresponden a vectores que representan los v√©rtices del paralel√≥gramo. Si uno de los v√©rtices es el origen del sistema de coordenadas, los vectores $\\mathbf{a} =\\left( a_{11},a_{12}\\right)$ y $\\mathbf{b} =\\left( a_{21},a_{22}\\right)$ ser√°n los v√©rtices m√°s cercanos al origen, mientras que el v√©rtice opuesto ser√° igual a la suma $\\mathbf{a} +\\mathbf{b} =\\left( a_{11}+a_{21},a_{12}+a_{22}\\right)$. El √°rea del paralel√≥gramo puede expresarse igualmente como $\\left\\Vert \\mathbf{a} \\right\\Vert  \\left\\Vert \\mathbf{b} \\right\\Vert  \\mathrm{sen} \\left( \\theta \\right)$, donde $\\theta$ es el √°ngulo formado por los vectores $\\mathbf{a}$ y $\\mathbf{b}$. Si consideramos la proyecci√≥n ortogonal del vector $\\mathbf{a}$ sobre $\\mathbf{b}$ (que llamamos $\\pi_{\\mathbf{b}}(\\mathbf{a})$), podemos escribir\n",
    "\n",
    "$$\\mathrm{Area} =\\left\\Vert \\mathbf{a} \\right\\Vert  \\left\\Vert \\mathbf{b} \\right\\Vert  \\mathrm{sen} \\left( \\theta \\right)  =\\left\\Vert \\pi_{\\mathbf{b} } \\left( \\mathbf{a} \\right)  \\right\\Vert  \\left\\Vert \\mathbf{b} \\right\\Vert  \\cos \\left( \\frac{\\pi }{2} -\\theta \\right)  =a_{11}a_{12}-a_{12}a_{21}=\\det \\left( \\mathbf{A} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.6)$</p>\n",
    "\n",
    "La interpretaci√≥n geom√©trica anterior puede extenderse al caso de matrices de $3\\times 3$, considerando en este caso un paralelep√≠pedo generado por las submatrices columna que generan la matriz completa $\\mathbf{A} =\\left\\{ a_{ij}\\right\\}  \\in \\mathbb{R}^{3\\times 3}$. Si tales submatrices son representadas como $\\mathbf{r}_{1}$, $\\mathbf{r}_{2}$ y $\\mathbf{r}_{3}$, entonces el volumen del paralelep√≠pedo es igual a $V=\\det \\left( \\left( \\mathbf{r}_{1} ,\\mathbf{r}_{2} ,\\mathbf{r}_{3} \\right)  \\right)  =\\det \\left( \\mathbf{A} \\right)$, tal y como se ilustra en la Fig (3.2b).\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"figures/fig_3_2.png\" width=\"900\"></p>\n",
    "<p style=\"text-align: center;\">Fig. (3.2): (a) Esquema que muestra c√≥mo el determinante de una matriz de $2\\times 2$ permite transformar un cuadrado unitario en un paralel√≥gramo cuyos v√©rtices son las productos que componen dicho determinante y su √°rea ser√° igual al valor de tal determinante; (b) Misma transformaci√≥n para el caso de una matriz de $3\\times 3$. En este caso, la transformaci√≥n se aplica sobre un paralelep√≠pedo, obteni√©ndose un trapezoedro cuyo volumen es igual al correspondiente determinante</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "07edc8da",
   "metadata": {},
   "source": [
    "### Propiedades de los determinantes.\n",
    "El c√°lculo de un determinante para una matriz arbitraria $\\mathbf{A} =\\left\\{ a_{ij}\\right\\}  \\in \\mathbb{R}^{n\\times n}$ requiere de un algoritmo generalizado para poder resolver los casos en los cuales $n>3$. Existen varios procedimientos para ello, siendo indudablemente el m√°s popular el **m√©todo de Laplace**, que definiremos a continuaci√≥n.\n",
    "\n",
    "**<font color='blue'>Definici√≥n 3.1 ‚Äì Determinante:</font>** Sea $\\mathbf{A} =\\left\\{ a_{ij}\\right\\}  \\in \\mathbb{R}^{n\\times n}$. Definimos el **determinante** de la matriz $\\mathbf{A}$ como\n",
    "\n",
    "1. Respecto a la columna $j$: $\\det \\left( \\mathbf{A} \\right)  =\\sum^{n}_{k=1} \\left( -1\\right)^{k+j}  a_{kj}\\det \\left( \\mathbf{A}_{kj} \\right)$.\n",
    "2. Respecto a la fila $j$: $\\det \\left( \\mathbf{A} \\right)  =\\sum^{n}_{k=1} \\left( -1\\right)^{k+j}  a_{jk}\\det \\left( \\mathbf{A}_{jk} \\right)$.\n",
    "\n",
    "En la f√≥rmulas anteriores, conocidas en la pr√°ctica como **expansiones de Laplace**, $\\mathbf{A}_{jk}$ corresponde a la submatriz resultante de eliminar de $\\mathbf{A}$ la fila $j$ y la columna $k$ y se denomina como **menor complementario** en la posici√≥n $(j, k)$, mientras que el n√∫mero real $\\triangle_{jk} =\\left( -1\\right)^{k+j}  a_{jk}$ es llamado **cofactor** asociado al $jk$-√©simo elemento de la matriz $\\mathbf{A}$.\n",
    "\n",
    "Resulta sencillo darnos cuenta de que, si bien las expansiones de Laplace nos permiten obtener una f√≥rmula cerrada para el c√°lculo del determinante de cualquier matriz cuadrada, su tiempo de ejecuci√≥n y complejidad computacional escala enormemente con la dimensi√≥n de la matriz para la cual queremos calcular su determinante. Por esa raz√≥n, en t√©rminos algebraicos, es mejor considerar ciertas propiedades que se desprenden directamente de la definici√≥n que hemos ido construyendo del determinante a fin de disponer de m√©todos m√°s efectivos para su c√°lculo en dimensiones superiores (considerando, adem√°s, las transformaciones elementales sobre matrices que hemos aprendido previamente). Vamos, por tanto, a desarrollar tales propiedades:\n",
    "\n",
    "- **(P1) ‚Äì Invariancia ante la transposici√≥n:** Si $\\mathbf{A} =\\left\\{ a_{{}ij}\\right\\}  \\in \\mathbb{R}^{n\\times n}$, entonces, de la definici√≥n (3.1), se tiene que $\\det(\\mathbf{A})=\\det(\\mathbf{A}^{\\top})$, ya que $\\det \\left( \\mathbf{A} \\right)  =\\sum^{n}_{k=1} \\triangle_{jk} \\det \\left( \\mathbf{A}_{jk} \\right)  =\\sum^{n}_{s=1} \\triangle_{sj} \\det \\left( \\mathbf{A}_{sj} \\right)$.\n",
    "- **(P2) ‚Äì Columna o fila nula:** Si $\\mathbf{A} =\\left\\{ a_{{}ij}\\right\\}  \\in \\mathbb{R}^{n\\times n}$ posee una columna o fila nula (conformada √∫nicamente por elementos iguales a cero), entonces $\\det(\\mathbf{A})=0$.\n",
    "- **(P3) ‚Äì Determinante de un producto de matrices:** Si $\\mathbf{A},\\mathbf{B}\\in \\mathbb{R}^{n\\times n}$, entonces $\\det(\\mathbf{A}\\mathbf{B})=\\det(\\mathbf{A})\\det(\\mathbf{B})$.\n",
    "- **(P4) ‚Äì Determinante de la matriz inversa:** Si $\\mathbf{A} =\\left\\{ a_{{}ij}\\right\\}  \\in \\mathbb{R}^{n\\times n}$ es una matriz no singular, entonces $\\det \\left( \\mathbf{A}^{-1} \\right)  =1/\\det \\left( \\mathbf{A} \\right)$.\n",
    "- **(P5) ‚Äì Invariancia ante operaciones elementales:** Cualquier matriz $\\mathbf{A}\\in \\mathbb{R}^{n\\times n}$ mantiene el valor de su determinante, aunque hayamos operado sobre ella mediante cualquier de las transformaciones elementales vistas en la [clase 1.1](https://github.com/rquezadac/udd_data_science_lectures/blob/main/PARTE%20I%20-%20Fundamentos%20matem%C3%A1ticos%20elementales/clase_1_1.ipynb).\n",
    "- **(P6) ‚Äì Escalamiento del determinante:** Sea $\\mathbf{A} =\\left\\{ a_{ij}\\right\\}  \\in \\mathbb{R}^{n\\times n}$ y $\\lambda \\in \\mathhb{R}$. Entonces $\\det \\left( \\lambda \\mathbf{A} \\right)  =\\lambda^{n} \\det \\left( \\mathbf{A} \\right)$.\n",
    "- **(P7) ‚Äì Columna o fila repetida:** Si una matriz $\\mathbf{A} =\\left\\{ a_{ij}\\right\\}  \\in \\mathbb{R}^{n\\times n}$ tiene columnas o filas repetidas (o, m√°s general, linealmente dependientes), entonces $\\det(\\mathbf{A})=0$. Esta propiedad generaliza **(P2)** y establece que toda matriz $\\mathbf{A}$ no singular tiene rango completo.\n",
    "\n",
    "**Ejemplo 3.2:** Calcularemos el determinante de la matriz $\\mathbf{A}\\in \\mathbb{R}^{5\\times 5}$, definida como\n",
    "\n",
    "$$\\mathbf{A} =\\left( \\begin{matrix}0&1&0&1&0\\\\ -1&a&0&0&0\\\\ 0&0&a&0&0\\\\ -1&0&0&a&0\\\\ 0&0&0&0&a\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.7)$</p>\n",
    "\n",
    "En efecto, aplicando la definici√≥n (3.1) y transformaciones elementales,\n",
    "\n",
    "$$\\begin{array}{rcl}\\det \\left( \\begin{matrix}0&1&0&1&0\\\\ -1&a&0&0&0\\\\ 0&0&a&0&0\\\\ -1&0&0&a&0\\\\ 0&0&0&0&a\\end{matrix} \\right)  &\\overbrace{=}^{\\mathrm{definicion} } &\\underbrace{a}_{\\mathrm{cofactor} \\  \\triangle_{55} } \\det \\left( \\begin{matrix}0&1&0&1\\\\ -1&a&0&0\\\\ 0&0&a&0\\\\ -1&0&0&a\\end{matrix} \\right)  \\\\ &\\overbrace{=}^{F_{42}\\left( -1\\right)  } &a\\det \\left( \\begin{matrix}0&1&0&1\\\\ -1&a&0&0\\\\ 0&0&a&0\\\\ 0&-a&0&a\\end{matrix} \\right)  \\\\ &\\overbrace{=}^{\\mathrm{definicion} } &\\underbrace{1}_{\\mathrm{cofactor} \\  \\triangle_{21} } \\cdot a\\det \\left( \\begin{matrix}1&0&1\\\\ 0&a&0\\\\ -a&0&a\\end{matrix} \\right)  \\\\ &\\overbrace{=}^{\\mathrm{definicion} } &a^{2}\\det \\left( \\begin{matrix}1&1\\\\ -a&a\\end{matrix} \\right)  \\overbrace{=}^{\\mathrm{definicion} } 2a^{3}\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.8)$</p>\n",
    "‚óºÔ∏é\n",
    "\n",
    "**Ejemplo 3.3 ‚Äì Los determinantes en <font color='purple'>Numpy</font>:** En librer√≠as de Python especializadas en el uso de arreglos vectorizados, como <font color='purple'>Numpy</font>, es razonable esperar que existan rutinas prefabricadas para el c√°lculo de determinantes. En particular, podemos usar la funci√≥n `det()`, del m√≥dulo de √°lgebra lineal `numpy.linalg()` para calcular el determinante de cualquier matriz expresada por medio de un arreglo bidimensional. Por ejemplo, si consideramos la matriz $\\mathbf{A}\\in \\mathbb{R}^{5\\times 5}$, definida como\n",
    "\n",
    "$$\\mathbf{A} =\\left( \\begin{matrix}0&-1&2&-1&0\\\\ 3&-1&2&2&0\\\\ 6&-1&0&0&9\\\\ 0&1&4&-5&9\\\\ 2&2&-4&5&-3\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.9)$</p>\n",
    "\n",
    "Podemos calcular su determinante f√°cilmente en <font color='purple'>Numpy</font> definiendo, primeramente, un arreglo bidimensional, digamos `A`, donde almacenamos esta matriz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eedebc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d87c5dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el arreglo en cuesti√≥n.\n",
    "A = np.array([\n",
    "    [0, -1, 2, -1, 0],\n",
    "    [3, -1, 2, 2, 0],\n",
    "    [6, -1, 0, 0, 9],\n",
    "    [0, 1, 4, -5, 9],\n",
    "    [2, 2, -4, 5, -3],\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd175bec",
   "metadata": {},
   "source": [
    "Y luego aplicando la funci√≥n `np.linalg.det()` para calcular su determinante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f968b07e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-162.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculamos el determinante de A (redondeado a 3 decimales).\n",
    "np.around(np.linalg.det(A), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941981cf",
   "metadata": {},
   "source": [
    "Vemos pues que no fue nada dif√≠cil calcular el determinante de una matriz de $5\\times 5$ en <font color='purple'>Numpy</font>. Sin embargo, tal y como comentamos previamente, el c√°lculo de determinantes corresponde a un esfuerzo computacional ostensiblemente grande y que escala enormemente a medida que aumentan las dimensiones de las matrices de inter√©s. Incluso trabajando con una librer√≠a muy eficiente como <font color='purple'>Numpy</font>, los tiempos de ejecuci√≥n pueden verse muy afectados. Para ejemplificar aquello, consideraremos el c√°lculo de los determinantes de cuatro matrices $\\mathbf{A}\\in \\mathbb{R}^{10\\times 10}$, $\\mathbf{B}\\in \\mathbb{R}^{100\\times 100}$, $\\mathbf{C}\\in \\mathbb{R}^{1000\\times 1000}$ y $\\mathbf{D}\\in \\mathbb{R}^{10000\\times 10000}$, las que representaremos mediante los arreglos bidimensionales `A`, `B`, `C` y `D`, y que estar√°n compuestas por n√∫meros reales uniformemente distribuidos entre 0 y 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c06dcc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos una semilla aleatoria fija.\n",
    "rng = np.random.default_rng(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "caff1fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos algunas matrices de distintos tama√±os.\n",
    "A = rng.random(size=(10, 10))\n",
    "B = rng.random(size=(100, 100))\n",
    "C = rng.random(size=(1000, 1000))\n",
    "D = rng.random(size=(10000, 10000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efadf9ab",
   "metadata": {},
   "source": [
    "Vamos a estimar el tiempo de ejecuci√≥n asociado al c√°lculo de los determinantes de estas matrices, a fin de observar qu√© tal escala con respecto al incremento en dimensionalidad de las mismas. Notemos que cada matriz es 10 veces m√°s grande que su antecesora:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67af2451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.34 ¬µs ¬± 83.7 ns per loop (mean ¬± std. dev. of 7 runs, 100,000 loops each)\n",
      "90 ¬µs ¬± 740 ns per loop (mean ¬± std. dev. of 7 runs, 10,000 loops each)\n",
      "8.37 ms ¬± 706 ¬µs per loop (mean ¬± std. dev. of 7 runs, 100 loops each)\n",
      "3.47 s ¬± 285 ms per loop (mean ¬± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.linalg.det(A)\n",
    "%timeit np.linalg.det(B)\n",
    "%timeit np.linalg.det(C)\n",
    "%timeit np.linalg.det(D)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34ff64a9",
   "metadata": {},
   "source": [
    "Podemos observar que el c√°lculo del determinante de `B` tiene un tiempo de ejecuci√≥n 12 veces mayor que el c√°lculo del determinante de `A`. El c√°lculo del determinante de `C` tiene un tiempo de ejecuci√≥n de aproximadamente unas 82 veces superior al del c√°lculo del determinante de `B` (y, por extensi√≥n, 1038 veces m√°s lento que el c√°lculo del determinante de `A`). Y el c√°lculo del determinante de `D` tiene un tiempo de ejecuci√≥n aproximadamente unas 445 veces m√°s lento que el c√°lculo del determinante de `C` (y, por extensi√≥n... ¬°es m√°s de 36000 veces m√°s lento que el c√°lculo del determinante de `B`, y m√°s de 460000 veces m√°s lento que el c√°lculo del determinante de `A`!). Esto definitivamente nos har√° pensarlo dos veces antes de calcular determinantes en el mundo real, donde resulta com√∫n vernos enfrentados a bases de datos con cientos de miles de registros. ‚óºÔ∏é \n",
    "\n",
    "**Ejemplo 3.4:** Vamos a demostrar que\n",
    "\n",
    "$$\\det \\left( \\begin{matrix}1&1&1\\\\ x&y&z\\\\ x^{2}&y^{2}&z^{2}\\end{matrix} \\right)  =\\left( x-y\\right)  \\left( y-z\\right)  \\left( z-x\\right)$$\n",
    "<p style=\"text-align: right;\">$(3.10)$</p>\n",
    "\n",
    "En efecto, utilizando transformaciones elementales, propiedades de los determinantes y la definici√≥n (3.1), tenemos que\n",
    "\n",
    "$$\\begin{array}{rcl}\\det \\left( \\begin{matrix}1&1&1\\\\ x&y&z\\\\ x^{2}&y^{2}&z^{2}\\end{matrix} \\right)  &\\overbrace{=}^{F_{21}\\left( -x\\right)  } &\\det \\left( \\begin{matrix}1&1&1\\\\ 0&y-x&z-x\\\\ x^{2}&y^{2}&z^{2}\\end{matrix} \\right)  \\\\ &\\overbrace{=}^{F_{31}\\left( -x^{2}\\right)  } &\\det \\left( \\begin{matrix}1&1&1\\\\ 0&y-x&z-x\\\\ 0&y^{2}-x^{2}&z^{2}-x^{2}\\end{matrix} \\right)  \\\\ &\\overbrace{=}^{\\mathrm{definicion} } &\\det \\left( \\begin{matrix}y-x&z-x\\\\ y^{2}-x^{2}&z^{2}-x^{2}\\end{matrix} \\right)  \\\\ &=&\\left( y-z\\right)  \\left( z^{2}-x^{2}\\right)  -\\left( z-x\\right)  \\left( y^{2}-x^{2}\\right)  \\\\ &=&\\left( y-x\\right)  \\left( z-x\\right)  \\left( z+x-y+x\\right)  \\\\ &=&\\left( y-x\\right)  \\left( z-x\\right)  \\left( z-y\\right)  \\\\ &=&\\left( x-y\\right)  \\left( y-z\\right)  \\left( z-x\\right)  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.11)$</p>\n",
    "\n",
    "Tal como quer√≠amos demostrar. ‚óºÔ∏é\n",
    "\n",
    "**Ejemplo 3.5:** Vamos a determinar todos los valores de $a\\in \\mathbb{R}$ tales que\n",
    "\n",
    "$$\\det \\left( \\begin{matrix}1&a&a^{2}&\\left( a^{3}+6\\right)  \\\\ 1&1&1&\\left( 1+6a\\right)  \\\\ 1&2&4&\\left( 8+3a\\right)  \\\\ 1&3&9&\\left( 27+2a\\right)  \\end{matrix} \\right)  =0$$\n",
    "<p style=\"text-align: right;\">$(3.12)$</p>\n",
    "\n",
    "En efecto,\n",
    "\n",
    "$$\\begin{array}{rcl}\\det \\left( \\begin{matrix}1&a&a^{2}&\\left( a^{3}+6\\right)  \\\\ 1&1&1&\\left( 1+6a\\right)  \\\\ 1&2&4&\\left( 8+3a\\right)  \\\\ 1&3&9&\\left( 27+2a\\right)  \\end{matrix} \\right)  &\\overbrace{=}^{\\begin{array}{c}F_{21}\\left( -1\\right)  \\\\ F_{31}\\left( -1\\right)  \\\\ F_{41}\\left( -1\\right)  \\end{array} } &\\det \\left( \\begin{matrix}1&a&a^{2}&\\left( a^{3}+6\\right)  \\\\ 0&\\left( 1-a\\right)  &\\left( 1-a^{2}\\right)  &\\left( 1-a^{3}+6\\left( a-1\\right)  \\right)  \\\\ 0&\\left( 2-a\\right)  &\\left( 4-a^{2}\\right)  &\\left( 8-a^{3}+3a-6\\right)  \\\\ 0&\\left( 3-a\\right)  &\\left( 9-a^{2}\\right)  &\\left( 27-a^{3}+2a-6\\right)  \\end{matrix} \\right)  \\\\ &\\overbrace{=}^{\\mathrm{definicion} } &\\det \\left( \\begin{matrix}\\left( 1-a\\right)  &\\left( 1-a^{2}\\right)  &\\left( 1-a^{3}+6\\left( a-1\\right)  \\right)  \\\\ \\left( 2-a\\right)  &\\left( 4-a^{2}\\right)  &\\left( 8-a^{3}+3a-6\\right)  \\\\ \\left( 3-a\\right)  &\\left( 9-a^{2}\\right)  &\\left( 27-a^{3}+2a-6\\right)  \\end{matrix} \\right)  \\\\ &=&\\det \\left( \\begin{matrix}\\left( 1-a\\right)  &\\left( 1-a^{2}\\right)  &\\left[ \\left( 1-a\\right)  \\left( a^{2}+a+1\\right)  +6\\left( a-1\\right)  \\right]  \\\\ \\left( 2-a\\right)  &\\left( 4-a^{2}\\right)  &\\left[ \\left( 2-a\\right)  \\left( a^{2}+2a+4\\right)  +3\\left( a-2\\right)  \\right]  \\\\ \\left( 3-a\\right)  &\\left( 9-a^{2}\\right)  &\\left[ \\left( 3-a\\right)  \\left( a^{2}+3a+9\\right)  +2\\left( a-3\\right)  \\right]  \\end{matrix} \\right)  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.13)$</p>\n",
    "\n",
    "Es claro, conforme el desarrollo anterior, que el determinante se anula cuando $a=1$, $a=2$ o $a=3$. Para $a\\neq 1$, $a\\neq 2$ y $a\\neq 3$, proseguimos con el desarrollo del determinante, con lo cual,\n",
    "\n",
    "$$\\begin{array}{rcl}\\det \\left( \\begin{matrix}1&a&a^{2}&\\left( a^{3}+6\\right)  \\\\ 1&1&1&\\left( 1+6a\\right)  \\\\ 1&2&4&\\left( 8+3a\\right)  \\\\ 1&3&9&\\left( 27+2a\\right)  \\end{matrix} \\right)  &\\overbrace{=}^{\\mathrm{propiedades} } &\\left( 1-a\\right)  \\left( 2-a\\right)  \\left( 3-a\\right)  \\det \\left( \\begin{matrix}1&\\left( 1+a\\right)  &\\left( a^{2}+a-5\\right)  \\\\ 1&\\left( 2+a\\right)  &\\left( a^{2}+2a+1\\right)  \\\\ 1&\\left( 3+a\\right)  &\\left( a^{2}+3a+7\\right)  \\end{matrix} \\right)  \\\\ &\\overbrace{=}^{\\begin{array}{c}F_{21}\\left( -1\\right)  \\\\ F_{31}\\left( -1\\right)  \\end{array} } &\\left( 1-a\\right)  \\left( 2-a\\right)  \\left( 3-a\\right)  \\det \\left( \\begin{matrix}1&\\left( 1+a\\right)  &\\left( a^{2}+a-5\\right)  \\\\ 0&1&\\left( a+6\\right)  \\\\ 0&2&\\left( a+6\\right)  \\end{matrix} \\right)  \\\\ &=&2\\left( 1-a\\right)  \\left( 2-a\\right)  \\left( 3-a\\right)  \\det \\left( \\begin{matrix}1&\\left( 1+a\\right)  &\\left( a^{2}+a-5\\right)  \\\\ 0&1&\\left( a+6\\right)  \\\\ 0&1&\\left( a+6\\right)  \\end{matrix} \\right)  \\\\ &=&0\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.14)$</p>\n",
    "\n",
    "Luego tenemos que $\\det(\\mathbf{A})=0$ para todo $a\\in \\mathbb{R}$. ‚óºÔ∏é"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f626970c",
   "metadata": {},
   "source": [
    "## Diagonalizaci√≥n de matrices.\n",
    "Una vez estudiado el concepto de determinante, vamos a ocuparnos de un problema m√°s general y que consiste en saber cuando, para una transformaci√≥n lineal del tipo $T:\\mathbb{R}^{n}\\longrightarrow \\mathbb{R}^{n}$, es posible encontrar una base $\\alpha$ con respecto a la cual la matriz asociada $\\mathbf{A}=[T]_{\\alpha}^{\\alpha}$ sea de tipo **diagonal**. De manera equivalente, queremos determinar las condiciones para las cuales una matriz $\\mathbf{A}\\in \\mathbb{R}^{n\\times n}$ puede *descomponerse* de la forma\n",
    "\n",
    "$$\\mathbf{A} =\\mathbf{P} \\mathbf{D} \\mathbf{P}^{-1}$$\n",
    "<p style=\"text-align: right;\">$(3.15)$</p>\n",
    "\n",
    "Donde $\\mathbf{D}\\in \\mathbb{R}^{n\\times n}$ es una matriz diagonal.\n",
    "\n",
    "Este problema de naturaleza puramente algebraica tiene una cantidad significativa de aplicaciones en otras ramas de las matem√°ticas, como en ecuaciones diferenciales, estad√≠stica y, por supuesto, en machine learning. Puntualmente, la diagonalizaci√≥n es un procedimiento esencial en la derivaci√≥n de la descomposici√≥n de matrices en **valores singulares** y que, a su vez, constituye la base del **an√°lisis de componentes principales**, uno de los modelos de aprendizaje no supervisado m√°s utilizados para la reducci√≥n de la dimensi√≥n de conjuntos de datos con un elevado n√∫mero de variables, sin perder una cantidad significativa de informaci√≥n. Tal vez el teorema m√°s importante de esta subsecci√≥n es el que dice que toda matriz sim√©trica puede representarse mediante la expresi√≥n (3.15).\n",
    "\n",
    "Para comenzar con el estudio de la diagonalizaci√≥n de matrices, primero introduciremos algunos conceptos y resultados esenciales.\n",
    "\n",
    "**<font color='blue'>Definici√≥n 3.2 ‚Äì Autovalores y autovectores:</font>** Sea $V$ un $\\mathbb{K}$-espacio vectorial y $T:V\\longrightarrow V$ una transformaci√≥n lineal. Diremos que $v\\in V$ es un **autovector o vector propio** de $T$ si se cumplen las siguientes condiciones:\n",
    "\n",
    "- **(C1):** $v\\in O_{V}$.\n",
    "- **(C2):** Existe un escalar $\\lambda \\in \\mathbb{K}$ tal que $T(v)=\\lambda v$.\n",
    "\n",
    "El escalar $\\lambda$ se denomina **autovalor o valor propio** asociado al autovector $v$.\n",
    "\n",
    "Equivalentemente, diremos que $v\\in V-\\left\\{ O_{V}\\right\\}$ es un autovector asociado a la matriz $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$ si $v$ es un autovector de la transformaci√≥n lineal $T:V\\longrightarrow V$ expl√≠citamente definida como $T(v)=\\mathbf{A}v$. Es decir, existe un escalar $\\lambda \\in \\mathbb{K}$ tal que $\\mathbf{A}v=\\lambda v$. De la misma forma, diremos que $\\lambda$ es un autovalor de la matriz $\\mathbf{A}$.\n",
    "\n",
    "**<font color='crimson'>Teorema 3.2:</font>** *Dada una matriz $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$, si $\\lambda \\in \\mathbb{K}$ es un autovalor de $\\mathbf{A}$, entonces las siguientes expresiones son equivalentes:*\n",
    "\n",
    "- **(T1):** $\\exists v\\neq O_{V}\\  |\\  \\mathbf{A} v=\\lambda v$, *donde $V$ es un $\\mathbb{K}$-espacio vectorial y $v$ es un autovector de la matriz $\\mathbf{A}$.*\n",
    "- **(T2):** $\\exists v\\in V$ *tal que $v$ es una soluci√≥n no trivial del sistema de ecuaciones $\\left( \\mathbf{A} -\\lambda \\mathbf{I}_{n} \\right)  v=O_{V}$.*\n",
    "- **(T3):** $\\ker \\left( \\mathbf{A} -\\lambda \\mathbf{I}_{n} \\right)  \\neq \\left\\{ O_{V}\\right\\}$.\n",
    "- **(T4):** $\\left( \\mathbf{A} -\\lambda \\mathbf{I}_{n} \\right)$ *es una matriz no invertible*.\n",
    "‚óÜ\n",
    "\n",
    "Queda claro pues que necesitamos una forma sencilla de determinar qu√© valores de $\\lambda \\in \\mathbb{K}$ son, en efecto, autovalores. Para ello, es √∫til reconocer que la condici√≥n **(T4)** en el teorema (3.2) puede expresarse como una ecuaci√≥n en la variable $\\lambda$. Por supuesto, es ac√° donde cobra sentido el desarrollo que hicimos del concepto de determinante de una matriz $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$, puesto que, como ya verificamos con el teorema (3.1), toda matriz es no singular (invertible) si su determinante es no nulo. Por lo tanto, la condici√≥n **(T4)** puede expresarse como $\\det \\left( \\mathbf{A} -\\lambda \\mathbf{I}_{n} \\right)  =0$, donde $\\mathbf{I}_{n}$ es la matriz identidad.\n",
    "\n",
    "Tiene sentido, por lo tanto, la siguiente definici√≥n.\n",
    "\n",
    "**<font color='blue'>Definici√≥n 3.3 ‚Äì Polinomio caracter√≠stico:</font>** Sea $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$ una matriz tal que √©sta coincide con la representaci√≥n matricial de una transformaci√≥n lineal $T:V\\longrightarrow V$ que opera sobre el $\\mathbb{K}$-espacio vectorial $V$. La expresi√≥n $P_{T}\\left( \\lambda \\right)  =\\det \\left( \\mathbf{A} -\\lambda \\mathbf{I}_{n} \\right)  \\in \\mathbb{K}_{n} \\left[ \\lambda \\right]$ ser√° llamada **polinomio caracter√≠stico** de la matriz $\\mathbf{A}$ (y, por extensi√≥n, de la transformaci√≥n lineal $T$).\n",
    "\n",
    "De la definici√≥n (3.2), se tiene que $v\\in V$ es un autovector de $\\mathbf{A}$ si $v\\neq O_{V}$ y existe un escalar $\\lambda \\in \\mathbb{K}$ tal que $\\mathbf{A}v=\\lambda v$. De esta manera, podemos verificar que $\\lambda$ es un autovalor de la matriz $\\mathbf{A}$ si y s√≥lo si es una soluci√≥n no nula de la ecuaci√≥n\n",
    "\n",
    "$$\\left( \\mathbf{A} -\\lambda \\mathbf{I}_{n} \\right)  v=O_{V}$$\n",
    "<p style=\"text-align: right;\">$(3.16)$</p>\n",
    "\n",
    "As√≠ pues, todo escalar $\\lambda \\in \\mathbb{K}$ que satisfaga (3.16) ser√° un autovalor de $\\mathbf{A}$. Notemos adem√°s que, conforme la expresi√≥n anterior, si $v$ es un autovector, tambi√©n lo es cualquier otro vector que sea linealmente dependiente con respecto a $v$. Es decir, si ùë£ es un autovector de la matriz $\\mathbf{A}$, entonces tambi√©n lo es $\\alpha v;\\forall v\\in \\mathbb{K}$. M√°s a√∫n, si $v$ es un autovector, cualquier autovalor asociado a $v$ es √∫nico, puesto que si $T(v)=\\lambda_{1}v=\\lambda_{2}v$, entonces $(\\lambda_{1}-\\lambda_{2})v=O_{V}$. Como $v\\neq O_{V}$, entonces $\\lambda_{1}-\\lambda_{2}=0$, lo que implica que $\\lambda_{1}=\\lambda_{2}$.\n",
    "\n",
    "**<font color='blue'>Definici√≥n 3.4 ‚Äì Autoespacio:</font>** Sea $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$ una matriz tal que √©sta coincide con la representaci√≥n matricial de una transformaci√≥n lineal $T:V\\longrightarrow V$, siendo $V$ un $\\mathbb{K}$-espacio vectorial. Para cada autovalor $\\lambda$ de $\\mathbf{A}$ definimos el **autoespacio o espacio propio** de $\\lambda$, denotado como $W_{\\lambda}$, como\n",
    "\n",
    "$$W_{\\lambda }=\\ker \\left( \\mathbf{A} -\\lambda \\mathbf{I}_{n} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.17)$</p>\n",
    "\n",
    "**<font color='blue'>Definici√≥n 3.5 ‚Äì Similitud entre matrices:</font>** Sean $\\mathbf{A} ,\\mathbf{B} \\in \\mathbb{K}^{n\\times n}$ dos matrices no singulares. Diremos que $\\mathbf{A}$ y $\\mathbf{B}$ son **similares** si existe otra matriz $\\mathbf{P}\\in \\mathbb{K}^{n\\times 1}$ tal que\n",
    "\n",
    "$$\\mathbf{A} =\\mathbf{P} \\mathbf{B} \\mathbf{P}^{-1}$$\n",
    "<p style=\"text-align: right;\">$(3.18)$</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
