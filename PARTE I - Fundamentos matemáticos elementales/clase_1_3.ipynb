{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5db779a0",
   "metadata": {},
   "source": [
    "# CLASE 1.3: Descomposiciones matriciales\n",
    "\n",
    "## Introducci√≥n.\n",
    "En las secciones anteriores estudiamos algunas formas de manipular y obtener ciertas m√©tricas para los vectores, proyecciones de esos vectores con respecto a determinados subespacios vectoriales y transformaciones lineales. Las aplicaciones y transformaciones que permiten operar con vectores pueden ser convenientemente descritas por medio de matrices. Adem√°s, la mayor√≠a de los conjuntos de datos *bien comportados* que podemos encontrar en el mundo real vienen especificados en estructuras que pueden ser arregladas y/o representadas igualmente por medio de matrices. Por ejemplo, las filas de estos conjuntos de datos suelen representar **registros** u **observaciones** (como personas, fechas, unidades, entre otras) y las columnas suelen representar diferentes **atributos** para cada fila (como edad, altura, propiedades extensivas de alg√∫n fen√≥meno o el valor de alguna variable en el tiempo). En esta secci√≥n, presentaremos tres aspectos relativos a las matrices: C√≥mo **resumirlas**, como **descomponerlas** y como utilizar tales descomposiciones para construir **aproximaciones** para determinadas matrices.\n",
    "\n",
    "A diferencia de la secci√≥n anterior, en √©sta volveremos a escribir algo de c√≥digo, a fin de corresponder algunos conceptos esenciales con librer√≠as tales como <font color='purple'>Numpy</font> o <font color='purple'>Scipy</font>. No ser√° demasiado c√≥digo, pero s√≠ el suficiente para darle algo de sentido pr√°ctico a los conceptos que desarrollaremos desde la perspectiva computacional."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e7992dc1",
   "metadata": {},
   "source": [
    "## Determinantes.\n",
    "\n",
    "### Un interludio previo.\n",
    "El concepto de determinante corresponde a otro de los elementos m√°s importantes del √°lgebra lineal. Corresponde a un objeto matem√°tico que es importante en el an√°lisis y soluci√≥n de sistemas de ecuaciones lineales (los mismos que vimos en el inicio de la [clase 1.1](https://github.com/rquezadac/udd_data_science_lectures/blob/main/PARTE%20I%20-%20Fundamentos%20matem%C3%A1ticos%20elementales/clase_1_1.ipynb)) y que puede expresarse por medio de una funci√≥n, denominada como **funci√≥n determinante**, y que permite aplicar cualquier matriz cuadrada de orden $n$ en el cuerpo $\\mathbb{K}$ donde sus elementos est√°n definidos. Dicha funci√≥n, para una matriz $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$, se denota como $\\det(\\mathbf{A})=|\\mathbf{A}|$, y es tal que $\\left| \\  \\cdot \\  \\right|  :\\mathbb{K}^{n} \\times \\mathbb{K}^{n} \\longrightarrow \\mathbb{K}$, pudi√©ndose escribir el determinante de la matriz $\\mathbf{A}$ como\n",
    "\n",
    "$$\\mathbf{A} =\\left\\{ a_{ij}\\in \\mathbb{K} \\right\\}  =\\left( \\begin{matrix}a_{11}&a_{12}&\\cdots &a_{1n}\\\\ a_{21}&a_{22}&\\cdots &a_{2n}\\\\ \\vdots &\\vdots &\\ddots &\\vdots \\\\ a_{n1}&a_{n2}&\\cdots &a_{nn}\\end{matrix} \\right)  \\in \\mathbb{K}^{n\\times n} \\wedge \\det \\left( \\mathbf{A} \\right)  =\\left| \\begin{matrix}a_{11}&a_{12}&\\cdots &a_{1n}\\\\ a_{21}&a_{22}&\\cdots &a_{2n}\\\\ \\vdots &\\vdots &\\ddots &\\vdots \\\\ a_{n1}&a_{n2}&\\cdots &a_{nn}\\end{matrix} \\right|  \\in \\mathbb{K}$$\n",
    "<p style=\"text-align: right;\">$(3.1)$</p>\n",
    "\n",
    "**Ejemplo 3.1:** Comenzaremos a motivar el estudio de los determinantes explorando la posibilidad de que una matriz cuadrada, digamos $\\mathbf{A}\\in \\mathbb{R}^{n\\times n}$, sea **invertible**. Para los casos de menor dimensi√≥n, ya conocemos los casos que aseguran que $\\mathbf{A}$ cumpla con esta condici√≥n. Por ejemplo, si $\\mathbf{A}\\in \\mathbb{R}^{1\\times 1}$ (es decir, $\\mathbf{A}$ es un escalar), sabemos que $\\mathbf{A}=a\\ \\Longrightarrow \\mathbf{A}^{-1}=1/a$, lo que implica que $\\mathbf{A}$ tiene una inversa siempre que $a\\neq 0$. Para el caso de matrices de $2\\times 2$, sabemos que la inversa $\\mathbf{A}^{-1}$ cumple con la condici√≥n de que $\\mathbf{A}\\mathbf{A}^{-1}=\\mathbf{I}_{2}$. De esta manera, podemos escribir\n",
    "\n",
    "$$\\mathbf{A} =\\left( \\begin{matrix}a_{11}&a_{12}\\\\ a_{21}&a_{22}\\end{matrix} \\right)  \\Longrightarrow \\mathbf{A} \\mathbf{A}^{-1} =\\mathbf{I}_{2} \\Longleftrightarrow \\mathbf{A}^{-1} =\\frac{1}{a_{11}a_{22}-a_{12}a_{21}} \\left( \\begin{matrix}a_{22}&-a_{12}\\\\ -a_{21}&a_{11}\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.2)$</p>\n",
    "\n",
    "Por lo tanto, $\\mathbf{A}$ es invertible si y s√≥lo si $a_{11}a_{22}-a_{12}a_{21}\\neq 0$. Para matrices de $2\\times 2$, dicha cantidad corresponde al **determinante** de la matriz respectiva. De esta manera, para la matriz $\\mathbf{A} =\\left\\{ a_{ij}\\right\\}  \\in \\mathbb{R}^{2\\times 2}$, su determinante se define como\n",
    "\n",
    "$$\\det \\left( \\mathbf{A} \\right)  =\\det \\left( \\begin{matrix}a_{11}&a_{12}\\\\ a_{21}&a_{22}\\end{matrix} \\right)  =\\left| \\begin{matrix}a_{11}&a_{12}\\\\ a_{21}&a_{22}\\end{matrix} \\right|  =a_{11}a_{22}-a_{12}a_{21}$$\n",
    "<p style=\"text-align: right;\">$(3.3)$</p>\n",
    "‚óºÔ∏é\n",
    "\n",
    "El ejemplo (3.1) permite establecer un hecho que puede ser generalizado a conjuntos de mayor dimensi√≥n: Una matriz es invertible siempre que su determinante no sea nulo. Formalicemos este hecho mediante un teorema.\n",
    "\n",
    "**<font color='crimson'>Teorema 3.1 ‚Äì Existencia de matriz inversa:</font>** *Sea $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$ una matriz cuadrada con elementos en el cuerpo $\\mathbb{K}$. Entonces $\\mathbf{A}$ se dir√° **invertible** o **no singular** (es decir, existe la matriz inversa $\\mathbf{A}^{-1}$) si y solo si $\\det(\\mathbf{A})\\neq 0$.*\n",
    "‚óÜ\n",
    "\n",
    "Ya disponemos de una expresi√≥n cerrada que permite calcular el determinante de cualquier matriz de dimensi√≥n $2\\times 2$. Sin embargo, no es tan sencillo generalizar dicho c√°lculo para dimensiones superiores. Por ejemplo, para el caso de matrices de $3\\times 3$, es com√∫n el c√°lculo de sus determinantes mediante la llamada regla de Sarrus:\n",
    "\n",
    "$$\\det \\left( \\begin{matrix}a_{11}&a_{12}&a_{13}\\\\ a_{21}&a_{22}&a_{23}\\\\ a_{31}&a_{32}&a_{33}\\end{matrix} \\right)  =a_{11}a_{22}a_{33}+a_{21}a_{32}a_{13}+a_{31}a_{12}a_{23}-a_{31}a_{22}a_{13}-a_{11}a_{32}a_{23}-a_{21}a_{12}a_{33}$$\n",
    "<p style=\"text-align: right;\">$(3.4)$</p>\n",
    "\n",
    "Si bien, en un principio, la regla de Sarrus parece una f√≥rmula complicada de entender, √©sta no es m√°s que un recurso mnemot√©cnico, ya que los triples productos involucrados en la f√≥rmula y sus signos guardan relaci√≥n con las diagonales (y subdiagonales) presentes en la matriz correspondiente, como se observa en el esquema de la Fig. (3.1)\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"figures/fig_3_1.png\" width=\"650\"></p>\n",
    "<p style=\"text-align: center;\">Fig. (3.1): Esquema que ilustra c√≥mo opera la regla de Sarrus</p>\n",
    "\n",
    "Recordemos que, al resolver sistemas de ecuaciones lineales, nuestro objetivo era transformar la matriz ampliada de un sistema en una tal que s√≥lo tuviera elementos no nulos en su regi√≥n superior derecha (matriz triangular superior), aunque tambi√©n es posible operar para llegar al caso opuesto, donde la matriz resultante tenga elementos no nulos en su regi√≥n inferior izquierda (matriz triangular inferior). Este tipo de matrices fueron formalizadas previamente en la definici√≥n (1.7).\n",
    "\n",
    "Para una matriz triangular, digamos $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$, definimos su determinante como\n",
    "\n",
    "$$\\det \\left( \\mathbf{A} \\right)  =\\prod^{n}_{i=1} a_{ii}$$\n",
    "<p style=\"text-align: right;\">$(3.5)$</p>\n",
    "\n",
    "Donde $a_{ii}$ es el correspondiente elemento relativo a la diagonal principal de la matriz $\\mathbf{A}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b96e74aa",
   "metadata": {},
   "source": [
    "### Interpretaci√≥n geom√©trica del determinante.\n",
    "Si una matriz $\\mathbf{A}\\in \\mathbb{R}^{n\\times n}$ tiene elementos $a_{ij}\\in \\mathbb{R}$, entonces puede ser utilizada para representar dos transformaciones lineales. Una que aplica la base can√≥nica de $\\mathbb{R}^{n}$ a las filas de $\\mathbf{A}$, y otra que aplica la misma base a las columnas de $\\mathbf{A}$. Cualquiera sea el caso, si la matriz $\\mathbf{A}$ es de $2\\times 2$, las im√°genes de cada uno de los vectores de la base can√≥nica de $\\mathbb{R}^{2}$ forman un paralel√≥gramo que representa la imagen del cuadrado unitario bajo la transformaci√≥n lineal respectiva, y cuyos v√©rtices se corresponden con combinaciones de los elementos de $\\mathbf{A}$, como se observa en la Fig. (3.2a).\n",
    "\n",
    "Si $\\mathbf{A} =\\left\\{ {}a_{ij}\\right\\}  \\in \\mathbb{R}^{2\\times 2}$ es la matriz que conforma los v√©rtices del paralel√≥gramo en la Fig. (3.2a), se tendr√° que el √°rea encerrada por el mismo ser√° igual a $\\det \\left( \\mathbf{A} \\right)  =a_{11}a_{22}-a_{12}a_{21}$. Para mostrar este resultado, podemos considerar que los elementos de la matriz $\\mathbf{A}$ corresponden a vectores que representan los v√©rtices del paralel√≥gramo. Si uno de los v√©rtices es el origen del sistema de coordenadas, los vectores $\\mathbf{a} =\\left( a_{11},a_{12}\\right)$ y $\\mathbf{b} =\\left( a_{21},a_{22}\\right)$ ser√°n los v√©rtices m√°s cercanos al origen, mientras que el v√©rtice opuesto ser√° igual a la suma $\\mathbf{a} +\\mathbf{b} =\\left( a_{11}+a_{21},a_{12}+a_{22}\\right)$. El √°rea del paralel√≥gramo puede expresarse igualmente como $\\left\\Vert \\mathbf{a} \\right\\Vert  \\left\\Vert \\mathbf{b} \\right\\Vert  \\mathrm{sen} \\left( \\theta \\right)$, donde $\\theta$ es el √°ngulo formado por los vectores $\\mathbf{a}$ y $\\mathbf{b}$. Si consideramos la proyecci√≥n ortogonal del vector $\\mathbf{a}$ sobre $\\mathbf{b}$ (que llamamos $\\pi_{\\mathbf{b}}(\\mathbf{a})$), podemos escribir\n",
    "\n",
    "$$\\mathrm{Area} =\\left\\Vert \\mathbf{a} \\right\\Vert  \\left\\Vert \\mathbf{b} \\right\\Vert  \\mathrm{sen} \\left( \\theta \\right)  =\\left\\Vert \\pi_{\\mathbf{b} } \\left( \\mathbf{a} \\right)  \\right\\Vert  \\left\\Vert \\mathbf{b} \\right\\Vert  \\cos \\left( \\frac{\\pi }{2} -\\theta \\right)  =a_{11}a_{12}-a_{12}a_{21}=\\det \\left( \\mathbf{A} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.6)$</p>\n",
    "\n",
    "La interpretaci√≥n geom√©trica anterior puede extenderse al caso de matrices de $3\\times 3$, considerando en este caso un paralelep√≠pedo generado por las submatrices columna que generan la matriz completa $\\mathbf{A} =\\left\\{ a_{ij}\\right\\}  \\in \\mathbb{R}^{3\\times 3}$. Si tales submatrices son representadas como $\\mathbf{r}_{1}$, $\\mathbf{r}_{2}$ y $\\mathbf{r}_{3}$, entonces el volumen del paralelep√≠pedo es igual a $V=\\det \\left( \\left( \\mathbf{r}_{1} ,\\mathbf{r}_{2} ,\\mathbf{r}_{3} \\right)  \\right)  =\\det \\left( \\mathbf{A} \\right)$, tal y como se ilustra en la Fig (3.2b).\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"figures/fig_3_2.png\" width=\"900\"></p>\n",
    "<p style=\"text-align: center;\">Fig. (3.2): (a) Esquema que muestra c√≥mo el determinante de una matriz de $2\\times 2$ permite transformar un cuadrado unitario en un paralel√≥gramo cuyos v√©rtices son las productos que componen dicho determinante y su √°rea ser√° igual al valor de tal determinante; (b) Misma transformaci√≥n para el caso de una matriz de $3\\times 3$. En este caso, la transformaci√≥n se aplica sobre un paralelep√≠pedo, obteni√©ndose un trapezoedro cuyo volumen es igual al correspondiente determinante</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f45c8fa9",
   "metadata": {},
   "source": [
    "### Propiedades de los determinantes.\n",
    "El c√°lculo de un determinante para una matriz arbitraria $\\mathbf{A} =\\left\\{ a_{ij}\\right\\}  \\in \\mathbb{R}^{n\\times n}$ requiere de un algoritmo generalizado para poder resolver los casos en los cuales $n>3$. Existen varios procedimientos para ello, siendo indudablemente el m√°s popular el **m√©todo de Laplace**, que definiremos a continuaci√≥n.\n",
    "\n",
    "**<font color='blue'>Definici√≥n 3.1 ‚Äì Determinante:</font>** Sea $\\mathbf{A} =\\left\\{ a_{ij}\\right\\}  \\in \\mathbb{R}^{n\\times n}$. Definimos el **determinante** de la matriz $\\mathbf{A}$ como\n",
    "\n",
    "1. Respecto a la columna $j$: $\\det \\left( \\mathbf{A} \\right)  =\\sum^{n}_{k=1} \\left( -1\\right)^{k+j}  a_{kj}\\det \\left( \\mathbf{A}_{kj} \\right)$.\n",
    "2. Respecto a la fila $j$: $\\det \\left( \\mathbf{A} \\right)  =\\sum^{n}_{k=1} \\left( -1\\right)^{k+j}  a_{jk}\\det \\left( \\mathbf{A}_{jk} \\right)$.\n",
    "\n",
    "En la f√≥rmulas anteriores, conocidas en la pr√°ctica como **expansiones de Laplace**, $\\mathbf{A}_{jk}$ corresponde a la submatriz resultante de eliminar de $\\mathbf{A}$ la fila $j$ y la columna $k$ y se denomina como **menor complementario** en la posici√≥n $(j, k)$, mientras que el n√∫mero real $\\triangle_{jk} =\\left( -1\\right)^{k+j}  a_{jk}$ es llamado **cofactor** asociado al $jk$-√©simo elemento de la matriz $\\mathbf{A}$.\n",
    "\n",
    "Resulta sencillo darnos cuenta de que, si bien las expansiones de Laplace nos permiten obtener una f√≥rmula cerrada para el c√°lculo del determinante de cualquier matriz cuadrada, su tiempo de ejecuci√≥n y complejidad computacional escala enormemente con la dimensi√≥n de la matriz para la cual queremos calcular su determinante. Por esa raz√≥n, en t√©rminos algebraicos, es mejor considerar ciertas propiedades que se desprenden directamente de la definici√≥n que hemos ido construyendo del determinante a fin de disponer de m√©todos m√°s efectivos para su c√°lculo en dimensiones superiores (considerando, adem√°s, las transformaciones elementales sobre matrices que hemos aprendido previamente). Vamos, por tanto, a desarrollar tales propiedades:\n",
    "\n",
    "- **(P1) ‚Äì Invariancia ante la transposici√≥n:** Si $\\mathbf{A} =\\left\\{ a_{{}ij}\\right\\}  \\in \\mathbb{R}^{n\\times n}$, entonces, de la definici√≥n (3.1), se tiene que $\\det(\\mathbf{A})=\\det(\\mathbf{A}^{\\top})$, ya que $\\det \\left( \\mathbf{A} \\right)  =\\sum^{n}_{k=1} \\triangle_{jk} \\det \\left( \\mathbf{A}_{jk} \\right)  =\\sum^{n}_{s=1} \\triangle_{sj} \\det \\left( \\mathbf{A}_{sj} \\right)$.\n",
    "- **(P2) ‚Äì Columna o fila nula:** Si $\\mathbf{A} =\\left\\{ a_{{}ij}\\right\\}  \\in \\mathbb{R}^{n\\times n}$ posee una columna o fila nula (conformada √∫nicamente por elementos iguales a cero), entonces $\\det(\\mathbf{A})=0$.\n",
    "- **(P3) ‚Äì Determinante de un producto de matrices:** Si $\\mathbf{A},\\mathbf{B}\\in \\mathbb{R}^{n\\times n}$, entonces $\\det(\\mathbf{A}\\mathbf{B})=\\det(\\mathbf{A})\\det(\\mathbf{B})$.\n",
    "- **(P4) ‚Äì Determinante de la matriz inversa:** Si $\\mathbf{A} =\\left\\{ a_{{}ij}\\right\\}  \\in \\mathbb{R}^{n\\times n}$ es una matriz no singular, entonces $\\det \\left( \\mathbf{A}^{-1} \\right)  =1/\\det \\left( \\mathbf{A} \\right)$.\n",
    "- **(P5) ‚Äì Invariancia ante operaciones elementales:** Cualquier matriz $\\mathbf{A}\\in \\mathbb{R}^{n\\times n}$ mantiene el valor de su determinante, aunque hayamos operado sobre ella mediante cualquier de las transformaciones elementales vistas en la [clase 1.1](https://github.com/rquezadac/udd_data_science_lectures/blob/main/PARTE%20I%20-%20Fundamentos%20matem%C3%A1ticos%20elementales/clase_1_1.ipynb).\n",
    "- **(P6) ‚Äì Escalamiento del determinante:** Sea $\\mathbf{A} =\\left\\{ a_{ij}\\right\\}  \\in \\mathbb{R}^{n\\times n}$ y $\\lambda \\in \\mathbb{R}$. Entonces $\\det \\left( \\lambda \\mathbf{A} \\right)  =\\lambda^{n} \\det \\left( \\mathbf{A} \\right)$.\n",
    "- **(P7) ‚Äì Columna o fila repetida:** Si una matriz $\\mathbf{A} =\\left\\{ a_{ij}\\right\\}  \\in \\mathbb{R}^{n\\times n}$ tiene columnas o filas repetidas (o, m√°s general, linealmente dependientes), entonces $\\det(\\mathbf{A})=0$. Esta propiedad generaliza **(P2)** y establece que toda matriz $\\mathbf{A}$ no singular tiene rango completo.\n",
    "\n",
    "**Ejemplo 3.2:** Calcularemos el determinante de la matriz $\\mathbf{A}\\in \\mathbb{R}^{5\\times 5}$, definida como\n",
    "\n",
    "$$\\mathbf{A} =\\left( \\begin{matrix}0&1&0&1&0\\\\ -1&a&0&0&0\\\\ 0&0&a&0&0\\\\ -1&0&0&a&0\\\\ 0&0&0&0&a\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.7)$</p>\n",
    "\n",
    "En efecto, aplicando la definici√≥n (3.1) y transformaciones elementales,\n",
    "\n",
    "$$\\begin{array}{rcl}\\det \\left( \\begin{matrix}0&1&0&1&0\\\\ -1&a&0&0&0\\\\ 0&0&a&0&0\\\\ -1&0&0&a&0\\\\ 0&0&0&0&a\\end{matrix} \\right)  &\\overbrace{=}^{\\mathrm{definicion} } &\\underbrace{a}_{\\mathrm{cofactor} \\  \\triangle_{55} } \\det \\left( \\begin{matrix}0&1&0&1\\\\ -1&a&0&0\\\\ 0&0&a&0\\\\ -1&0&0&a\\end{matrix} \\right)  \\\\ &\\overbrace{=}^{F_{42}\\left( -1\\right)  } &a\\det \\left( \\begin{matrix}0&1&0&1\\\\ -1&a&0&0\\\\ 0&0&a&0\\\\ 0&-a&0&a\\end{matrix} \\right)  \\\\ &\\overbrace{=}^{\\mathrm{definicion} } &\\underbrace{1}_{\\mathrm{cofactor} \\  \\triangle_{21} } \\cdot a\\det \\left( \\begin{matrix}1&0&1\\\\ 0&a&0\\\\ -a&0&a\\end{matrix} \\right)  \\\\ &\\overbrace{=}^{\\mathrm{definicion} } &a^{2}\\det \\left( \\begin{matrix}1&1\\\\ -a&a\\end{matrix} \\right)  \\overbrace{=}^{\\mathrm{definicion} } 2a^{3}\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.8)$</p>\n",
    "‚óºÔ∏é\n",
    "\n",
    "**Ejemplo 3.3 ‚Äì Los determinantes en <font color='purple'>Numpy</font>:** En librer√≠as de Python especializadas en el uso de arreglos vectorizados, como <font color='purple'>Numpy</font>, es razonable esperar que existan rutinas prefabricadas para el c√°lculo de determinantes. En particular, podemos usar la funci√≥n `det()`, del m√≥dulo de √°lgebra lineal `numpy.linalg()` para calcular el determinante de cualquier matriz expresada por medio de un arreglo bidimensional. Por ejemplo, si consideramos la matriz $\\mathbf{A}\\in \\mathbb{R}^{5\\times 5}$, definida como\n",
    "\n",
    "$$\\mathbf{A} =\\left( \\begin{matrix}0&-1&2&-1&0\\\\ 3&-1&2&2&0\\\\ 6&-1&0&0&9\\\\ 0&1&4&-5&9\\\\ 2&2&-4&5&-3\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.9)$</p>\n",
    "\n",
    "Podemos calcular su determinante f√°cilmente en <font color='purple'>Numpy</font> definiendo, primeramente, un arreglo bidimensional, digamos `A`, donde almacenamos esta matriz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adbfde30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "618b347b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el arreglo en cuesti√≥n.\n",
    "A = np.array([\n",
    "    [0, -1, 2, -1, 0],\n",
    "    [3, -1, 2, 2, 0],\n",
    "    [6, -1, 0, 0, 9],\n",
    "    [0, 1, 4, -5, 9],\n",
    "    [2, 2, -4, 5, -3],\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb19826",
   "metadata": {},
   "source": [
    "Y luego aplicando la funci√≥n `np.linalg.det()` para calcular su determinante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50c46cd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-162.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculamos el determinante de A (redondeado a 3 decimales).\n",
    "np.around(np.linalg.det(A), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de25b304",
   "metadata": {},
   "source": [
    "Vemos pues que no fue nada dif√≠cil calcular el determinante de una matriz de $5\\times 5$ en <font color='purple'>Numpy</font>. Sin embargo, tal y como comentamos previamente, el c√°lculo de determinantes corresponde a un esfuerzo computacional ostensiblemente grande y que escala enormemente a medida que aumentan las dimensiones de las matrices de inter√©s. Incluso trabajando con una librer√≠a muy eficiente como <font color='purple'>Numpy</font>, los tiempos de ejecuci√≥n pueden verse muy afectados. Para ejemplificar aquello, consideraremos el c√°lculo de los determinantes de cuatro matrices $\\mathbf{A}\\in \\mathbb{R}^{10\\times 10}$, $\\mathbf{B}\\in \\mathbb{R}^{100\\times 100}$, $\\mathbf{C}\\in \\mathbb{R}^{1000\\times 1000}$ y $\\mathbf{D}\\in \\mathbb{R}^{10000\\times 10000}$, las que representaremos mediante los arreglos bidimensionales `A`, `B`, `C` y `D`, y que estar√°n compuestas por n√∫meros reales uniformemente distribuidos entre 0 y 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d251663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos una semilla aleatoria fija.\n",
    "rng = np.random.default_rng(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8eed46cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos algunas matrices de distintos tama√±os.\n",
    "A = rng.random(size=(10, 10))\n",
    "B = rng.random(size=(100, 100))\n",
    "C = rng.random(size=(1000, 1000))\n",
    "D = rng.random(size=(10000, 10000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b1c26d",
   "metadata": {},
   "source": [
    "Vamos a estimar el tiempo de ejecuci√≥n asociado al c√°lculo de los determinantes de estas matrices, a fin de observar qu√© tal escala con respecto al incremento en dimensionalidad de las mismas. Notemos que cada matriz es 10 veces m√°s grande que su antecesora:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b8eddcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.34 ¬µs ¬± 83.7 ns per loop (mean ¬± std. dev. of 7 runs, 100,000 loops each)\n",
      "90 ¬µs ¬± 740 ns per loop (mean ¬± std. dev. of 7 runs, 10,000 loops each)\n",
      "8.37 ms ¬± 706 ¬µs per loop (mean ¬± std. dev. of 7 runs, 100 loops each)\n",
      "3.47 s ¬± 285 ms per loop (mean ¬± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.linalg.det(A)\n",
    "%timeit np.linalg.det(B)\n",
    "%timeit np.linalg.det(C)\n",
    "%timeit np.linalg.det(D)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9a7fbc63",
   "metadata": {},
   "source": [
    "Podemos observar que el c√°lculo del determinante de `B` tiene un tiempo de ejecuci√≥n 12 veces mayor que el c√°lculo del determinante de `A`. El c√°lculo del determinante de `C` tiene un tiempo de ejecuci√≥n de aproximadamente unas 82 veces superior al del c√°lculo del determinante de `B` (y, por extensi√≥n, 1038 veces m√°s lento que el c√°lculo del determinante de `A`). Y el c√°lculo del determinante de `D` tiene un tiempo de ejecuci√≥n aproximadamente unas 445 veces m√°s lento que el c√°lculo del determinante de `C` (y, por extensi√≥n... ¬°es m√°s de 36000 veces m√°s lento que el c√°lculo del determinante de `B`, y m√°s de 460000 veces m√°s lento que el c√°lculo del determinante de `A`!). Esto definitivamente nos har√° pensarlo dos veces antes de calcular determinantes en el mundo real, donde resulta com√∫n vernos enfrentados a bases de datos con cientos de miles de registros. ‚óºÔ∏é \n",
    "\n",
    "**Ejemplo 3.4:** Vamos a demostrar que\n",
    "\n",
    "$$\\det \\left( \\begin{matrix}1&1&1\\\\ x&y&z\\\\ x^{2}&y^{2}&z^{2}\\end{matrix} \\right)  =\\left( x-y\\right)  \\left( y-z\\right)  \\left( z-x\\right)$$\n",
    "<p style=\"text-align: right;\">$(3.10)$</p>\n",
    "\n",
    "En efecto, utilizando transformaciones elementales, propiedades de los determinantes y la definici√≥n (3.1), tenemos que\n",
    "\n",
    "$$\\begin{array}{rcl}\\det \\left( \\begin{matrix}1&1&1\\\\ x&y&z\\\\ x^{2}&y^{2}&z^{2}\\end{matrix} \\right)  &\\overbrace{=}^{F_{21}\\left( -x\\right)  } &\\det \\left( \\begin{matrix}1&1&1\\\\ 0&y-x&z-x\\\\ x^{2}&y^{2}&z^{2}\\end{matrix} \\right)  \\\\ &\\overbrace{=}^{F_{31}\\left( -x^{2}\\right)  } &\\det \\left( \\begin{matrix}1&1&1\\\\ 0&y-x&z-x\\\\ 0&y^{2}-x^{2}&z^{2}-x^{2}\\end{matrix} \\right)  \\\\ &\\overbrace{=}^{\\mathrm{definicion} } &\\det \\left( \\begin{matrix}y-x&z-x\\\\ y^{2}-x^{2}&z^{2}-x^{2}\\end{matrix} \\right)  \\\\ &=&\\left( y-z\\right)  \\left( z^{2}-x^{2}\\right)  -\\left( z-x\\right)  \\left( y^{2}-x^{2}\\right)  \\\\ &=&\\left( y-x\\right)  \\left( z-x\\right)  \\left( z+x-y+x\\right)  \\\\ &=&\\left( y-x\\right)  \\left( z-x\\right)  \\left( z-y\\right)  \\\\ &=&\\left( x-y\\right)  \\left( y-z\\right)  \\left( z-x\\right)  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.11)$</p>\n",
    "\n",
    "Tal como quer√≠amos demostrar. ‚óºÔ∏é\n",
    "\n",
    "**Ejemplo 3.5:** Vamos a determinar todos los valores de $a\\in \\mathbb{R}$ tales que\n",
    "\n",
    "$$\\det \\left( \\begin{matrix}1&a&a^{2}&\\left( a^{3}+6\\right)  \\\\ 1&1&1&\\left( 1+6a\\right)  \\\\ 1&2&4&\\left( 8+3a\\right)  \\\\ 1&3&9&\\left( 27+2a\\right)  \\end{matrix} \\right)  =0$$\n",
    "<p style=\"text-align: right;\">$(3.12)$</p>\n",
    "\n",
    "En efecto,\n",
    "\n",
    "$$\\begin{array}{rcl}\\det \\left( \\begin{matrix}1&a&a^{2}&\\left( a^{3}+6\\right)  \\\\ 1&1&1&\\left( 1+6a\\right)  \\\\ 1&2&4&\\left( 8+3a\\right)  \\\\ 1&3&9&\\left( 27+2a\\right)  \\end{matrix} \\right)  &\\overbrace{=}^{\\begin{array}{c}F_{21}\\left( -1\\right)  \\\\ F_{31}\\left( -1\\right)  \\\\ F_{41}\\left( -1\\right)  \\end{array} } &\\det \\left( \\begin{matrix}1&a&a^{2}&\\left( a^{3}+6\\right)  \\\\ 0&\\left( 1-a\\right)  &\\left( 1-a^{2}\\right)  &\\left( 1-a^{3}+6\\left( a-1\\right)  \\right)  \\\\ 0&\\left( 2-a\\right)  &\\left( 4-a^{2}\\right)  &\\left( 8-a^{3}+3a-6\\right)  \\\\ 0&\\left( 3-a\\right)  &\\left( 9-a^{2}\\right)  &\\left( 27-a^{3}+2a-6\\right)  \\end{matrix} \\right)  \\\\ &\\overbrace{=}^{\\mathrm{definicion} } &\\det \\left( \\begin{matrix}\\left( 1-a\\right)  &\\left( 1-a^{2}\\right)  &\\left( 1-a^{3}+6\\left( a-1\\right)  \\right)  \\\\ \\left( 2-a\\right)  &\\left( 4-a^{2}\\right)  &\\left( 8-a^{3}+3a-6\\right)  \\\\ \\left( 3-a\\right)  &\\left( 9-a^{2}\\right)  &\\left( 27-a^{3}+2a-6\\right)  \\end{matrix} \\right)  \\\\ &=&\\det \\left( \\begin{matrix}\\left( 1-a\\right)  &\\left( 1-a^{2}\\right)  &\\left[ \\left( 1-a\\right)  \\left( a^{2}+a+1\\right)  +6\\left( a-1\\right)  \\right]  \\\\ \\left( 2-a\\right)  &\\left( 4-a^{2}\\right)  &\\left[ \\left( 2-a\\right)  \\left( a^{2}+2a+4\\right)  +3\\left( a-2\\right)  \\right]  \\\\ \\left( 3-a\\right)  &\\left( 9-a^{2}\\right)  &\\left[ \\left( 3-a\\right)  \\left( a^{2}+3a+9\\right)  +2\\left( a-3\\right)  \\right]  \\end{matrix} \\right)  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.13)$</p>\n",
    "\n",
    "Es claro, conforme el desarrollo anterior, que el determinante se anula cuando $a=1$, $a=2$ o $a=3$. Para $a\\neq 1$, $a\\neq 2$ y $a\\neq 3$, proseguimos con el desarrollo del determinante, con lo cual,\n",
    "\n",
    "$$\\begin{array}{rcl}\\det \\left( \\begin{matrix}1&a&a^{2}&\\left( a^{3}+6\\right)  \\\\ 1&1&1&\\left( 1+6a\\right)  \\\\ 1&2&4&\\left( 8+3a\\right)  \\\\ 1&3&9&\\left( 27+2a\\right)  \\end{matrix} \\right)  &\\overbrace{=}^{\\mathrm{propiedades} } &\\left( 1-a\\right)  \\left( 2-a\\right)  \\left( 3-a\\right)  \\det \\left( \\begin{matrix}1&\\left( 1+a\\right)  &\\left( a^{2}+a-5\\right)  \\\\ 1&\\left( 2+a\\right)  &\\left( a^{2}+2a+1\\right)  \\\\ 1&\\left( 3+a\\right)  &\\left( a^{2}+3a+7\\right)  \\end{matrix} \\right)  \\\\ &\\overbrace{=}^{\\begin{array}{c}F_{21}\\left( -1\\right)  \\\\ F_{31}\\left( -1\\right)  \\end{array} } &\\left( 1-a\\right)  \\left( 2-a\\right)  \\left( 3-a\\right)  \\det \\left( \\begin{matrix}1&\\left( 1+a\\right)  &\\left( a^{2}+a-5\\right)  \\\\ 0&1&\\left( a+6\\right)  \\\\ 0&2&\\left( a+6\\right)  \\end{matrix} \\right)  \\\\ &=&2\\left( 1-a\\right)  \\left( 2-a\\right)  \\left( 3-a\\right)  \\det \\left( \\begin{matrix}1&\\left( 1+a\\right)  &\\left( a^{2}+a-5\\right)  \\\\ 0&1&\\left( a+6\\right)  \\\\ 0&1&\\left( a+6\\right)  \\end{matrix} \\right)  \\\\ &=&0\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.14)$</p>\n",
    "\n",
    "Luego tenemos que $\\det(\\mathbf{A})=0$ para todo $a\\in \\mathbb{R}$. ‚óºÔ∏é"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "39d47379",
   "metadata": {},
   "source": [
    "## Diagonalizaci√≥n de matrices.\n",
    "Una vez estudiado el concepto de determinante, vamos a ocuparnos de un problema m√°s general y que consiste en saber cuando, para una transformaci√≥n lineal del tipo $T:\\mathbb{R}^{n}\\longrightarrow \\mathbb{R}^{n}$, es posible encontrar una base $\\alpha$ con respecto a la cual la matriz asociada $\\mathbf{A}=[T]_{\\alpha}^{\\alpha}$ sea de tipo **diagonal**. De manera equivalente, queremos determinar las condiciones para las cuales una matriz $\\mathbf{A}\\in \\mathbb{R}^{n\\times n}$ puede *descomponerse* de la forma\n",
    "\n",
    "$$\\mathbf{A} =\\mathbf{P} \\mathbf{D} \\mathbf{P}^{-1}$$\n",
    "<p style=\"text-align: right;\">$(3.15)$</p>\n",
    "\n",
    "Donde $\\mathbf{D}\\in \\mathbb{R}^{n\\times n}$ es una matriz diagonal.\n",
    "\n",
    "Este problema de naturaleza puramente algebraica tiene una cantidad significativa de aplicaciones en otras ramas de las matem√°ticas, como en ecuaciones diferenciales, estad√≠stica y, por supuesto, en machine learning. Puntualmente, la diagonalizaci√≥n es un procedimiento esencial en la derivaci√≥n de la descomposici√≥n de matrices en **valores singulares** y que, a su vez, constituye la base del **an√°lisis de componentes principales**, uno de los modelos de aprendizaje no supervisado m√°s utilizados para la reducci√≥n de la dimensi√≥n de conjuntos de datos con un elevado n√∫mero de variables, sin perder una cantidad significativa de informaci√≥n. Tal vez el teorema m√°s importante de esta subsecci√≥n es el que dice que toda matriz sim√©trica puede representarse mediante la expresi√≥n (3.15).\n",
    "\n",
    "Para comenzar con el estudio de la diagonalizaci√≥n de matrices, primero introduciremos algunos conceptos y resultados esenciales.\n",
    "\n",
    "**<font color='blue'>Definici√≥n 3.2 ‚Äì Autovalores y autovectores:</font>** Sea $V$ un $\\mathbb{K}$-espacio vectorial y $T:V\\longrightarrow V$ una transformaci√≥n lineal. Diremos que $v\\in V$ es un **autovector o vector propio** de $T$ si se cumplen las siguientes condiciones:\n",
    "\n",
    "- **(C1):** $v\\in O_{V}$.\n",
    "- **(C2):** Existe un escalar $\\lambda \\in \\mathbb{K}$ tal que $T(v)=\\lambda v$.\n",
    "\n",
    "El escalar $\\lambda$ se denomina **autovalor o valor propio** asociado al autovector $v$.\n",
    "\n",
    "Equivalentemente, diremos que $v\\in V-\\left\\{ O_{V}\\right\\}$ es un autovector asociado a la matriz $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$ si $v$ es un autovector de la transformaci√≥n lineal $T:V\\longrightarrow V$ expl√≠citamente definida como $T(v)=\\mathbf{A}v$. Es decir, existe un escalar $\\lambda \\in \\mathbb{K}$ tal que $\\mathbf{A}v=\\lambda v$. De la misma forma, diremos que $\\lambda$ es un autovalor de la matriz $\\mathbf{A}$.\n",
    "\n",
    "**<font color='crimson'>Teorema 3.2:</font>** *Dada una matriz $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$, si $\\lambda \\in \\mathbb{K}$ es un autovalor de $\\mathbf{A}$, entonces las siguientes expresiones son equivalentes:*\n",
    "\n",
    "- **(T1):** $\\exists v\\neq O_{V}\\  |\\  \\mathbf{A} v=\\lambda v$, *donde $V$ es un $\\mathbb{K}$-espacio vectorial y $v$ es un autovector de la matriz $\\mathbf{A}$.*\n",
    "- **(T2):** $\\exists v\\in V$ *tal que $v$ es una soluci√≥n no trivial del sistema de ecuaciones $\\left( \\mathbf{A} -\\lambda \\mathbf{I}_{n} \\right)  v=O_{V}$.*\n",
    "- **(T3):** $\\ker \\left( \\mathbf{A} -\\lambda \\mathbf{I}_{n} \\right)  \\neq \\left\\{ O_{V}\\right\\}$.\n",
    "- **(T4):** $\\left( \\mathbf{A} -\\lambda \\mathbf{I}_{n} \\right)$ *es una matriz no invertible*.\n",
    "‚óÜ\n",
    "\n",
    "Queda claro pues que necesitamos una forma sencilla de determinar qu√© valores de $\\lambda \\in \\mathbb{K}$ son, en efecto, autovalores. Para ello, es √∫til reconocer que la condici√≥n **(T4)** en el teorema (3.2) puede expresarse como una ecuaci√≥n en la variable $\\lambda$. Por supuesto, es ac√° donde cobra sentido el desarrollo que hicimos del concepto de determinante de una matriz $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$, puesto que, como ya verificamos con el teorema (3.1), toda matriz es no singular (invertible) si su determinante es no nulo. Por lo tanto, la condici√≥n **(T4)** puede expresarse como $\\det \\left( \\mathbf{A} -\\lambda \\mathbf{I}_{n} \\right)  =0$, donde $\\mathbf{I}_{n}$ es la matriz identidad.\n",
    "\n",
    "Tiene sentido, por lo tanto, la siguiente definici√≥n.\n",
    "\n",
    "**<font color='blue'>Definici√≥n 3.3 ‚Äì Polinomio caracter√≠stico:</font>** Sea $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$ una matriz tal que √©sta coincide con la representaci√≥n matricial de una transformaci√≥n lineal $T:V\\longrightarrow V$ que opera sobre el $\\mathbb{K}$-espacio vectorial $V$. La expresi√≥n $P_{T}\\left( \\lambda \\right)  =\\det \\left( \\mathbf{A} -\\lambda \\mathbf{I}_{n} \\right)  \\in \\mathbb{K}_{n} \\left[ \\lambda \\right]$ ser√° llamada **polinomio caracter√≠stico** de la matriz $\\mathbf{A}$ (y, por extensi√≥n, de la transformaci√≥n lineal $T$).\n",
    "\n",
    "De la definici√≥n (3.2), se tiene que $v\\in V$ es un autovector de $\\mathbf{A}$ si $v\\neq O_{V}$ y existe un escalar $\\lambda \\in \\mathbb{K}$ tal que $\\mathbf{A}v=\\lambda v$. De esta manera, podemos verificar que $\\lambda$ es un autovalor de la matriz $\\mathbf{A}$ si y s√≥lo si es una soluci√≥n no nula de la ecuaci√≥n\n",
    "\n",
    "$$\\left( \\mathbf{A} -\\lambda \\mathbf{I}_{n} \\right)  v=O_{V}$$\n",
    "<p style=\"text-align: right;\">$(3.16)$</p>\n",
    "\n",
    "As√≠ pues, todo escalar $\\lambda \\in \\mathbb{K}$ que satisfaga (3.16) ser√° un autovalor de $\\mathbf{A}$. Notemos adem√°s que, conforme la expresi√≥n anterior, si $v$ es un autovector, tambi√©n lo es cualquier otro vector que sea linealmente dependiente con respecto a $v$. Es decir, si ùë£ es un autovector de la matriz $\\mathbf{A}$, entonces tambi√©n lo es $\\alpha v;\\forall v\\in \\mathbb{K}$. M√°s a√∫n, si $v$ es un autovector, cualquier autovalor asociado a $v$ es √∫nico, puesto que si $T(v)=\\lambda_{1}v=\\lambda_{2}v$, entonces $(\\lambda_{1}-\\lambda_{2})v=O_{V}$. Como $v\\neq O_{V}$, entonces $\\lambda_{1}-\\lambda_{2}=0$, lo que implica que $\\lambda_{1}=\\lambda_{2}$.\n",
    "\n",
    "**<font color='blue'>Definici√≥n 3.4 ‚Äì Autoespacio:</font>** Sea $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$ una matriz tal que √©sta coincide con la representaci√≥n matricial de una transformaci√≥n lineal $T:V\\longrightarrow V$, siendo $V$ un $\\mathbb{K}$-espacio vectorial. Para cada autovalor $\\lambda$ de $\\mathbf{A}$ definimos el **autoespacio o espacio propio** de $\\lambda$, denotado como $W_{\\lambda}$, como\n",
    "\n",
    "$$W_{\\lambda }=\\ker \\left( \\mathbf{A} -\\lambda \\mathbf{I}_{n} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.17)$</p>\n",
    "\n",
    "**<font color='blue'>Definici√≥n 3.5 ‚Äì Similitud entre matrices:</font>** Sean $\\mathbf{A} ,\\mathbf{B} \\in \\mathbb{K}^{n\\times n}$ dos matrices no singulares. Diremos que $\\mathbf{A}$ y $\\mathbf{B}$ son **similares** si existe otra matriz $\\mathbf{P}\\in \\mathbb{K}^{n\\times 1}$ tal que\n",
    "\n",
    "$$\\mathbf{A} =\\mathbf{P} \\mathbf{B} \\mathbf{P}^{-1}$$\n",
    "<p style=\"text-align: right;\">$(3.18)$</p>\n",
    "\n",
    "**<font color='crimson'>Teorema 3.3 ‚Äì Preservaci√≥n de autovalores en matrices similares:</font>** *Sean $\\mathbf{A} ,\\mathbf{B} \\in \\mathbb{K}^{n\\times n}$ dos matrices similares entre s√≠. Entonces ambas matrices tienen los mismos autovalores (y, por extensi√≥n, el mismo polinomio caracter√≠stico).*\n",
    "\n",
    "Vamos a demostrar el teorema (3.3) a fin de entender completamente este resultado. En efecto, sean $\\mathbf{A} ,\\mathbf{B} \\in \\mathbb{K}^{n\\times n}$, tal que ambas matrices son similares (es decir, $\\mathbf{A} =\\mathbf{P} \\mathbf{B} \\mathbf{P}^{-1}$). Construyendo la expresi√≥n $\\mathbf{A} -\\lambda \\mathbf{I}_{n}$, tenemos que\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{A} -\\lambda \\mathbf{I}_{n} &=&\\mathbf{P} \\mathbf{B} \\mathbf{P}^{-1} -\\lambda \\mathbf{P} \\mathbf{P}^{-1} =\\mathbf{P} \\left( \\mathbf{B} -\\lambda \\mathbf{I}_{n} \\right)  \\mathbf{P}^{-1} \\\\ &\\Longrightarrow &\\det \\left( \\mathbf{A} -\\lambda \\mathbf{I}_{n} \\right)  =\\det \\left( \\mathbf{P} \\left( \\mathbf{B} -\\lambda \\mathbf{I}_{n} \\right)  \\mathbf{P}^{-1} \\right)  \\\\ &\\Longrightarrow &\\det \\left( \\mathbf{P} \\right)  \\det \\left( \\mathbf{B} -\\lambda \\mathbf{I}_{n} \\right)  \\det \\left( \\mathbf{P}^{-1} \\right)  \\  \\left( \\mathrm{pero} \\  \\det \\left( \\mathbf{P}^{-1} \\right)  =\\frac{1}{\\det \\left( \\mathbf{P} \\right)  } \\right)  \\\\ &\\Longrightarrow &\\det \\left( \\mathbf{B} -\\lambda \\mathbf{I}_{n} \\right)  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.19)$</p>\n",
    "\n",
    "As√≠, efectivamente, $\\mathbf{A}$ y $\\mathbf{B}$ tienen el mismo polinomio caracter√≠stico y, por tanto, los mismos autovalores.\n",
    "\n",
    "**Ejemplo 3.5:** Sea $T:\\mathbb{R}^{3}\\longrightarrow \\mathbb{R}^{3}$ una transformaci√≥n lineal definida como $T(x,y,z)=(3x+2y+z,3y+2z,-z)$. Vamos a determinar los autovalores y autovectores asociados a $T$. \n",
    "\n",
    "En primer lugar, debemos construir la matriz $\\mathbf{A}$ asociada a $T$ considerando la base can√≥nica de vectores en $\\mathbb{R}^{3}$ (que, recordemos, es $\\mathbf{e}(3)=\\left\\{ \\mathbf{e}_{1} ,\\mathbf{e}_{2} ,\\mathbf{e}_{3} \\right\\}  =\\left\\{ \\left( 1,0,0\\right)  ,\\left( 0,1,0\\right)  ,\\left( 0,0,1\\right)  \\right\\}$). Esto resulta sencillo, ya que\n",
    "\n",
    "$$T\\left( x,y,z\\right)  =\\left( 3x+2y+z,3y+2z,-z\\right)  =x\\begin{pmatrix}3\\\\ 0\\\\ 0\\end{pmatrix} +y\\begin{pmatrix}2\\\\ 3\\\\ 0\\end{pmatrix} +z\\begin{pmatrix}1\\\\ 2\\\\ -1\\end{pmatrix} \\Longrightarrow \\mathbf{A}= \\begin{pmatrix}3&2&1\\\\ 0&3&2\\\\ 0&0&-1\\end{pmatrix}$$\n",
    "<p style=\"text-align: right;\">$(3.20)$</p>\n",
    "\n",
    "Ahora debemos resolver la ecuaci√≥n $P_{\\mathbf{A}}(\\lambda)=0$. En efecto,\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{A} =\\begin{pmatrix}3&2&1\\\\ 0&3&2\\\\ 0&0&-1\\end{pmatrix} &\\Longrightarrow &P_{\\mathbf{A} }\\left( \\lambda \\right)  =\\det \\left( \\begin{matrix}3-\\lambda &2&1\\\\ 0&3-\\lambda &2\\\\ 0&0&-1-\\lambda \\end{matrix} \\right)  =0\\\\ &\\Longrightarrow &P_{\\mathbf{A} }\\left( \\lambda \\right)  =\\left( \\lambda -3\\right)^{3}  \\left( \\lambda +1\\right)  =0\\Longleftrightarrow \\lambda_{1} =3\\wedge \\lambda_{2} =-1\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.21)$</p>\n",
    "\n",
    "Por lo tanto, los autovalores de la matriz $\\mathbf{A}$ (y, por extensi√≥n, de $T$) son $\\lambda_{1}=3$ y $\\lambda_{2}=-1$. Ahora determinamos los autoespacios respectivos,\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{u} \\in \\left( \\mathbb{R}^{3} \\right)_{\\lambda }  &\\Longleftrightarrow &\\mathbf{u} \\in \\mathbb{R}^{3} \\wedge T\\left( \\mathbf{u} \\right)  =\\lambda \\mathbf{u} \\\\ &\\Longleftrightarrow &\\mathbf{u} =\\left( x,y,z\\right)  \\wedge T\\left( x,y,z\\right)  =\\lambda \\left( x,y,z\\right)  \\\\ &\\Longleftrightarrow &\\mathbf{u} =\\left( x,y,z\\right)  \\wedge T\\left( x,y,z\\right)  =\\left( \\lambda x,\\lambda y,\\lambda z\\right)  \\\\ &\\Longleftrightarrow &\\mathbf{u} =\\left( x,y,z\\right)  \\wedge \\left( 3x+2y+z,3y+2z,-z\\right)  =\\left( \\lambda x,\\lambda y,\\lambda z\\right)  \\\\ &\\Longleftrightarrow &\\mathbf{u} =\\left( x,y,z\\right)  \\wedge \\begin{cases}\\begin{array}{rcl}3x+2y+z&=&\\lambda x\\\\ 3y+2z&=&\\lambda y\\\\ -z&=&\\lambda z\\end{array} &\\end{cases} \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.22)$</p>\n",
    "\n",
    "Debemos por tanto evaluar el sistema de ecuaciones determinado en (3.22) usando los autovalores determinados previamente. As√≠ tenemos, para $\\lambda_{1}=3$,\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{u} \\in \\left( \\mathbb{R}^{3} \\right)_{\\lambda =3} &\\Longleftrightarrow &\\mathbf{u} =\\left( x,y,z\\right)  \\wedge \\begin{cases}\\begin{array}{rcl}3x+2y+z&=&\\lambda x\\\\ 3y+2z&=&\\lambda y\\\\ -z&=&\\lambda z\\end{array} &\\end{cases} \\\\ &\\Longleftrightarrow &\\mathbf{u} =\\left( x,y,z\\right)  \\wedge z=0,y=0\\\\ &\\Longleftrightarrow &\\mathbf{u} =\\left( x,0,0\\right)  ;x\\in \\mathbb{R} \\\\ &\\Longleftrightarrow &\\mathbf{u} =x\\left( 1,0,0\\right)  \\\\ &\\Longleftrightarrow &\\left( \\mathbb{R}^{3} \\right)_{\\lambda =3}  =\\left< \\left\\{ \\left( 1,0,0\\right)  \\right\\}  \\right>  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.23)$</p>\n",
    "\n",
    "Por otro lado, para $\\lambda_{2}=-1$, tenemos,\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{u} \\in \\left( \\mathbb{R}^{3} \\right)_{\\lambda =-1}  &\\Longleftrightarrow &\\mathbf{u} =\\left( x,y,z\\right)  \\wedge \\begin{cases}\\begin{array}{rcl}3x+2y+z&=&-x\\\\ 3y+2z&=&-y\\\\ -z&=&-z\\end{array} &\\end{cases} \\\\ &\\Longleftrightarrow &\\mathbf{u} =\\left( x,y,z\\right)  \\wedge z=2y,x=0\\\\ &\\Longleftrightarrow &\\mathbf{u} =\\left( 0,y,-2y\\right)  ;y\\in \\mathbb{R} \\\\ &\\Longleftrightarrow &\\mathbf{u} =y\\left( 0,1,-2\\right)  \\\\ &\\Longleftrightarrow &\\left( \\mathbb{R}^{3} \\right)_{\\lambda =-1}  =\\left< \\left\\{ \\left( 0,1,-2\\right)  \\right\\}  \\right>  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.24)$</p>\n",
    "‚óºÔ∏é\n",
    "\n",
    "Sea $T:\\mathbb{K}^{n}\\longrightarrow \\mathbb{K}^{n}$ una transformaci√≥n lineal y sea $\\mathbf{A}=[T]_{\\alpha}^{\\alpha}$ la matriz asociada a $T$ en la base can√≥nica $\\alpha$. Supongamos que existe una base de autovectores de $\\mathbf{A}$, definida como $\\beta =\\left\\{ \\mathbf{v}_{1},...,\\mathbf{v}_{n}\\right\\}$, siendo $\\beta$ por tanto una base de $\\mathbb{K}^{n}$. Para todo $i\\in \\mathbb{N}$, definimos $\\lambda_{i}\\in \\mathbb{K}$ tal que $\\mathbf{A}\\lambda \\mathbf{v}_{i}=\\lambda_{i} \\mathbf{v}_{i}$. Vemos que la matriz asociada a $T$ en la base $\\beta$, que denominamos como $\\mathbf{D}=[T]_{\\beta}^{\\beta}$, es diagonal, ya que\n",
    "\n",
    "$$\\begin{array}{rcl}\\mathbf{A} \\mathbf{v}_{1} =\\lambda_{1} \\mathbf{v}_{1} &\\Longrightarrow &\\lambda_{1} \\mathbf{v}_{1} =\\lambda_{1} \\mathbf{v}_{1} +0\\mathbf{v}_{2} +\\cdots +0\\mathbf{v}_{n} \\\\ \\mathbf{A} \\mathbf{v}_{2} =\\lambda_{2} \\mathbf{v}_{2} &\\Longrightarrow &\\lambda_{2} \\mathbf{v}_{2} =0\\mathbf{v}_{1} +\\lambda_{2} \\mathbf{v}_{2} +\\cdots +0\\mathbf{v}_{n} \\\\ &\\vdots &\\\\ \\mathbf{A} \\mathbf{v}_{n} =\\lambda_{n} \\mathbf{v}_{n} &\\Longrightarrow &\\lambda_{n} \\mathbf{v}_{n} =0\\mathbf{v}_{1} +0\\mathbf{v}_{2} +\\cdots +\\lambda_{n} \\mathbf{v}_{n} \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.25)$</p>\n",
    "\n",
    "Por lo tanto, podemos escribir\n",
    "\n",
    "$$\\left[ T\\right]^{\\beta }_{\\beta }  =\\left( \\left[ T\\left( \\mathbf{v}_{1} \\right)  \\right]_{\\beta }  ,\\left[ T\\left( \\mathbf{v}_{2} \\right)  \\right]_{\\beta }  ,...,\\left[ T\\left( \\mathbf{v}_{n} \\right)  \\right]_{\\beta }  \\right)  =\\left( \\begin{matrix}\\lambda_{1} &0&\\cdots &0\\\\ 0&\\lambda_{2} &\\cdots &0\\\\ \\vdots &\\vdots &\\ddots &\\vdots \\\\ 0&0&\\cdots &\\lambda_{n} \\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.26)$</p>\n",
    "\n",
    "As√≠ que, efectivamente, la matriz $\\mathbf{D}=[ T]^{\\beta }_{\\beta }$ y, por ende, la matriz $\\mathbf{A}$ puede expresarse mediante la **descomposici√≥n propia** (o auto-descomposici√≥n) definida como $\\mathbf{A}=\\mathbf{P}\\mathbf{D}\\mathbf{P}^{-1}$. Esto resulta conveniente, ya que algunas ventajas de conocer la matriz diagonal $\\mathbf{D}$ son las siguientes:\n",
    "\n",
    "- **(V1):** $\\rho(\\mathbf{A})=\\rho(\\mathbf{D})=$ n√∫mero de autovalores no nulos de $\\mathbf{A}$ (y, por extensi√≥n, de $\\mathbf{D}$). Recordemos que $\\rho(\\mathbf{A})$ denota el rango de la matriz $\\mathbf{A}$.\n",
    "- **(V2):** $\\det(\\mathbf{A})=\\det(\\mathbf{P}\\mathbf{D}\\mathbf{P}^{-1})=\\det(\\mathbf{P})\\det(\\mathbf{D})\\det(\\mathbf{P}^{-1})=\\det(\\mathbf{D})=\\prod^{n}_{k=1} \\lambda_{k}$.\n",
    "- **(V3):** Si $\\mathbf{A}$ es una matriz no singular (de decir, si $\\det(\\mathbf{A})\\neq 0$), entonces, para cada $\\lambda_{k}\\neq 0$ y $\\mathbf{A}^{-1}=\\mathbf{P}\\mathbf{D^{-1}}\\mathbf{P^{-1}}$, donde\n",
    "\n",
    "$$\\mathbf{D}^{-1} =\\left( \\begin{matrix}\\lambda^{-1}_{1} &0&\\cdots &0\\\\ 0&\\  \\lambda^{-1}_{2} &\\cdots &0\\\\ \\vdots &\\vdots &\\ddots &\\vdots \\\\ 0&0&\\cdots &\\lambda^{-1}_{n} \\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.27)$</p>\n",
    "\n",
    "- **(V4):** $\\mathbf{A}^{m} =\\left( \\mathbf{P} \\mathbf{D} \\mathbf{P}^{-1} \\right)^{m}  =\\left( \\mathbf{P} \\mathbf{D} \\mathbf{P}^{-1} \\right)  \\left( \\mathbf{P} \\mathbf{D} \\mathbf{P}^{-1} \\right)  \\overbrace{\\cdots }^{m\\  \\mathrm{veces} } \\left( \\mathbf{P} \\mathbf{D} \\mathbf{P}^{-1} \\right)$. As√≠ que,\n",
    "\n",
    "$$\\mathbf{A}^{m} =\\mathbf{P} \\left( \\begin{matrix}\\lambda^{-m}_{1} &0&\\cdots &0\\\\ 0&\\lambda^{-m}_{2} &\\cdots &0\\\\ \\vdots &\\vdots &\\ddots &\\vdots \\\\ 0&0&\\cdots &\\lambda^{-m}_{n} \\end{matrix} \\right)  \\mathbf{P}^{-1}$$\n",
    "<p style=\"text-align: right;\">$(3.28)$</p>\n",
    "\n",
    "Como hemos visto, $\\mathbf{A} =\\mathbf{P} \\mathbf{D} \\mathbf{P}^{-1}$, donde $\\mathbf{P}=[\\mathbf{e}]_{\\beta}^{\\alpha}$ (donde $\\mathbf{e}(n)$ es la base can√≥nica de vectores de $\\mathbb{K}^{n}$). Es decir, tenemos que expresar los correspondientes autovectores en t√©rminos de la base can√≥nica de $\\mathbb{K}^{n}$. De esta manera, $\\mathbf{P}=(\\mathbf{v}_{1},...,\\mathbf{v}_{n})$. Tiene sentido entonces la siguiente definici√≥n.\n",
    "\n",
    "**<font color='blue'>Definici√≥n 3.6 ‚Äì Matriz diagonalizable:</font>** Sea $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$ una matriz no singular. Diremos que $\\mathbf{A}$ es **diagonalizable** si $\\mathbb{K}^{n}$ admite una base de autovectores de $\\mathbf{A}$ (es decir, los autovectores de $\\mathbf{A}$ conforman un sistema de generadores para $\\mathbb{K}^{n}$ y son linealmente independientes).\n",
    "\n",
    "Con esta definici√≥n, ya podemos enunciar dos importantes teoremas relativos al proceso de diagonalizaci√≥n.\n",
    "\n",
    "**<font color='crimson'>Teorema 3.4:</font>** *Sea $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$ una matriz no singular. Entonces $\\mathbf{A}$ es diagonalizable si y s√≥lo si $\\mathbf{A}$ es similar a una matriz diagonal.* ‚óÜ\n",
    "\n",
    "**<font color='crimson'>Teorema 3.5:</font>** *Sea $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$ una matriz no singular y sea $\\left\\{ \\lambda_{i} \\right\\}^{k}_{i=1}$ un conjunto de autovalores de $\\mathbf{A}$ (todos distintos). Si $\\left\\{ v_{i}\\right\\}^{k}_{i=1}$ representa el conjunto de autovectores de $\\mathbf{A}$ para el cual se tiene que $\\mathbf{A} v_{i}=\\lambda_{i} v_{i}$, entonces $\\left\\{ v_{i}\\right\\}^{k}_{i=1}$ es un conjunto linealmente independiente.* ‚óÜ\n",
    "\n",
    "Antes de proesguir, veremos la extensi√≥n natural del concepto de suma directa para m√°s de dos subespacios vectoriales. Definamos primero los $k$ subespacios $U_{1},...,U_{k}$ de $V$. Definiremos la **suma** de tales subespacios como\n",
    "\n",
    "$$\\sum^{k}_{i=1} U_{i}=U_{1}+\\cdots +U_{k}\\triangleq \\left\\{ v=\\sum^{k}_{i=1} u_{i}\\  |\\  \\forall i\\in \\left\\{ 1,...,k\\right\\}  ,u_{i}\\in U_{i}\\right\\}$$\n",
    "<p style=\"text-align: right;\">$(3.29)$</p>\n",
    "\n",
    "La suma de subespacios as√≠ definida tambi√©n es, como cabr√≠a esperar, un subespacio de $V$. Estamos en condiciones, por tanto, de establecer la siguiente definici√≥n.\n",
    "\n",
    "**<font color='blue'>Definici√≥n 3.7 ‚Äì Suma directa m√∫ltiple:</font>** Sea $V$ un $\\mathbb{K}$-espacio vectorial y $\\left\\{ U_{i}\\right\\}^{k^{}}_{i=1}$ una colecci√≥n de $k$ subespacios vectoriales de $V$. Diremos que el subespacio $Z=\\sum^{k}_{i=1} U_{i}$ es la **suma directa** de $\\left\\{ U_{i}\\right\\}^{k^{}}_{i=1}$, lo que denotamos como $Z=\\bigoplus^{k}_{i=1} U_{i}=U_{1}\\oplus \\cdots \\oplus U_{k}$ si, para todo $v\\in Z$, $v$ se escribe de manera √∫nica como $v=\\sum^{k}_{i=1} u_{i}$, donde $u_{i}\\in U_{i}$, para $i=1,...,k$. Es decir,\n",
    "\n",
    "$$Z=\\bigoplus^{k}_{i=1} U_{i}=U_{1}\\oplus \\cdots \\oplus U_{k}\\Longleftrightarrow v=\\sum^{k}_{i=1} u_{i}\\  ;\\  u_{i}\\in U_{i},\\forall i\\in \\left\\{ 1,...,k\\right\\}$$\n",
    "<p style=\"text-align: right;\">$(3.30)$</p>\n",
    "\n",
    "La suma directa m√∫ltiple de subespacios cumple con las siguientes propiedades:\n",
    "\n",
    "- **(P1):** $Z=\\bigoplus^{k}_{i=1} U_{i}\\Longleftrightarrow \\left( Z=\\sum^{k}_{i=1} U_{i}\\wedge \\forall j\\in \\left\\{ 1,...,k\\right\\}  ,U_{j}\\cap \\left( \\sum^{k}_{\\begin{matrix}i=1\\\\ i\\neq j\\end{matrix} } U_{i}\\right)  =\\left\\{ O_{V}\\right\\}  \\right)$.\n",
    "- **(P2):** Si $Z=\\sum^{k}_{i=1} U_{i}$ y $Z$ es de dimensi√≥n finita (lo que suele denotarse como $\\dim(Z)<\\infty$), entonces las siguientes proposiciones son equivalentes:\n",
    "    - $Z=\\bigoplus^{k}_{i=1} U_{i}$.\n",
    "    - $\\left( \\forall i=\\left\\{ 1,...,k\\right\\}  \\right)  \\left( \\forall u_{i}\\in U_{i}-\\left\\{ O_{V}\\right\\}  \\right)  ,\\left\\{ u_{1},...,u_{k}\\right\\}$ es linealmente independiente.\n",
    "    - La yuxtaposici√≥n de las bases de las bases de los subespacios $\\left\\{ U_{i}\\right\\}^{k}_{i=1}$ es una base (y no s√≥lo un sistema de generadores) para $Z$.\n",
    "    - $\\dim(Z)=\\sum^{k}_{i=1} \\dim \\left( U_{i}\\right)$.\n",
    "\n",
    "Las propiedades que derivan de la definici√≥n (3.7) nos permiten formular el siguiente teorema.\n",
    "\n",
    "**<font color='crimson'>Teorema 3.6:</font>** *Sea $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$ una matriz no singular tal que $\\left\\{ \\lambda_{i} \\right\\}^{k}_{i=1}$ es el conjunto de autovalores de $\\mathbf{A}$ (todos distintos) y $W_{\\lambda_{i}}=\\ker(\\mathbf{A}-\\lambda \\mathbf{I}_{n})$ es el autoespacio asociado al autovalor $\\lambda_{i}$. Si $W=\\sum^{k}_{i=1} W_{\\lambda_{i} }$, entonces tenemos que*\n",
    "\n",
    "$$W=\\bigoplus^{k}_{i=1} W_{\\lambda_{i} }$$\n",
    "<p style=\"text-align: right;\">$(3.31)$</p>\n",
    "\n",
    "*En particular, $\\mathbf{A}$ es diagonalizable si y s√≥lo si*\n",
    "\n",
    "$$\\mathbb{K}^{n} =\\bigoplus^{k}_{i=1} W_{\\lambda_{i} }$$\n",
    "<p style=\"text-align: right;\">$(3.32)$</p>\n",
    "‚óÜ\n",
    "\n",
    "**<font color='DodgerBlue'>Corolario 3.1:</font>** *Sea $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$ una matriz no singular tal que $\\left\\{ \\lambda_{i} \\right\\}^{k}_{i=1}$ es el conjunto de autovalores de $\\mathbf{A}$ (todos distintos) y $W_{\\lambda_{i}}=\\ker(\\mathbf{A}-\\lambda \\mathbf{I}_{n})$ es el autoespacio asociado al autovalor $\\lambda_{i}$. Si $W=\\sum^{k}_{i=1} W_{\\lambda_{i} }$, entonces tenemos que*\n",
    "\n",
    "- **(T1):** $W_{\\lambda_{i} }=\\ker \\left( \\mathbf{A} -\\lambda \\mathbf{I}_{n} \\right)$ *es de dimensi√≥n 1.*\n",
    "- **(T2):** *Sea $v_{i}\\in W_{\\lambda_{i}}$ con $v_{i}\\neq O_{V}$. Entonces $\\left\\{ v_{1},...,v_{n}\\right\\}$ es una base de autovectores.*\n",
    "‚óÜ\n",
    "\n",
    "Este resultado nos entrega una condici√≥n suficiente (pero no necesaria) para establecer que una matriz $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$ sea diagonalizable: En s√≠ntesis, se tiene que $\\mathbf{A}$ es diagonalizable si $\\mathbf{A}$ tiene $n$ autovalores distintos. Sin embargo, esto no excluyente.\n",
    "\n",
    "**<font color='blue'>Definici√≥n 3.8 ‚Äì Multiplicidad geom√©trica:</font>** Sea $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$ una matriz no singular y $\\lambda$ un autovalor de $\\mathbf{A}$. Definimos la **multiplicidad geom√©trica** de $\\lambda$, denotada como $\\gamma_{\\mathbf{A}}(\\lambda)$, como la dimensi√≥n del autoespacio $W_{\\lambda }=\\ker \\left( \\mathbf{A} -\\lambda \\mathbf{I}_{n} \\right)$. Es decir, $\\gamma_{\\mathbf{A} } \\left( \\lambda \\right)  =\\dim \\left( W_{\\lambda }\\right)$.\n",
    "\n",
    "**<font color='blue'>Definici√≥n 3.9 ‚Äì Multiplicidad algebraica:</font>** Sea $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$ una matriz no singular y $\\lambda$ un autovalor de $\\mathbf{A}$. Definimos la **multiplicidad algebraica** del autovalor $\\lambda$, denotada como $\\alpha_{\\mathbf{A}}(\\lambda)$, como la m√°xima potencia de $(x-\\lambda)$ que es divisor del polinomio caracter√≠stico de $\\mathbf{A}$.\n",
    "\n",
    "Con ambas definiciones de multiplicidad ya establecidas, y con los resultados obtenidos previamente, estamos en condiciones de establecer el siguiente teorema, que establece las condiciones necesarias y suficientes para garantizar que una matriz es diagonalizable.\n",
    "\n",
    "**<font color='crimson'>Teorema 3.7 ‚Äì Criterio de diagonalizaci√≥n de matrices:</font>** *Sea $\\mathbf{A}\\in \\mathbb{K}^{n\\times n}$ una matriz no singular y $P_{\\mathbf{A}}(\\lambda)$ su polinomio caracter√≠stico. Se tiene entonces que $\\mathbf{A}$ es diagonalizable si y s√≥lo si $P_{\\mathbf{A}}(\\lambda)$ puede descomponerse en $\\mathbb{K}$ en una serie de factores lineales. Es decir,*\n",
    "\n",
    "$$P_{\\mathbf{A} }\\left( \\lambda \\right)  =c_{\\mathbf{A} }\\left( \\lambda -\\lambda_{1} \\right)^{\\alpha_{\\mathbf{A} } \\left( \\lambda_{1} \\right)  }  \\left( \\lambda -\\lambda_{2} \\right)^{\\alpha_{\\mathbf{A} } \\left( \\lambda_{2} \\right)  }  \\cdots \\left( \\lambda -\\lambda_{k} \\right)^{\\alpha_{\\mathbf{A} } \\left( \\lambda_{k} \\right)  }  =\\prod^{k}_{i=1} c_{\\mathbf{A} }\\left( \\lambda -\\lambda_{i} \\right)^{\\alpha_{\\mathbf{A} } \\left( \\lambda_{i} \\right)  }$$\n",
    "<p style=\"text-align: right;\">$(3.33)$</p>\n",
    "\n",
    "*Adem√°s, para cada autovalor $\\lambda$ de $\\mathbf{A}$, se debe tener que $\\gamma_{\\mathbf{A}}(\\lambda)=\\alpha_{\\mathbf{A}}(\\lambda)$.* ‚óÜ\n",
    "\n",
    "**Ejemplo 3.6:** Determinaremos los valores de $a$ y $b$ para los cuales la matriz\n",
    "\n",
    "$$\\mathbf{A} =\\left( \\begin{matrix}2a-b&0&2a-2b\\\\ 1&a&2\\\\ -a+b&0&-a+2b\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.34)$</p>\n",
    "\n",
    "es diagonalizable.\n",
    "\n",
    "En efecto, partimos calculando el polinomio caracter√≠stico de $\\mathbf{A}$,\n",
    "\n",
    "$$\\begin{array}{rcl}P_{\\mathbf{A} }\\left( \\lambda \\right)  &=&\\det \\left( \\mathbf{A} -\\lambda \\mathbf{I}_{3} \\right)  \\\\ &=&\\det \\left( \\begin{matrix}2a-b-\\lambda &0&2a-2b\\\\ 1&a-\\lambda &2\\\\ -a+b&0&-a+2b-\\lambda \\end{matrix} \\right)  \\\\ &=&\\left( a-\\lambda \\right)  \\det \\left( \\begin{matrix}2a-b-\\lambda &2a-2b\\\\ -a+b&-a+2b-\\lambda \\end{matrix} \\right)  \\\\ &\\underbrace{=}_{F_{21}\\left( 1\\right)  } &\\left( a-\\lambda \\right)  \\det \\left( \\begin{matrix}2a-b-\\lambda &2a-2b\\\\ a-\\lambda &a-\\lambda \\end{matrix} \\right)  \\\\ &=&\\left( a-\\lambda \\right)^{2}  \\det \\left( \\begin{matrix}2a-b-\\lambda &2a-2b\\\\ 1&1\\end{matrix} \\right)  \\\\ &=&\\left( a-\\lambda \\right)^{2}  \\left( b-\\lambda \\right)  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.36)$</p>\n",
    "\n",
    "As√≠ pues las ra√≠ces del polinomio caracter√≠stico $P_{\\mathbf{A} }\\left( \\lambda \\right)$ son $\\lambda=a$ y $\\lambda=b$. Por lo tanto, separamos la soluci√≥n de este problema en dos casos posibles, en los cuales se puede tener que $a\\neq b$ o $a=b$. Entonces, si $a\\neq b$, los autovalores asociados a $\\mathbf{A}$ son $\\lambda_{1}=a$ y $\\lambda_{2}=b$. La multiplicidad algebraica de $\\lambda_{1}=a$ es 2. Para determinar su multiplicidad geom√©trica, debemos determinar la dimensi√≥n del autoespacio asociado a este autovalor. As√≠ tenemos que,\n",
    "\n",
    "$$W_{a}=\\ker \\left( \\mathbf{A} -a\\mathbf{I}_{3} \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.37)$</p>\n",
    "\n",
    "Luego,\n",
    "\n",
    "$$\\begin{array}{lll}\\left( \\mathbf{A} -a\\mathbf{I}_{3} \\right)  \\mathbf{x} =\\left( \\begin{matrix}0\\\\ 0\\\\ 0\\end{matrix} \\right)  &\\Longleftrightarrow &\\mathbf{x} =\\left( \\begin{matrix}x_{1}\\\\ x_{2}\\\\ x_{3}\\end{matrix} \\right)  \\in \\mathbb{R}^{3\\times 1} \\wedge a\\neq b;\\left( \\begin{matrix}a-b&0&2a-2b\\\\ 1&0&2\\\\ -a+b&0&-2a+2b\\end{matrix} \\right)  \\left( \\begin{matrix}x_{1}\\\\ x_{2}\\\\ x_{3}\\end{matrix} \\right)  =\\left( \\begin{matrix}0\\\\ 0\\\\ 0\\end{matrix} \\right)  \\\\ &\\Longleftrightarrow &\\mathbf{x} =\\left( \\begin{matrix}x_{1}\\\\ x_{2}\\\\ x_{3}\\end{matrix} \\right)  \\in \\mathbb{R}^{3\\times 1} \\wedge a\\neq b;\\left( \\begin{matrix}1&0&2\\\\ a-b&0&2a-2b\\\\ -a+b&0&-2a+2b\\end{matrix} \\right)  \\left( \\begin{matrix}x_{1}\\\\ x_{2}\\\\ x_{3}\\end{matrix} \\right)  =\\left( \\begin{matrix}0\\\\ 0\\\\ 0\\end{matrix} \\right)  \\\\ &\\Longleftrightarrow &\\mathbf{x} =\\left( \\begin{matrix}x_{1}\\\\ x_{2}\\\\ x_{3}\\end{matrix} \\right)  \\in \\mathbb{R}^{3\\times 1} \\wedge a\\neq b;\\left( \\begin{matrix}1&0&2\\\\ 0&0&0\\\\ 0&0&0\\end{matrix} \\right)  \\left( \\begin{matrix}x_{1}\\\\ x_{2}\\\\ x_{3}\\end{matrix} \\right)  =\\left( \\begin{matrix}0\\\\ 0\\\\ 0\\end{matrix} \\right)  \\\\ &\\Longrightarrow &\\rho \\left( \\mathbf{A} -a\\mathbf{I}_{3} \\right)  =1\\\\ &\\Longrightarrow &\\dim \\left( W_{a}\\right)  =2\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.38)$</p>\n",
    "\n",
    "Por lo tanto, la multiplicidad geom√©trica de $\\lambda_{1}=a$ es 2. Para $\\lambda_{2}=b$, la multiplicidad geom√©trica es 1, ya que su multiplicidad algebraica es tambi√©n igual a 1. En resumen, si $a\\neq b$, se tiene que\n",
    "\n",
    "$$\\begin{array}{l}\\lambda_{1} =a\\  ;\\  \\gamma_{\\mathbf{A} } \\left( \\lambda_{1} \\right)  =2\\  ;\\  \\alpha_{\\mathbf{A} } \\left( \\lambda_{1} \\right)  =2\\\\ \\lambda_{2} =b\\  ;\\  \\gamma_{\\mathbf{A} } \\left( \\lambda_{2} \\right)  =1\\  ;\\  \\alpha_{\\mathbf{A} } \\left( \\lambda_{2} \\right)  =1\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.39)$</p>\n",
    "\n",
    "As√≠ que, por el teorema (3.7), la matriz $\\mathbf{A}$ es diagonalizable cuando $a\\neq b$.\n",
    "\n",
    "Cuando $a=b$, tenemos un √∫nico autovalor ùúÜ=ùëé con multiplicidad algebraica $\\alpha_{\\mathbf{A}}(\\lambda)=3$. Calculamos por tanto su multiplicidad geom√©trica como sigue\n",
    "\n",
    "$$\\begin{array}{lll}\\left( \\mathbf{A} -a\\mathbf{I}_{3} \\right)  \\mathbf{x} =\\left( \\begin{matrix}0\\\\ 0\\\\ 0\\end{matrix} \\right)  &\\Longleftrightarrow &\\mathbf{x} =\\left( \\begin{matrix}x_{1}\\\\ x_{2}\\\\ x_{3}\\end{matrix} \\right)  \\in \\mathbb{R}^{3} \\wedge \\left( \\begin{matrix}0&0&0\\\\ 1&0&2\\\\ 0&0&0\\end{matrix} \\right)  \\left( \\begin{matrix}x_{1}\\\\ x_{2}\\\\ x_{3}\\end{matrix} \\right)  =\\left( \\begin{matrix}0\\\\ 0\\\\ 0\\end{matrix} \\right)  \\\\ &\\Longrightarrow &\\rho \\left( \\mathbf{A} -a\\mathbf{I}_{3} \\right)  =1\\\\ &\\Longrightarrow &\\dim \\left( W_{a}\\right)  =1\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(3.40)$</p>\n",
    "\n",
    "Por lo tanto, la multiplicidad geom√©trica de $\\lambda=a$ es igual a 1. De esta manera, conforme el teorema (3.7), dado que las multiplicidades algebraica y geom√©trica de $\\lambda=a$ son distintas, deducimos que $\\mathbf{A}$ no es diagonalizable cuando $a=b$. ‚óºÔ∏é"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
