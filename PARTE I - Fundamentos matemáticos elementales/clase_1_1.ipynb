{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de258054",
   "metadata": {},
   "source": [
    "# CLASE 1.1: Un repaso (consciente) de álgebra lineal.\n",
    "---\n",
    "\n",
    "## Introducción.\n",
    "Cuando formalizamos conceptos intuitivos, un enfoque muy utilizado es construir un conjunto de objetos (símbolos) y reglas para manipular tales objetos. Ambos constituyen un marco de referencia que suele ser denominado como un álgebra. En particular, el álgebra lineal es el estudio de elementos conocidos como vectores y de un conjunto de reglas que permiten manipular adecuadamente estos vectores. Los vectores solemos conocerlos por primera vez en la enseñanza media, cuando en los cursos de Física abordamos ciertos cálculos soportados por elementos geométricos que solemos denotar por símbolos tales como $\\overrightarrow{x}$ o $\\overrightarrow{y}$. En esta asignatura, discutiremos conceptos más generales de vectores y usaremos una letra en **negrita** para representarlos cuando éstos sean tuplas de $\\mathbb{R}^{n}$; por ejemplo, $\\mathbf{x}$ o $\\mathbf{y}$.\n",
    "\n",
    "En general, los vectores son objetos especiales que pueden sumados y multiplicados por escalares para producir otro objeto del mismo tipo. Desde un punto de vista matemático y abstracto, cualquier objeto que satisfaga estas dos propiedades puede ser considerada un vector. A continuación, revisaremos ejemplos de este tipo de objetos:\n",
    "\n",
    "1. **Vectores geométricos:** Este ejemplo resulta familiar para estudiantes de secundaria que hayan cursado las asignaturas de física y matemáticas. Los vectores geométricos, como se observa en la Fig. (1.1a), corresponden a segmentos con una dirección bien definida, los cuales pueden ser dibujados (al menos en dos o tres dimensiones). Dos vectores geométricos, digamos $\\overrightarrow{x}$ e $\\overrightarrow{y}$, pueden sumarse, de tal forma que $\\overrightarrow{x} + \\overrightarrow{y} = \\overrightarrow{z}$ es otro vector. Además, la multiplicación de cualquier vector por un escalar arbitrario $\\lambda \\in \\mathbb{R}$, $\\lambda \\overrightarrow{x}$, también da como resultado otro vector. De hecho, el resultado de la última operación no es más que el mismo vector amplificado por un **factor de escalamiento** $\\lambda$. Por lo tanto, los vectores geométricos son instancias del concepto de vector que discutimos en un principio. La interpretación de vectores como objetos geométricos nos permite utilizar nuestra intuición para entender conceptos tales como la dirección y la magnitud de un vector, así como la aritmética entre ellos.\n",
    "\n",
    "2. **Los polinomios también son vectores:** Cualquier expresión de la forma $y=\\displaystyle \\sum\\nolimits^{n}_{i=0} a_{i}x^{i}$, con $\\left\\{ a_{i}\\right\\}^{n}_{i=1}  \\in \\mathbb{C} $, se denomina polinomio de orden $n$ (para $n\\neq 0$), y el conjunto de todos ellos se denota como $\\mathbb{R}_{n}[x]$. Se observan algunos ejemplos en la Fig. (1.1b). Dos polinomios pueden sumarse entre ellos, dando lugar a un nuevo polinomio. Además, cualquier multiplicación de un polinomio por un escalar también dará lugar a un nuevo polinomio. Por lo tanto, los polinomios constituyen también una instancia del concepto de vector discutido previamente. Estos objetos son muy distintos de los vectores geométricos, ya que si bien pueden graficarse en el plano, son entidades con un nivel muy superior de abstracción.\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"figures/fig_1_1.png\" width=\"800\"></p>\n",
    "<p style=\"text-align: center;\">Fig. (1.1): (a) Los vectores geométricos; (b) Un polinomio también es un vector</p>\n",
    "\n",
    "3. **Las señales de audio son vectores:** Dichas señales son representadas como una serie de números. Podemos, igualmente, sumar entre sí este tipo de señales, siendo su suma una nueva señal de audio. Si escalamos una señal de audio, también obtendremos una nueva señal. Por lo tanto, bajo nuestra primera conceptualización, las señales de audio son también vectores.\n",
    "\n",
    "4. **Los elementos de $\\mathbf{R}^{n}$ (tuplas de $n$ números reales) son también vectores:** Estos vectores serán el tipo de objeto que abordaremos con mayor detenimiento en estos apuntes. Por ejemplo, $\\mathbf{a}=(1, 2, 3)\\in \\mathbb{R}^{3}$ es una tripleta de números que conforman un vector en un espacio euclídeo de tres dimensiones. La adición de dos vectores $\\mathbf{a}, \\mathbf{b} \\in \\mathbb{R}^{3}$, componente a componente, genera un nuevo vector (que podemos escribir como $\\mathbf{c} = \\mathbf{a} + \\mathbf{c}$). Además, la multiplicación de un vector arbitrario $\\mathbf{a} \\in \\mathbb{R}^{n}$ por un escalar $\\lambda \\in \\mathbb{R}$ resulta en otro vector (que podemos escribir como $\\mathbf{d} = \\lambda \\mathbf{a}$). La consideración de estas tuplas como vectores tiene el beneficio adicional de que es posible representar tales objetos como arreglos a nivel computacional (por ejemplo, en Python, podemos utilizar listas, tuplas o arreglos de **Numpy** para representar vectores).\n",
    "\n",
    "El álgebra lineal se enfoca, principalmente, en las similitudes existentes entre estos conceptos de vector. Podemos sumar vectores y multiplicarlos por escalares. Nos enfocaremos fundamentalmente en vectores en $\\mathbb{R}^{n}$, debido a que la mayoría de los algoritmos basados en álgebra lineal se formulan en dicho conjunto. Más adelante, veremos que con frecuencia consideraremos que la data del mundo real se representará mediante vectores en $\\mathbb{R}^{n}$. Además, nos limitaremos al estudio de otras estructuras generales como espacios vectoriales cuya dimensión será finita, de tal forma que siempre habrá una correspondencia 1 a 1 entre cualquier tipo de vector y el conjunto $\\mathbb{R}^{n}$. Cuando sea conveniente (y para ir migrando poco a poco al dominio de lo que es la implementación de estos conocimientos en la práctica), utilizaremos nuestra intuición relativa a vectores geométricos y consideraremos algoritmos basados en estructuras tales como arreglos.\n",
    "\n",
    "Una idea importante en matemáticas corresponde al concepto de clausura. La pregunta asociada a la formulación de dicho concepto es la siguiente: ¿Cuál es el conjunto de todos los objetos que pueden resultar de las operaciones que propongamos? O en el caso de los vectores: ¿Cuál es el conjunto de vectores que pueden resultar partiendo de un conjunto pequeño de vectores iniciales, sumándolos y escalándolos? Esto último resulta en un espacio vectorial (que veremos en detalle más adelante), el cual es un concepto que conforma la base de mucho de lo que comporta a lo relativo a Machine Learning (aprendizaje automatizado), una serie de pautas, metodologías y algoritmos que permiten modelar una serie de procesos, fenómenos y sistemas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8296699",
   "metadata": {},
   "source": [
    "## Sistemas de ecuaciones lineales.\n",
    "Los sistemas de ecuaciones lineales juegan un papel fundamental en el álgebra lineal. Muchos problemas físicos (en todo tipo de contextos fenomenológicos e industriales) pueden ser resueltos mediante la formulación de los mismos en base a sistemas de ecuaciones lineales y, como cabría esperar, el álgebra lineal nos entrega las herramientas para resolverlos.\n",
    "\n",
    "**Ejemplo 1.1:** Una compañía produce diferentes productos $N_{1},...,N_{n}$ para los cuales se requieren varios recursos $R_{1},...,R_{m}$. Para producir una unidad del producto $N_{j}$, se necesitan $a_{ij}$ unidades del recurso $R_{i}$, donde $1\\leq i\\leq m$ y $1\\leq j\\leq n$.\n",
    "\n",
    "El objetivo es encontrar un plan de producción óptimo; es decir, un plan que estime cuántas unidades $x_{j}$ del producto $N_{j}$ deberían ser producidos si un total de $b_{i}$ unidades del recurso $R_{i}$ están disponibles y (idealmente) se utilizan todos los recursos disponibles (no quedan holguras de ninguno).\n",
    "\n",
    "Si producimos $x_{1},...,x_{n}$ unidades de los productos respectivos, necesitamos un total de\n",
    "\n",
    "$$a_{i1}x_{1}+\\cdots +a_{in}x_{n}$$\n",
    "<p style=\"text-align: right;\">$(1.1)$</p>\n",
    "\n",
    "unidades del recurso $R_{i}$. Un plan de producción óptimo $(x_{1},...,x_{n})\\in \\mathbb{R}^{n}$, por lo tanto, debe satisfacer el siguiente sistema de ecuaciones lineales:\n",
    "\n",
    "$$\\begin{array}{rlr}a_{11}x_{1}+a_{12}x_{2}+\\cdots +a_{1n}x_{n}&=&b_{1}\\\\ a_{21}x_{1}+a_{22}x_{2}+\\cdots +a_{2n}x_{n}&=&b_{2}\\\\ \\vdots &&\\\\ a_{m1}x_{1}+a_{m2}x_{2}+\\cdots +a_{mn}x_{n}&=&b_{m}\\end{array} $$\n",
    "<p style=\"text-align: right;\">$(1.2)$</p>\n",
    "\n",
    "Donde $a_{ij}$ y $b_{i}\\in \\mathbb{R}$. ◼︎\n",
    "\n",
    "La ecuación (1.2) ilustra el esquema general de un sistema de ecuaciones lineales, donde los valores $x_{1},...,x_{n}$ son las **incógnitas** del sistema. Cada tupla $\\mathbf{x}=(x_{1},...,x_{n})\\in \\mathbb{R}^{n}$ que satisface (1.2) es una **solución** del sistema.\n",
    "\n",
    "**Ejemplo 1.2:** El sistema de ecuaciones lineales\n",
    "\n",
    "$$\\begin{array}{rcll}x_{1}+x_{2}+x_{3}&=&3&\\left( 1\\right)  \\\\ x_{1}-x_{2}+2x_{3}&=&2&\\left( 2\\right)  \\\\ 2x_{1}+3x_{3}&=&1&\\left( 3\\right)  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(1.3)$</p>\n",
    "\n",
    "no tiene solución. Si sumamos las ecuaciones (1) y (2), obtenemos $2x_{1}+3x_{3}=5$, lo que contradice (3).\n",
    "\n",
    "Ahora observemos el siguiente sistema de ecuaciones:\n",
    "\n",
    "$$\\begin{array}{rcll}x_{1}+x_{2}+x_{3}&=&3&\\left( 1\\right)  \\\\ x_{1}-x_{2}+2x_{3}&=&2&\\left( 2\\right)  \\\\ x_{2}+x_{3}&=&1&\\left( 3\\right)  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(1.4)$</p>\n",
    "\n",
    "De (1) y (3), se tiene que $x_{1}=1$. De la operación (1) + (2), obtenemos $2x_{1}+3x_{3}=5$, lo que implica que $x_{3}=1$. De (3), obtenemos que $x_{2}=1$. Por lo tanto, el vector $\\mathbf{x}=(x_{1}, x_{2}, x_{3})=(1, 1, 1)$ es la **única** y posible solución del sistema (1.4).\n",
    "\n",
    "Consideremos, como tercer ejemplo, el siguiente sistema de ecuaciones:\n",
    "\n",
    "$$\\begin{array}{rcll}x_{1}+x_{2}+x_{3}&=&3&\\left( 1\\right)  \\\\ x_{1}-x_{2}+2x_{3}&=&2&\\left( 2\\right)  \\\\ 2x_{1}+3x_{3}&=&5&\\left( 3\\right)  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(1.5)$</p>\n",
    "\n",
    "Dado que (1) + (2) = (3), podemos omitir la tercera ecuación, ya que resulta ser **redundante**. De (1) y (2), obtenemos $2x_{1}=5-3x_{3}$ y $2x_{2}=1+x_{3}$. Definimos $x_{3}=a\\in \\mathbb{R}$ con una *variable libre*, de manera que cualquier tripleta del tipo $\\left( \\frac{1}{2} \\left( 5-3a\\right)  ,\\frac{1}{2} \\left( 1+a\\right)  ,a\\right)\\in \\mathbb{R}^{3}$ es una solución de (1.5). Por lo tanto, el conjunto definido previamente establece infinitas soluciones para el sistema. ◼︎\n",
    "\n",
    "En general, para un sistema de ecuaciones lineales con dominio en un subconjunto de $\\mathbb{R}$, pueden darse tres casos distintos: El sistema no tiene solución, tiene una solución única, o bien, tiene infinitas soluciones. El modelo de regresión lineal, por ejemplo (y como ya veremos más adelante), es un caso particular de solución analíticamente cerrada de un sistema de ecuaciones lineales cuando no podemos resolver dicho sistema por métodos más convencionales.\n",
    "\n",
    "En un sistema de ecuaciones con dos variables, digamos $x_{1}$ y $x_{2}$, cada ecuación lineal define una recta en el plano $(x_{1},x_{2})$. Dado que una solución para el sistema debe satisfacer simultáneamente todas sus ecuaciones, el conjunto solución del mismo corresponde a la intersección de ambas rectas. Esta intersección puede estar representada por otra recta (si las ecuaciones lineales respectivas describen a la misma recta), un punto, o un conjunto vacío (cuando ambas rectas son paralelas). En la Fig. (1.2) se observa un ejemplo geométrico de representación de la solución de un sistema lineal descrito por las ecuaciones\n",
    "\n",
    "$$\\begin{array}{lll}4x_{1}+4x_{2}&=&5\\\\ 2x_{1}-4x_{2}&=&1\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(1.6)$</p>\n",
    "\n",
    "donde el espacio solución es el punto $(x_{1},x_{2})=\\left( 1,\\frac{1}{4} \\right)$.\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"figures/fig_1_2.png\" width=\"400\"></p>\n",
    "<p style=\"text-align: center;\">Fig. (1.2): Representación de la solución de un sistema de dos ecuaciones lineales con solución única. La solución del sistema corresponde a la intersección de las rectas que resultan de cada ecuación del mismo</p>\n",
    "\n",
    "Similarmente, para un sistema de tres ecuaciones, cada ecuación describe un plano en el espacio $\\mathbb{R}^{3}$. Cuando intersectamos estos planos (satisfacer las tres ecuaciones de manera simultánea), podemos obtener un conjunto solución que puede ser un plano, una recta, un punto o un conjunto vacío (cuando los planos no tienen una intersección común).\n",
    "\n",
    "Para construir un enfoque sistemático a fin de resolver de manera general un sistema lineal de ecuaciones, introduciremos una notación compacta muy útil para estos efectos. Vamos a construir un arreglo vectorial con los coeficientes $a_{ij}$ y, a su vez, arreglaremos cada uno de estos vectores en una estructura más general, conocida como **matriz**. En otras palabras, escribiremos el sistema de ecuaciones (1.2) como\n",
    "\n",
    "$$\\left( \\begin{matrix}a_{11}\\\\ \\vdots \\\\ a_{m1}\\end{matrix} \\right)  x_{1}+\\left( \\begin{matrix}a_{12}\\\\ \\vdots \\\\ a_{m2}\\end{matrix} \\right)  x_{2}+\\cdots +\\left( \\begin{matrix}a_{1n}\\\\ \\vdots \\\\ a_{mn}\\end{matrix} \\right)  x_{n}=\\left( \\begin{matrix}b_{1}\\\\ \\vdots \\\\ b_{m}\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(1.7)$</p>\n",
    "\n",
    "El cual puede reordenarse conforme el uso de matrices como\n",
    "\n",
    "$$\\left( \\begin{matrix}a_{11}&\\cdots &a_{1n}\\\\ \\vdots &\\ddots &\\vdots \\\\ a_{m1}&\\cdots &a_{mn}\\end{matrix} \\right)  \\left( \\begin{matrix}x_{1}\\\\ \\vdots \\\\ x_{n}\\end{matrix} \\right)  =\\left( \\begin{matrix}b_{1}\\\\ \\vdots \\\\ b_{m}\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(1.8)$</p>\n",
    "\n",
    "En la siguiente subsección, nos detendremos a revisar el concepto de matriz y definiremos ciertas reglas para operar con ellas. Una vez hecho eso, volveremos al tema de los sistemas lineales de ecuaciones para mostrar como resolverlos usando estos maravillosos artilugios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a2347d",
   "metadata": {},
   "source": [
    "## Matrices.\n",
    "Las matrices juegan un papel fundamental en el álgebra lineal. Pueden ser utilizadas para representar de manera compacta sistemas de ecuaciones lineales, además de otras entidades con un trasfondo mucho más profundo, como es el caso de las trasformaciones lineales (y que veremos más adelante). Antes de discutir estos tópicos (que resultan ser ciertamente muy interesantes), primero definiremos qué es una matriz y qué podemos hacer con ellas. Veremos más propiedades de las matrices cuando comencemos a abordar temas un tanto más complejos y que tienen aplicaciones muy importantes en el contexto de la ciencia de datos, como las descomposiciones matriciales.\n",
    "\n",
    "**<font color='blue'>Definición 1.1 – Matriz:</font>** Sean $m$ y $n$ dos números naturales. Una matriz con valores reales de dimensión $m\\times n$, que denotamos como $\\mathbf{A}$, es un arreglo rectangular con $m$ filas y $n$ columnas, donde cada valor en la posición $(i, j)$ es denotado como $a_{ij}$, siendo $1\\leq i\\leq m$ y $1\\leq j\\leq n$ y $a_{ij}\\in \\mathbb{R}$. La matriz $\\mathbf{A}$ puede ser escrita entonces como\n",
    "\n",
    "$$\\mathbf{A} =\\left( \\begin{matrix}a_{11}&a_{12}&\\cdots &a_{1n}\\\\ a_{21}&a_{22}&\\cdots &a_{2n}\\\\ \\vdots &\\vdots &\\ddots &\\vdots \\\\ a_{m1}&a_{m2}&\\cdots &a_{mn}\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(1.9)$</p>\n",
    "\n",
    "Por convención, las matrices de dimensión $1\\times n$ son llamadas **matrices fila**, mientras que aquellas de dimensión $m\\times 1$ son llamadas **matrices columna**. El conjunto de todas las matrices con elementos reales de dimensión $m\\times n$ suele escribirse como $\\mathbb{M}_{\\mathbb{R}}(m,n)$ o $\\mathbb{R}^{m\\times n}$. Esta última notación suele utilizarse para representar que las matrices simplemente son *arreglos rectangulares* que pueden ser *redimensionados* de la forma que queramos, mientras mantengamos su **dimensión** constante; esto es, la multiplicación del número de filas y columnas, $mn$. Dicha dimensión suele denotarse como $\\dim(\\mathbf{A})$.\n",
    "\n",
    "Una matriz así definida suele definirse rápidamente como $\\mathbf{A}=\\left\\{ a_{ij}\\right\\}\\in \\mathbb{R}^{m\\times n}$.\n",
    "\n",
    "Una interpretación geométrica del redimensionamiento se observa en la Fig. (1.3). Librerías de Python especializadas en el análisis de datos como **<font color='darkmagenta'>Numpy</font>** hacen un uso intensivo del redimensionamiento a fin de compatibilizar arreglos de números para la realización de un sinnúmero de operaciones.\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"figures/fig_1_3.png\" width=\"200\"></p>\n",
    "<p style=\"text-align: center;\">Fig. (1.3): Representación geométrica del redimensionamiento de matrices</p>\n",
    "\n",
    "### Adición y multiplicación de matrices.\n",
    "Sean las matrices $\\mathbf{A}=\\left\\{ a_{ij}\\right\\}\\in \\mathbb{R}^{m\\times n}$ y $\\mathbf{B}=\\left\\{ b_{ij}\\right\\}\\in \\mathbb{R}^{m\\times n}$. La **suma** $\\mathbf{A}+\\mathbf{B}$ de ambas matrices da lugar a otra matriz, de las mismas dimensiones, definida como\n",
    "\n",
    "$$\\mathbf{A} +\\mathbf{B} :=\\left( \\begin{matrix}a_{11}+b_{11}&\\cdots &a_{1n}+b_{1n}\\\\ \\vdots &\\ddots &\\vdots \\\\ a_{m1}+b_{m1}&\\cdots &a_{mn}+b_{mn}\\end{matrix} \\right)  =\\left\\{ a_{ij}+b_{ij}\\right\\}  \\in \\mathbb{R}^{m\\times n} $$\n",
    "<p style=\"text-align: right;\">$(1.10)$</p>\n",
    "\n",
    "Por otro lado, sean las matrices $\\mathbf{A} =\\left\\{ a_{is}\\right\\}  \\in \\mathbb{R}^{n\\times k} ,\\mathbf{B} =\\left\\{ b_{sj}\\right\\}  \\in \\mathbb{R}^{k\\times n}$. Los elementos $\\left\\{ c_{ij}\\right\\}$ de la **matriz producto** $\\mathbf{C}=\\mathbf{A}\\mathbf{B}\\in \\mathbb{R}^{m\\times n}$ se definen como\n",
    "\n",
    "$$c_{ij}=\\sum^{k}_{s=1} a_{is}b_{sj}\\  ;\\  i=1,...,m\\wedge j=1,...,n$$\n",
    "<p style=\"text-align: right;\">$(1.11)$</p>\n",
    "\n",
    "Por lo tanto, para computar el elemento $\\left\\{ c_{ij}\\right\\}$ de la matriz $\\mathbf{C}$, multiplicamos los elementos de la $i$-ésima fila de $\\mathbf{A}$ con los elementos de la $j$-ésima columna de $\\mathbf{B}$, y luego sumamos todos los productos obtenidos. La multiplicación de matrices así definida pone de manifiesto que las matrices $\\mathbf{A}$ y $\\mathbf{B}$ deben ser **compatibles** para su realización. Por ejemplo, una matriz $\\mathbf{A}\\in \\mathbb{R}^{m\\times k}$ puede multiplicarse con otra matriz $\\mathbf{B}\\in \\mathbb{R}^{k\\times n}$, pero solamente de izquierda a derecha. Es decir,\n",
    "\n",
    "$$\\underbrace{\\mathbf{A} }_{m\\times k} \\  \\underbrace{\\mathbf{B} }_{k\\times n} =\\underbrace{\\mathbf{C} }_{m\\times n}$$\n",
    "<p style=\"text-align: right;\">$(1.12)$</p>\n",
    "\n",
    "El producto $\\mathbf{B}\\mathbf{A}$ no está definido si $m\\neq n$, ya que, de no ser así, las dimensiones respectivas no son compatibles.\n",
    "\n",
    "Cabe destacar que la multiplicación matricial, por lo tanto, no es una operación que se realiza componente a componente; es decir, $c_{ij}\\neq a_{ij}b_{ij}$ (incluso si el tamaño de las matrices $\\mathbf{A}$ y $\\mathbf{B}$ ha sido elegido apropiadamente). Este tipo de multiplicación aparece con frecuencia en lenguajes de programación cuando multiplicamos arreglos multidimensionales entre sí (por ejemplo, es característica de la multiplicación convencional de arreglos en **<font color='darkmagenta'>Numpy</font>**) y, formalmente, se conoce como producto de Hadamard. Dicho producto se denota como $\\mathbf{A} \\odot \\mathbf{B}$, y puede definirse como\n",
    "\n",
    "$$\\mathbf{A} \\odot \\mathbf{B} =\\left\\{ a_{ij}b_{ij}\\right\\}  =\\left( \\begin{matrix}a_{11}b_{11}&\\cdots &a_{1n}b_{1n}\\\\ \\vdots &\\ddots &\\vdots \\\\ a_{m1}b_{m1}&\\cdots &a_{mn}b_{mn}\\end{matrix} \\right)  \\in \\mathbb{R}^{m\\times n}$$\n",
    "<p style=\"text-align: right;\">$(1.13)$</p>\n",
    "\n",
    "**Ejemplo 1.3 – Una implementación del producto matricial en <font color='darkmagenta'>Numpy</font>:** Las matrices $\\mathbf{A}$ y $\\mathbf{B}$, definidas como\n",
    "\n",
    "$$\\mathbf{A} =\\left( \\begin{matrix}1&2&3\\\\ 3&2&1\\end{matrix} \\right)  \\  ;\\  \\mathbf{B} =\\left( \\begin{matrix}0&2\\\\ 1&-1\\\\ 0&1\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(1.14)$</p>\n",
    "\n",
    "son compatibles para la multiplicación en ambos sentidos. De esta manera, tenemos que:\n",
    "\n",
    "$$\\mathbf{A} \\mathbf{B} =\\left( \\begin{matrix}1&2&3\\\\ 3&2&1\\end{matrix} \\right)  \\left( \\begin{matrix}0&2\\\\ 1&-1\\\\ 0&1\\end{matrix} \\right)  =\\left( \\begin{matrix}2&3\\\\ 2&5\\end{matrix} \\right)  \\  ;\\  \\mathbf{B} \\mathbf{A} =\\left( \\begin{matrix}0&2\\\\ 1&-1\\\\ 0&1\\end{matrix} \\right)  \\left( \\begin{matrix}1&2&3\\\\ 3&2&1\\end{matrix} \\right)  =\\left( \\begin{matrix}6&4&2\\\\ -2&0&2\\\\ 3&2&1\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(1.15)$</p>\n",
    "\n",
    "Lo que nos permite verificar que la multiplicación matricial no es una operación conmutativa. Es decir, $\\mathbf{A} \\mathbf{B}\\neq \\mathbf{B} \\mathbf{A}$. Este hecho se ilustra en la Fig. (1.4).\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"figures/fig_1_4.png\" width=\"500\"></p>\n",
    "<p style=\"text-align: center;\">Fig. (1.4): Representación geométrica de la multiplicación de matrices</p>\n",
    "\n",
    "En **<font color='darkmagenta'>Numpy</font>**, es posible multiplicar matrices fácilmente haciendo uso del operador `@`, o bien, mediante la función `numpy.matmul()`. Si definimos las matrices anteriores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bb46b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb2e1268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos las matrices A y B.\n",
    "A = np.array([\n",
    "    [1, 2, 3],\n",
    "    [3, 2, 1]\n",
    "])\n",
    "B = np.array([\n",
    "    [0, 2],\n",
    "    [1, -1],\n",
    "    [0, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70393c24",
   "metadata": {},
   "source": [
    "Entonces tendremos que:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d545911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 3],\n",
       "       [2, 5]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multiplicación AB.\n",
    "A @ B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcdb5bea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6,  4,  2],\n",
       "       [-2,  0,  2],\n",
       "       [ 3,  2,  1]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multiplicación BA.\n",
    "B @ A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e765bc2c",
   "metadata": {},
   "source": [
    "Estos resultados, naturalmente, son los mismos que obtuvimos previamente. ◼︎\n",
    "\n",
    "**<font color='blue'>Definición 1.2 – Matriz identidad:</font>** En el conjunto $\\mathbb{R}^{n\\times n}$, definimos la matriz identidad $\\mathbf{I}_{n}$ como la matriz de $n\\times n$ que contiene únicamente 1s en su diagonal principal y 0s en el resto de sus posiciones. De esta manera, podemos escribir\n",
    "\n",
    "$$\\mathbf{I}_{n} :=\\left\\{ a_{ij}\\right\\}  \\  ;\\  a_{ij}=\\begin{cases}1&;\\  \\mathrm{si} \\  i=j\\\\ 0&;\\  \\mathrm{si} \\  i\\neq j\\end{cases}$$\n",
    "<p style=\"text-align: right;\">$(1.16)$</p>\n",
    "\n",
    "Ahora que hemos definido la adición y multiplicación de matrices, y la matriz identidad, repasaremos algunas de las propiedades que se pueden definir a partir de la propia aritmética subyacente a estas operaciones:\n",
    "\n",
    "- **(P1) – Asociatividad:** $\\forall \\mathbf{A} \\in \\mathbb{R}^{m\\times n} ,\\mathbf{B} \\in \\mathbb{R}^{n\\times p} ,\\mathbf{C} \\in \\mathbb{R}^{p\\times q} :\\  \\left( \\mathbf{A} \\mathbf{B} \\right)  \\mathbf{C} =\\mathbf{A} \\left( \\mathbf{B} \\mathbf{C} \\right)$\n",
    "- **(P2) – Distributividad:** $\\forall \\mathbf{A} ,\\mathbf{B} \\in \\mathbb{R}^{m\\times n} \\wedge \\mathbf{C} ,\\mathbf{D} \\in \\mathbb{R}^{n\\times p} :\\  \\left( \\mathbf{A} +\\mathbf{B} \\right)  \\mathbf{C} =\\mathbf{A} \\mathbf{C} +\\mathbf{B} \\mathbf{C}$\n",
    "- **(P3) – Elemento neutro:** $\\forall \\mathbf{A} \\in \\mathbb{R}^{m\\times n} :\\  \\mathbf{I}_{m} \\mathbf{A} =\\mathbf{A} \\mathbf{I}_{n} =\\mathbf{A}$\n",
    "\n",
    "Notemos que, en (P3), $\\mathbf{I}_{m}\\neq \\mathbf{I}_{n}$ si $m\\neq n$.\n",
    "\n",
    "### Matriz inversa y transpuesta.\n",
    "Vamos a ampliar el conjunto de operaciones algebraicas disponibles para las matrices introduciendo dos conceptos nuevos aplicables a este tipo de objetos.\n",
    "\n",
    "**<font color='blue'>Definición 1.3 – Matriz inversa:</font>** Consideremos una **matriz cuadrada** (esto es, una matriz con el mismo número de filas que de columnas) denotada como $\\mathbf{A}\\in \\mathbb{R}^{n\\times n}$. Sea $\\mathbf{b}\\in \\mathbb{R}^{n\\times n}$ otra matriz cuadrada tal que $\\mathbf{A}\\mathbf{B}=\\mathbf{B}\\mathbf{A}=\\mathbf{I}_{n}$. La matriz $\\mathbf{B}$ es llamada **inversa** de $\\mathbf{A}$, y es denotada como $\\mathbf{A}^{-1}$.\n",
    "\n",
    "Desafortunadamente, no toda matriz $\\mathbf{A}$ posee una inversa $\\mathbf{A}^{-1}$. Si tal inversa existe, la matriz $\\mathbf{A}$ se denomina **invertible** o **no singular**. Además, en caso de que la inversa exista, ésta siempre es única. Cuando retomemos el estudio de la resolución de un sistema de ecuaciones lineales, veremos un método general para calcular la inversa de cualquier matriz no singular.\n",
    "\n",
    "Sin embargo, veamos el caso particular del cálculo de la inversa para una matriz cuadrada $\\mathbf{A}\\in \\mathbb{R}^{2\\times 2}$. En este caso, tenemos que\n",
    "\n",
    "$$\\mathbf{A} \\mathbf{A}^{-1} =\\left( \\begin{matrix}1&0\\\\ 0&1\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(1.17)$</p>\n",
    "\n",
    "Por lo tanto, podemos escribir\n",
    "\n",
    "$$\\mathbf{A} \\mathbf{A}^{-1} =\\left( \\begin{matrix}a_{11}a_{22}-a_{12}a_{21}&0\\\\ 0&a_{11}a_{22}-a_{12}a_{21}\\end{matrix} \\right)  =\\left( a_{11}a_{22}-a_{12}a_{21}\\right)  \\mathbf{I}_{2}$$\n",
    "<p style=\"text-align: right;\">$(1.18)$</p>\n",
    "\n",
    "Así que, al final, obtenemos\n",
    "\n",
    "$$\\mathbf{A} \\mathbf{A}^{-1} =\\frac{1}{a_{11}a_{22}-a_{12}a_{21}} \\left( \\begin{matrix}a_{22}&-a_{12}\\\\ -a_{21}&a_{11}\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(1.19)$</p>\n",
    "\n",
    "Lo que se cumple si y sólo si $a_{11}a_{22}-a_{12}a_{21}\\neq 0$. Más adelante, al abordar el concepto de descomposición matricial, veremos que la cantidad $a_{11}a_{22}-a_{12}a_{21}$ es llamada **determinante** de la matriz $\\mathbf{A}\\in \\mathbb{R}^{2\\times 2}$. Además, verificaremos que la existencia de dicho determinante, y que éste no sea nulo, son condiciones necesarias y suficientes para determinar la existencia de la inversa de una matriz cuadrada.\n",
    "\n",
    "**<font color='blue'>Definición 1.4 – Matriz transpuesta:</font>** Para la matriz $\\mathbf{A}\\in \\mathbb{R}^{m\\times n}$, se tendrá que la matriz $\\mathbf{B}\\in \\mathbb{R}^{n\\times m}$ cuyos elementos son tales que $b_{ji}=a_{ij}$, es llamada **matriz transpuesta** de $\\mathbf{A}$, y se denota como $\\mathbf{B}=\\mathbf{A}^{\\top }$. Es decir, la matriz transpuesta $\\mathbf{A}^{\\top }$ resulta simplemente de intercambiar las filas por las columnas de $\\mathbf{A}$.\n",
    "\n",
    "A continuación, se listan algunas importantes propiedades de las matrices inversas y transpuestas:\n",
    "\n",
    "- **(P1):** $\\mathbf{A} \\mathbf{A}^{-1} =\\mathbf{A}^{-1} \\mathbf{A} =\\mathbf{I}_{n} \\  ;\\  \\forall \\mathbf{A} \\in \\mathbb{R}^{n\\times n}$.\n",
    "- **(P2):** $\\left( \\mathbf{A} \\mathbf{B} \\right)^{-1}  =\\mathbf{B}^{-1} \\mathbf{A}^{-1} \\  ;\\  \\forall \\mathbf{A} ,\\mathbf{B} \\in \\mathbb{R}^{n\\times n}$.\n",
    "- **(P3):** $\\left( \\mathbf{A} +\\mathbf{B} \\right)^{-1}  \\neq \\mathbf{A}^{-1} +\\mathbf{B}^{-1} \\  ;\\  \\forall \\mathbf{A} ,\\mathbf{B} \\in \\mathbb{R}^{n\\times n}$.\n",
    "- **(P4):** $\\left( \\mathbf{A}^{\\top } \\right)^{\\top }  =\\mathbf{A} \\  ;\\  \\forall \\mathbf{A} \\in \\mathbb{R}^{n\\times n}$.\n",
    "- **(P5):** $\\left( \\mathbf{A} +\\mathbf{B} \\right)^{\\top }  =\\mathbf{A}^{\\top } +\\mathbf{B}^{\\top } \\  ;\\  \\forall \\mathbf{A} ,\\mathbf{B} \\in \\mathbb{R}^{n\\times n}$.\n",
    "- **(P6):** $\\left( \\mathbf{A} \\mathbf{B} \\right)^{\\top }  =\\mathbf{B}^{\\top } \\mathbf{A}^{\\top } \\  ;\\  \\forall \\mathbf{A} ,\\mathbf{B} \\in \\mathbb{R}^{n\\times n}$.\n",
    "\n",
    "**<font color='blue'>Definición 1.5 – Matriz simétrica:</font>** Sea la matriz cuadrada $\\mathbf{A} \\in \\mathbb{R}^{n\\times n}$. Diremos que $\\mathbf{A}$ es **simétrica** si $\\mathbf{A}=\\mathbf{A}^{\\top}$.\n",
    "\n",
    "Notemos que, naturalmente, sólo las matrices cuadradas pueden ser simétricas. Además, si una matriz cuadrada es invertible, es posible demostrar que su transpuesta también lo es.\n",
    "\n",
    "**Ejemplo 1.4 – Transposición e inversión de matrices en <font color='darkmagenta'>Numpy</font>:** En <font color='darkmagenta'>Numpy</font> es posible transponer e invertir matrices de manera sencilla, aprovechando la flexibilidad del objeto `numpy.ndarray` y su capacidad de representar matrices cuando éste es bidimensional. De esta manera, si consideramos, por ejemplo, la matriz $\\mathbf{A}$ definida como\n",
    "\n",
    "$$\\mathbf{A} =\\left( \\begin{matrix}-1&0&2&1\\\\ -4&9&-1&-8\\\\ 0&1&0&-4\\\\ 5&-6&0&3\\end{matrix} \\right)  \\in \\mathbb{R}^{4\\times 4}$$\n",
    "<p style=\"text-align: right;\">$(1.20)$</p>\n",
    "\n",
    "Ésta puede definirse en <font color='darkmagenta'>Numpy</font> como:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb72e714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos la matriz A.\n",
    "A = np.array([\n",
    "    [-1, 0, 2, 1],\n",
    "    [-4, 9, -1, -8],\n",
    "    [0, 1, 0, -4],\n",
    "    [5, -6, 0, 3],\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372b366b",
   "metadata": {},
   "source": [
    "La transpuesta de `A` puede obtenerse por medio del atributo `T`. Mientras que su inversa puede calcularse rápidamente usando la función `numpy.invert()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7fb492a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1, -4,  0,  5],\n",
       "       [ 0,  9,  1, -6],\n",
       "       [ 2, -1,  0,  0],\n",
       "       [ 1, -8, -4,  3]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transpuesta de A.\n",
    "A.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21215969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,  -1,  -3,  -2],\n",
       "       [  3, -10,   0,   7],\n",
       "       [ -1,  -2,  -1,   3],\n",
       "       [ -6,   5,  -1,  -4]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inversa de A.\n",
    "np.invert(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca3985d",
   "metadata": {},
   "source": [
    "◼︎\n",
    "\n",
    "### Multiplicación de una matriz por un escalar.\n",
    "Sea $\\mathbf{A}=\\left\\{ a_{ij}\\right\\}  \\in \\mathbb{R}^{m\\times n}$ y $\\lambda \\in \\mathbb{R}$. Entonces se tiene que $\\lambda \\mathbf{A}=\\mathbf{K}\\in \\mathbb{R}^{m\\times n}$, donde $k_{ij}=\\lambda a_{ij}$. Por lo tanto, la multiplicación de una matriz $\\mathbf{A}$ por un escalar $\\lambda$ resulta en otra matriz $\\mathbf{K}$, donde cada uno de sus elementos $k_{ij}$ no es más que el correspondiente elemento $a_{ij}$ de $\\mathbf{A}$ multiplicado por $\\lambda$.\n",
    "\n",
    "Para $\\lambda, \\psi \\in \\mathbb{R}$, se cumplen las siguientes propiedades:\n",
    "\n",
    "- **(P1) – Asociatividad:** $\\left( \\lambda \\psi \\right)  \\mathbf{C} =\\lambda \\left( \\psi \\mathbf{C} \\right)  ;\\  \\mathbf{C} \\in \\mathbb{R}^{m\\times n} \\wedge \\lambda \\left( \\mathbf{B} \\mathbf{C} \\right)  =\\left( \\lambda \\mathbf{B} \\right)  \\mathbf{C} =\\mathbf{B} \\left( \\lambda \\mathbf{C} \\right)  =\\left( \\mathbf{B} \\mathbf{C} \\right)  \\lambda ;\\  \\mathbf{B} \\in \\mathbb{R}^{m\\times n} ,\\mathbf{C} \\in \\mathbb{R}^{m\\times k}$.\n",
    "- **(P2):** $\\left( \\lambda \\mathbf{C} \\right)^{\\top }  =\\mathbf{C}^{\\top } \\lambda^{\\top } =\\mathbf{C}^{\\top } \\lambda =\\lambda \\mathbf{C}^{\\top } ;\\  \\mathbf{C} \\in \\mathbb{R}^{m\\times n}$, ya que $\\lambda^{\\top } =\\lambda ;\\  \\forall \\lambda \\in \\mathbb{R}$.\n",
    "- **(P3) – Distributividad:** $\\left( \\lambda +\\psi \\right)  \\mathbf{C} =\\lambda \\mathbf{C} +\\psi \\mathbf{C} ;\\  \\mathbf{C} \\in \\mathbb{R}^{m\\times n} \\wedge \\lambda \\left( \\mathbf{B} +\\mathbf{C} \\right)  =\\lambda \\mathbf{B} +\\lambda \\mathbf{C} ;\\  \\mathbf{B} ,\\mathbf{C} \\in \\mathbb{R}^{m\\times n}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ad7211",
   "metadata": {},
   "source": [
    "## Solución de un sistema lineal de ecuaciones.\n",
    "\n",
    "### Representación matricial de un sistema lineal de ecuaciones.\n",
    "Si consideramos el siguiente sistema lineal de ecuaciones\n",
    "\n",
    "$$\\begin{array}{rcl}a_{11}x_{1}+a_{12}x_{2}+\\cdots +a_{1n}x_{n}&=&b_{1}\\\\ a_{21}x_{1}+a_{22}x_{2}+\\cdots +a_{2n}x_{n}&=&b_{2}\\\\ &\\vdots &\\\\ a_{m1}x_{1}+a_{m2}x_{2}+\\cdots +a_{mn}x_{n}&=&b_{m}\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(1.21)$</p>\n",
    "\n",
    "Y usamos las reglas de la multiplicación matricial, podemos escribir dicho sistema de una forma más compacta como\n",
    "\n",
    "$$\\left( \\begin{matrix}a_{11}&a_{12}&\\cdots &a_{1n}\\\\ a_{21}&a_{22}&\\cdots &a_{2n}\\\\ \\vdots &\\vdots &\\ddots &\\vdots \\\\ a_{m1}&a_{m2}&\\cdots &a_{mn}\\end{matrix} \\right)  \\left( \\begin{matrix}x_{1}\\\\ x_{2}\\\\ \\vdots \\\\ x_{n}\\end{matrix} \\right)  =\\left( \\begin{matrix}b_{1}\\\\ b_{2}\\\\ \\vdots \\\\ b_{m}\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(1.22)$</p>\n",
    "\n",
    "En general, un sistema de $m$ ecuaciones lineales con $n$ incógnitas puede escribirse de manera compacta como $\\mathbf{A} \\mathbf{x} =\\mathbf{b}$, donde $\\mathbf{A} \\in \\mathbb{R}^{m\\times n}$, $\\mathbf{x} \\in \\mathbb{R}^{n\\times 1}$ y $\\mathbf{b} \\in \\mathbb{R}^{m\\times 1}$. Los números $a_{ij}, b_{i}\\in \\mathbb{R}$ son parámetros conocidos y el conjunto $\\left\\{ x_{j}\\right\\}^{n}_{j=1}$ contiene las **incógnitas** del sistema. A continuación, nos enfocaremos en la solución de sistemas como (1.21), describiendo un algoritmo general para ello que, además, nos permitirá determinar la inversa de una matriz no singular.\n",
    "\n",
    "**Ejemplo 1.5 - Solución general y particular de un sistema:** Antes de discutir cómo resolver un sistema lineal de ecuaciones, consideremos un ejemplo preliminar:\n",
    "\n",
    "$$\\left( \\begin{matrix}1&0&8&-4\\\\ 0&1&2&12\\end{matrix} \\right)  \\left( \\begin{matrix}x_{1}\\\\ x_{2}\\\\ x_{3}\\\\ x_{4}\\end{matrix} \\right)  =\\left( \\begin{matrix}42\\\\ 8\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(1.23)$</p>\n",
    "\n",
    "El sistema (1.23) tiene dos ecuaciones y cuatro incógnitas. Por lo tanto, en general, esperaríamos que éste tenga infinitas soluciones. Dicho sistema se ha presentado en una forma que resulta bastante sencilla, puesto que las primeras dos columnas consisten únicamente de 1s y 0s. Recordemos que queremos encontrar escalares $x_{1},...,x_{4}$ tales que $\\sum\\nolimits^{4}_{j=1} \\mathbf{c}_{j} x_{j}=\\mathbf{b}$, donde $\\mathbf{c}_{j}$ es la $j$-ésima columna de la matriz $\\mathbf{A}$, que a su vez se conoce como **matriz de coeficientes del sistema**, mientras que $\\mathbf{b}$ es la matriz columna que se encuentra a la derecha del sistema (1.23) tomando 42 veces la primera columna y 8 veces la segunda, de manera que\n",
    "\n",
    "$$\\mathbf{b} =\\left( \\begin{matrix}42\\\\ 8\\end{matrix} \\right)  =42\\left( \\begin{matrix}1\\\\ 0\\end{matrix} \\right)  +8\\left( \\begin{matrix}0\\\\ 1\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(1.24)$</p>\n",
    "\n",
    "De esta manera, el vector $\\mathbf{x}=(42,8,0,0)^{\\top}$ es una solución del sistema (1.23). Una solución de este tipo es llamada **solución particular** del sistema respectivo. Sin embargo, esta no es la única solución de (1.23). Para capturar el resto de las soluciones, necesitamos ser algo creativos, generando convenientemente un cero por medio de las columnas de la matriz de coeficientes del sistema. Para ello, expresamos la tercera columna por medio del uso de las primeras dos de la forma\n",
    "\n",
    "$$\\left( \\begin{matrix}8\\\\ 2\\end{matrix} \\right)  =8\\left( \\begin{matrix}1\\\\ 0\\end{matrix} \\right)  +2\\left( \\begin{matrix}0\\\\ 1\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(1.25)$</p>\n",
    "\n",
    "Por lo tanto, se tiene que $\\mathbf{0} =8\\mathbf{c}_{1} +2\\mathbf{c}_{2} -1\\mathbf{c}_{3} +0\\mathbf{c}_{4}$ y $\\left( x_{1},x_{2},x_{3},x_{4}\\right)  =\\left( 8,2,-1,0\\right)$. De hecho, cualquier escalamiento a esta solución por un factor $\\lambda \\in \\mathbb{R}$ produce el vector $\\mathbf{0}$, ya que\n",
    "\n",
    "$$\\left( \\begin{matrix}1&0&8&-4\\\\ 0&1&2&12\\end{matrix} \\right)  \\left( \\lambda_{1} \\left( \\begin{matrix}8\\\\ 2\\\\ -1\\\\ 0\\end{matrix} \\right)  \\right)  =\\lambda_{1} \\left( 8\\mathbf{c}_{1} +2\\mathbf{c}_{2} -\\mathbf{c}_{3} \\right)  =\\mathbf{0}$$\n",
    "<p style=\"text-align: right;\">$(1.26)$</p>\n",
    "\n",
    "Siguiendo el mismo razonamiento, expresamos la cuarta columna de la matriz de coeficientes del sistema usando sus primeras dos columnas como\n",
    "\n",
    "$$\\left( \\begin{matrix}1&0&8&-4\\\\ 0&1&2&12\\end{matrix} \\right)  \\left( \\lambda_{2} \\left( \\begin{matrix}-4\\\\ 12\\\\ 0\\\\ -1\\end{matrix} \\right)  \\right)  =\\lambda_{2} \\left( -4\\mathbf{c}_{1} +12\\mathbf{c}_{2} -\\mathbf{c}_{4} \\right)  =\\mathbf{0}$$\n",
    "<p style=\"text-align: right;\">$(1.27)$</p>\n",
    "\n",
    "Donde $\\lambda_{2}\\in \\mathbb{R}$. Uniendo las expresiones encontradas en (1.26) y (1.27), podemos construir el **conjunto solución** (con infinitos elementos) del sistema (1.23), denominado **solución general** $S$ del mismo, de manera tal que\n",
    "\n",
    "$$S=\\left\\{ \\mathbf{x} \\in \\mathbb{R}^{4} :\\mathbf{x} =\\left( \\begin{matrix}42\\\\ 8\\\\ 0\\\\ 0\\end{matrix} \\right)  +\\lambda_{1} \\left( \\begin{matrix}8\\\\ 2\\\\ -1\\\\ 0\\end{matrix} \\right)  +\\lambda_{2} \\left( \\begin{matrix}-4\\\\ 12\\\\ 0\\\\ -1\\end{matrix} \\right)  ;\\lambda_{1} ,\\lambda_{2} \\in \\mathbb{R} \\right\\}$$\n",
    "<p style=\"text-align: right;\">$(1.28)$</p>\n",
    "\n",
    "◼︎\n",
    "\n",
    "El procedimiento general que puede desprenderse del ejemplo anterior es el siguiente:\n",
    "\n",
    "- Encontrar una solución particular del sistema $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$.\n",
    "- Encontrar todas las soluciones del sistema $\\mathbf{A}\\mathbf{x}=\\mathbf{0}$.\n",
    "- Combinar las soluciones encontradas en los pasos anteriores para construir la solución general.\n",
    "\n",
    "Por extensión, ni la solución general ni la solución particular son únicas.\n",
    "\n",
    "El sistema de ecuaciones (1.23) fue fácil de resolver, porque la matriz $\\mathbf{A}$ de coeficientes del sistema tenía una forma particular en sus primeras dos columnas que permitía su solución por medio de simple inspección. Sin embargo, en general, los sistemas de ecuaciones lineales no tienen esta forma tan particular. Afortunadamente, existe un método que permite transformar cualquier sistema de ecuaciones lineales en uno del tipo visto en (1.23) (de hecho, tal estructura es conocida en álgebra como *matriz triangular superior*), conocido como **eliminación Gaussiana**. Su piedra fundamental está constituida por una serie de operaciones algebraicas conocidas como **transformaciones elementales de una matriz**, las que permiten, en general, transformar cualquier matriz arbitraria en una matriz triangular inferior.\n",
    "\n",
    "**<font color='blue'>Definición 1.6 – Diagonal principal:</font>** Sea $\\mathbf{A}=\\left\\{ a_{ij}\\right\\}  \\in \\mathbb{R}^{n\\times n}$ una matriz cuadrada de orden $n$. Se define la diagonal principal de $\\mathbf{A}$ como el conjunto $\\left\\{ a_{ij}|\\  i=j\\right\\}$. La suma de los elementos que constituyen dicha diagonal principal es conocida como traza de la matriz $\\mathbf{A}$, y se denota como $\\mathrm{tr}(\\mathbf{A})$.\n",
    "\n",
    "**<font color='blue'>Definición 1.7 – Matriz triangular:</font>** Una **matriz triangular** es un tipo especial de matriz cuadrada cuyos elementos por encima o por debajo de su diagonal principal son cero. Si los elementos nulos se ubican por debajo de la diagonal principal, la matriz es llamada **triangular superior**, y toma la forma\n",
    "\n",
    "$$\\mathbf{U} =\\left( \\begin{matrix}u_{11}&u_{12}&u_{13}&\\cdots &u_{1n}\\\\ 0&u_{22}&u_{23}&\\cdots &u_{2n}\\\\ 0&0&u_{33}&\\cdots &u_{3n}\\\\ \\vdots &\\vdots &\\vdots &\\ddots &\\vdots \\\\ 0&0&0&\\cdots &u_{nn}\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(1.29)$</p>\n",
    "\n",
    "Por otro lado, si los elementos nulos se ubican en la parte superior de la diagonal principal, la matriz es llamada **triangular inferior**, y toma la forma\n",
    "\n",
    "$$\\mathbf{L} =\\left( \\begin{matrix}l_{11}&0&0&\\cdots &0\\\\ l_{21}&l_{22}&0&\\cdots &0\\\\ l_{31}&l_{32}&l_{33}&\\cdots &0\\\\ \\vdots &\\vdots &\\vdots &\\ddots &\\vdots \\\\ l_{n1}&l_{n2}&l_{n3}&\\cdots &l_{nn}\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(1.30)$</p>\n",
    "\n",
    "### Transformaciones elementales sobre una matriz.\n",
    "Las transformaciones elementales de una matriz corresponden a tres operaciones válidas sobre cualquier matriz cuyo objetivo es, como comentamos previamente, llegar a una matriz triangular superior. Dichas operaciones son las siguientes:\n",
    "\n",
    "- Multiplicar una fila cualquiera de una matriz por un escalar $c\\neq 0$. Esta transformación suele denotarse como $F_{i}(c)$, donde $F_{i}$ hace referencia a la fila $i$-ésima que se multiplica por el escalar $c$.\n",
    "- Sumar a una fila de una matriz un múltiplo de otra fila. Esta transformación suele denotarse como $F_{ij}(c)$, donde se referencia que la fila $i$ se multiplica por $c$ y el resultado se suma a la fila $j$.\n",
    "- Intercambiar dos filas cualquiera en una matriz. Esto suele denotarse como $F_{ij}$.\n",
    "\n",
    "Estas tres operaciones elementales son utilizadas para trabajar las matrices que contienen los coeficientes de un sistema lineal de ecuaciones, a fin de transformarlas en matrices triangulares superiores que permiten resolver muy fácilmente tales sistemas mediante sustituciones regresivas. Este trabajo de sustitución hacia atrás, para el caso de un sistema de $n$ ecuaciones con $n$ incógnitas, toma $n(n-1)/2$ multiplicaciones al reemplazar las incógnitas ya calculadas, y $n$ divisiones por los elementos de la diagonal principal respectiva resultantes después de las operaciones elementales, a fin de despejar la incógnita $x_{i}$.\n",
    "\n",
    "### Método de eliminación Gaussiana.\n",
    "La combinación del uso de las transformaciones elementales sobre una matriz y la construcción de una matriz triangular superior para la resolución de un sistema lineal de ecuaciones por simple sustitución regresiva se conoce como **método de eliminación Gaussiana**. Corresponde a un algoritmo que suele ser utilizado para la resolución analítica de sistemas lineales a nivel computacional por una gran cantidad de paquetes informáticos. Estos, por supuesto, incluyen a librerías de Python, como <font color='darkmagenta'>Numpy</font> y <font color='darkmagenta'>Scipy</font>.\n",
    "\n",
    "**<font color='blue'>Definición 1.8 – Matriz ampliada de un sistema:</font>** Consideremos un sistema lineal de ecuaciones del tipo $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$, donde $\\mathbf{A}$ es la matriz de coeficientes del sistema. La matriz resultante de añadir $\\mathbf{b}$ a la derecha de la última columna de $\\mathbf{A}$, y denotada como $[\\mathbf{A}|\\mathbf{b}]$, se conoce como la **matriz ampliada del sistema**.\n",
    "\n",
    "**<font color='blue'>Definición 1.9 – Pivote:</font>** El elemento $a_{qq}\\neq 0$ utilizado para eliminar los elementos $a_{rq}$ para $r=q+1,q+2,...,n$ en la matriz $\\mathbf{A}\\in \\mathbb{R}^{n\\times n}$ es llamado **elemento pivote** de la fila $q$, donde $1\\leq q\\leq n$.\n",
    "\n",
    "**<font color='blue'>Definición 1.10 – Multiplicadores:</font>** Los números $m_{rq}=a_{rq}/a_{qq}$ por los cuales se multiplica la fila que contiene a un elemento pivote para luego aplicar la operación elemental relativa a sumar o restar a la fila $r$, con $r=q+1,q+2,...,n$, a fin de llegar a una matriz triangular (superior), se conocen como **multiplicadores**.\n",
    "\n",
    "Las operaciones elementales, junto con los elementos pivotes y los multiplicadores, nos permiten, cuando esto sea posible, transformar la matriz ampliada de un sistema de ecuaciones lineales, en una matriz triangular superior (o inferior, si se prefiere) y resolver el sistema equivalente, por sustitución regresiva (o progresiva, si se quiere).\n",
    "\n",
    "**Ejemplo 1.6 - Aplicación del método de eliminación Gaussiana:** Vamos a resolver el siguiente sistema:\n",
    "\n",
    "$$\\begin{array}{rcl}2x_{1}+3x_{2}+2x_{3}+4x_{4}&=&4\\\\ 4x_{1}+10x_{2}-4x_{3}&=&-8\\\\ -3x_{1}-2x_{2}-5x_{3}-2x_{4}&=&-4\\\\ -2x_{1}+4x_{2}+4x_{3}-7x_{4}&=&-1\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(1.31)$</p>\n",
    "\n",
    "La matriz ampliada de este sistema es la siguiente:\n",
    "\n",
    "$$\\left[ \\mathbf{A} |\\mathbf{b} \\right]  =\\left( \\begin{matrix}2&3&2&4&4\\\\ 4&10&-4&0&-8\\\\ -3&-2&-5&-2&-4\\\\ -2&4&4&-7&-1\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(1.32)$</p>\n",
    "\n",
    "El elemento pivote de la primera fila es $a_{11}=2$, y los multiplicadores respectivos son $m_{21}=\\frac{a_{21}}{a_{11}}=2, m_{31}=\\frac{a_{31}}{a_{11}}=-\\frac{3}{2}$ y $m_{41}=\\frac{a_{41}}{a_{11}}=-1$. Tomando la primera fila para eliminar los elementos ubicados en la primera columna, debajo del elemento diagonal, se tiene que\n",
    "\n",
    "$$\\left( \\begin{matrix}2&3&2&4&4\\\\ 4&10&-4&0&-8\\\\ -3&-2&-5&-2&-4\\\\ -2&4&4&-7&-1\\end{matrix} \\right)  \\overbrace{=}^{\\begin{array}{l}F_{12}\\left( 2\\right)  \\\\ F_{13}\\left( -3/2\\right)  \\\\ F_{14}\\left( -1\\right)  \\end{array} } \\left( \\begin{matrix}2&3&2&4&4\\\\ 0&4&-8&-8&-16\\\\ 0&5/2&-2&4&2\\\\ 0&7&6&-3&3\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(1.33)$</p>\n",
    "\n",
    "Para la segunda fila, el elemento pivote es $a_{22}=4$, y los multiplicadores son $m_{32}=\\frac{5}{8}$ y $m_{42}=\\frac{7}{4}$. Luego tenemos,\n",
    "\n",
    "$$\\left( \\begin{matrix}2&3&2&4&4\\\\ 0&4&-8&-8&-16\\\\ 0&5/2&-2&4&2\\\\ 0&7&6&-3&3\\end{matrix} \\right)  \\overbrace{=}^{\\begin{array}{l}F_{23}\\left( 5/8\\right)  \\\\ F_{24}\\left( 7/4\\right)  \\end{array} } \\left( \\begin{matrix}2&3&2&4&4\\\\ 0&4&-8&-8&-16\\\\ 0&0&3&9&12\\\\ 0&0&20&11&31\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(1.34)$</p>\n",
    "\n",
    "Finalmente, para la tercera fila, el elemento pivote es $a_{33}=3$ y el correspondiente multiplicador es $m_{43}=\\frac{20}{3}$. Por lo tanto, aplicando nuevamente operaciones elementales a nuestra matriz ampliada, obtenemos\n",
    "\n",
    "$$\\left( \\begin{matrix}2&3&2&4&4\\\\ 0&4&-8&-8&-16\\\\ 0&0&3&9&12\\\\ 0&0&20&11&31\\end{matrix} \\right)  \\overbrace{=}^{F_{34}\\left( 20/3\\right)  } \\left( \\begin{matrix}2&3&2&4&4\\\\ 0&4&-8&-8&-16\\\\ 0&0&3&9&12\\\\ 0&0&0&-49&-49\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(1.35)$</p>\n",
    "\n",
    "La matriz ampliada anterior, transformada mediante operaciones elementales, permite obtener el siguiente sistema triangular superior, equivalente al sistema (1.31):\n",
    "\n",
    "$$\\begin{array}{rcl}2x_{1}+3x_{2}+2x_{3}+4x_{4}&=&4\\\\ 4x_{2}-8x_{3}-8x_{4}&=&-16\\\\ 3x_{3}+9x_{4}&=&12\\\\ -49x_{4}&=&-49\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(1.36)$</p>\n",
    "\n",
    "Por lo tanto, mediante una sustitución regresiva sencilla, podemos determinar que el conjunto solución del sistema (1.31) es el vector $\\mathbf{x}=(x_{1},x_{2},x_{3},x_{4})=(-1,0,1,1)$. ◼︎\n",
    "\n",
    "**Ejemplo 1.7 – Implementación *high-level* en <font color='darkmagenta'>Numpy</font>:** Es posible resolver fácilmente sistemas lineales de ecuaciones en <font color='darkmagenta'>Numpy</font> por medio de la función `solve()`, perteneciente al módulo de álgebra lineal `numpy.linalg`. Podemos resolver rápidamente el mismo sistema (1.31), definiendo previamente la matriz de coeficientes del sistema $\\mathbf{A}$ y la matriz de valores dependientes $\\mathbf{b}$ como sigue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bf0caa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos la matriz de coeficientes del sistema.\n",
    "A = np.array([\n",
    "    [2, 3, 2, 4],\n",
    "    [4, 10, -4, 0],\n",
    "    [-3, -2, -5, -2],\n",
    "    [-2, 4, 4, -7],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb224a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos la matriz de valores dependientes del sistema.\n",
    "b = np.array([4, -8, -4, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdf0170",
   "metadata": {},
   "source": [
    "Finalmente, aplicamos la función `solve()` para resolver el sistema correspondiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50324d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos el vector solución del sistema.\n",
    "x = np.linalg.solve(A, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66b7efbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La solución del sistema es x = [-1.  0.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "# Mostramos la solución en pantalla.\n",
    "print(f\"La solución del sistema es x = {x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394fe035",
   "metadata": {},
   "source": [
    "Tal como queríamos demostrar. En este caso... ¡la librería <font color='darkmagenta'>Numpy</font> nos ha ahorrado un montón de trabajo!\n",
    "\n",
    "Podemos comprobar rápidamente esta solución por medio de la función `numpy.allclose()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdf68ab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(x, np.linalg.solve(A, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b8562d",
   "metadata": {},
   "source": [
    "Lo que definitivamente resuelve nuestro problema. ◼︎\n",
    "\n",
    "**Ejemplo 1.8 – Implementación *low-level* en <font color='darkmagenta'>Numpy</font>:** Vamos a complicarnos un poco más la vida y resolveremos el mismo sistema anterior, pero utilizando únicamente funciones básicas de <font color='darkmagenta'>Numpy</font>, prescindiendo de su módulo de álgebra lineal (`numpy.linalg`). Haremos esto únicamente para ir entrenándonos (y comprendiendo de mejor forma) las herramientas que tenemos a la mano en una librería tan bella como <font color='darkmagenta'>Numpy</font>.\n",
    "\n",
    "Para ello, definiremos una sencilla función para resolver este problema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6112eff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos una función que implementará el método de eliminación Gaussiana.\n",
    "def gaussian_elimination(A: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Una función que aplicará el método de elimninación Gaussiana para resolver un\n",
    "    sistema lineal de ecuaciones, con la única condición de que el número de\n",
    "    incógnitas del sistema sea el mismo que el número de ecuaciones.\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    A : Matriz de coeficientes del sistema en la forma de un arreglo 2D de Numpy.\n",
    "    b : Matriz de valores dependientes del sistema en la forma de un arreglo 1D de \n",
    "    Numpy.\n",
    "    \n",
    "    Retorna:\n",
    "    --------\n",
    "    x : Arreglo de Numpy donde se almacenan las soluciones del sistema.\n",
    "    \"\"\"\n",
    "    # Determinamos el número de ecuaciones del sistema.\n",
    "    n = A.shape[0]\n",
    "    \n",
    "    # Construimos la matriz ampliada del sistema.\n",
    "    M = np.hstack((A, b.reshape(-1, 1)))\n",
    "    \n",
    "    # Recorremos las filas de la matriz ampliada.\n",
    "    for i in range(n):\n",
    "        # Obtenemos la posición asociada al máximo elemento pivote.\n",
    "        max_element_index = np.abs(M[i:, i]).argmax() + i\n",
    "        \n",
    "        # Detenemos el proceso si la matriz es singular (no tiene inversa).\n",
    "        if M[max_element_index, i] == 0:\n",
    "            raise ValueError(\"La matriz ampliada del sistema no tiene inversa.\")\n",
    "        \n",
    "        # Calculamos los multiplicadores.\n",
    "        M[[i, max_element_index]] = M[[max_element_index, i]]\n",
    "        M[i] = M[i] / M[i, i]\n",
    "        \n",
    "        # Aplicamos las operaciones elementales conforme los multiplicadores\n",
    "        # calculados previamente.\n",
    "        for j in range(i + 1, n):\n",
    "            M[j] = M[j] - M[j, i] * M[i]\n",
    "    \n",
    "    # Definimos la matriz x donde almacenaremos las soluciones.\n",
    "    x = np.zeros(n)\n",
    "    \n",
    "    # Por sustitución regresiva, calculamos los valores de x.\n",
    "    for i in range(n - 1, -1, -1):\n",
    "        x[i] = M[i, -1] - np.sum(M[i, i + 1:n] * x[i + 1:n])\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f59c977",
   "metadata": {},
   "source": [
    "Ya sólo nos resta aplicar nuestra función para resolver el sistema (1.31):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "525d7a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos el vector solución del sistema.\n",
    "x = gaussian_elimination(A, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a0215c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La solución del sistema es x = [-1.  0.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "# Mostramos la solución en pantalla.\n",
    "print(f\"La solución del sistema es x = {x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474c2330",
   "metadata": {},
   "source": [
    "Y ahí lo tenemos. Hemos resuelto igualmente el sistema (1.31), aunque con un poquito más de esfuerzo. ◼︎\n",
    "\n",
    "**<font color='blue'>Definición 1.11 – Rango de una matriz:</font>** Sea $\\mathbf{A}\\in \\mathbb{R}^{m\\times n}$ una matriz con elementos reales. Se define el **rango** de $\\mathbf{A}$ como el número mínimo de filas (o columnas) de $\\mathbf{A}$ que son **linealmente independientes** (es decir, que no son múltiplos de otra fila o columna respectiva). El rango de $\\mathbf{A}$ suele denotarse como $\\rho(\\mathbf{A})$.\n",
    "\n",
    "**<font color='crimson'>Teorema 1.1 – Rouché-Frobenius (o teorema del rango):</font>** *Un sistema lineal del tipo $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$ tiene solución si y sólo si $\\rho(\\mathbf{A})=\\rho([\\mathbf{A}|\\mathbf{b}])$.* ◆\n",
    "\n",
    "En efecto, es inmediato que $\\rho(\\mathbf{A})\\leq \\rho([\\mathbf{A}|\\mathbf{b}])\\leq m$, donde $m$ es el número de filas de la matriz de coeficientes $\\mathbf{A}$, pues una fila no nula (no llena de ceros) de $\\mathbf{A}$ es una fila no nula de $[\\mathbf{A}|\\mathbf{b}]$. Esto sugiere estudiar los siguientes casos:\n",
    "\n",
    "**<font color='forestgreen'>Caso 1 – $\\rho(\\mathbf{A})<\\rho([\\mathbf{A}|\\mathbf{b}])$:</font>** Entonces, de acuerdo a la misma definición de la matriz ampliada del sistema, debería suceder al menos una situación como la siguiente (después de haber efectuado al menos una operación elemental referida al método de eliminación Gaussiana):\n",
    "\n",
    "$$\\left[ \\mathbf{A} |\\mathbf{b} \\right]  =\\left( \\begin{matrix}\\tilde{a}_{11} &\\tilde{a}_{12} &\\ldots &\\tilde{a}_{1m} &|&\\tilde{b}_{1} \\\\ \\tilde{a}_{21} &\\tilde{a}_{22} &\\ldots &\\tilde{a}_{2m} &|&\\tilde{b}_{2} \\\\ \\vdots &\\vdots &\\ddots &\\vdots &\\vdots &\\vdots \\\\ \\tilde{a}_{\\left( n-1\\right)  ,1} &\\tilde{a}_{\\left( n-1\\right)  ,2} &\\cdots &\\tilde{a}_{\\left( n-1\\right)  ,m} &|&\\tilde{b}_{n-1} \\\\ 0&0&\\cdots &0&|&\\tilde{b}_{n} \\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(1.37)$</p>\n",
    "\n",
    "Donde $\\tilde{a}_{ij}$ es el elemento en la posición $(i,j)$ original (relativa a la matriz $\\mathbf{A}$) de la matriz ampliada $\\left[ \\mathbf{A} |\\mathbf{b} \\right]$ una vez que hemos aplicado al menos una transformación elemental sobre ella, mientras que $\\tilde{b}_{i}$ es el $i$-ésimo elemento de la matriz $\\mathbf{b}$ una vez que hemos aplicado igualmente la misma cantidad (al menos una) de transformaciones elementales sobre la correspondiente matriz ampliada del sistema.\n",
    "\n",
    "Podemos observar que $\\tilde{b}_{n}\\neq 0$, lo que implicaría, conforme la última fila de (1.37), que $0\\cdot x_{m}=0=\\tilde{b}_{n}$, lo que evidentemente resulta en una contradicción. Por lo tanto, en este caso, concluimos que, si $\\rho(\\mathbf{A})<\\rho([\\mathbf{A}|\\mathbf{b}])$, entonces el sistema $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$ no tiene solución.\n",
    "\n",
    "**<font color='forestgreen'>Caso 2:</font>** Verificaremos qué ocurre cuando $\\rho(\\mathbf{A})=\\rho([\\mathbf{A}|\\mathbf{b}])$. Aquí tenemos al menos una solución para el sistema $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$, de donde se desprenden dos sub-casos:\n",
    "\n",
    "<font color='forestgreen'>Caso 2.1 – $\\rho(\\mathbf{A})=\\rho([\\mathbf{A}|\\mathbf{b}])=m$:</font> En este caso, si aplicamos el método de eliminación Gaussiana sobre la matriz ampliada $[\\mathbf{A}|\\mathbf{b}]$ de manera que resulte de aquello una matriz escalonada por filas, de tal forma que las posiciones relativas a la matriz de coeficientes $\\mathbf{A}$ se correspondan con una matriz identidad, y la columna relativa a la matriz $\\mathbf{b}$ esté conformada por elementos arbitrarios $r_{i}\\in \\mathbb{R}$, con $1\\leq i\\leq m$; es decir,\n",
    "\n",
    "$$\\left[ \\mathbf{A} |\\mathbf{b} \\right]  =\\left( \\begin{matrix}1&0&\\cdots &0&|&r_{1}\\\\ 0&1&\\cdots &0&|&r_{2}\\\\ \\vdots &\\vdots &\\ddots &\\vdots &\\vdots &\\vdots \\\\ 0&0&\\cdots &1&|&r_{m}\\end{matrix} \\right)  \\Longleftrightarrow \\left( \\begin{matrix}1&0&\\cdots &0\\\\ 0&1&\\cdots &0\\\\ \\vdots &\\vdots &\\ddots &\\vdots \\\\ 0&0&\\cdots &1\\end{matrix} \\right)  \\left( \\begin{matrix}x_{1}\\\\ x_{2}\\\\ \\vdots \\\\ x_{m}\\end{matrix} \\right)  =\\left( \\begin{matrix}r_{1}\\\\ r_{2}\\\\ \\vdots \\\\ r_{m}\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(1.38)$</p>\n",
    "\n",
    "Luego, el sistema $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$ tiene solución única, dada por el vector $\\mathbf{x}=(r_{1},...,r_{m})\\in \\mathbb{R}^{m}$.\n",
    "\n",
    "<font color='forestgreen'>Caso 2.2 – $\\rho(\\mathbf{A})=\\rho([\\mathbf{A}|\\mathbf{b}])=s<m$:</font> En este caso, la matriz escalonada por filas resultante de reducir $[\\mathbf{A}|\\mathbf{b}]$ toma la forma\n",
    "\n",
    "$$\\left[ \\mathbf{A} |\\mathbf{b} \\right]  =\\left( \\begin{matrix}1&0&\\cdots &0&c^{\\left( s+1\\right)  }_{1}&c^{\\left( s+2\\right)  }_{1}&\\cdots &c^{\\left( m\\right)  }_{1}&r_{1}\\\\ 0&1&\\cdots &0&c^{\\left( s+1\\right)  }_{2}&c^{\\left( s+2\\right)  }_{2}&\\cdots &c^{\\left( m\\right)  }_{2}&r_{2}\\\\ \\vdots &\\vdots &\\ddots &\\vdots &\\vdots &\\vdots &\\ddots &\\vdots &\\vdots \\\\ 0&0&\\cdots &1&c^{\\left( s+1\\right)  }_{s}&c^{\\left( s+2\\right)  }_{s}&\\cdots &c^{\\left( m\\right)  }_{s}&r_{s}\\\\ 0&0&\\cdots &0&0&0&\\cdots &0&0\\\\ \\vdots &\\vdots &\\ddots &\\vdots &\\vdots &\\vdots &\\ddots &\\vdots &\\vdots \\\\ 0&0&\\cdots &0&0&0&\\cdots &0&0\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(1.39)$</p>\n",
    "\n",
    "Por lo tanto, el sistema $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$ tendrá infinitas soluciones de la forma\n",
    "\n",
    "$$\\mathbf{x} =\\left( \\begin{array}{c}x_{1}\\\\ x_{2}\\\\ \\vdots \\\\ x_{s}\\\\ x_{s+1}\\\\ \\vdots \\\\ x_{m}\\end{array} \\right)  =\\underbrace{\\left( \\begin{array}{c}r_{1}-\\sum^{m-s}_{i=1} c^{\\left( s+i\\right)  }_{1}x_{s+i}\\\\ r_{2}-\\sum^{m-s}_{i=1} c^{\\left( s+i\\right)  }_{2}x_{s+i}\\\\ \\vdots \\\\ r_{s}-\\sum^{m-s}_{i=1} c^{\\left( s+i\\right)  }_{s}x_{s+i}\\\\ x_{s+1}\\\\ \\vdots \\\\ x_{m}\\end{array} \\right)  }_{\\mathrm{solucion} \\  \\mathrm{general} } =\\underbrace{\\left( \\begin{array}{c}r_{1}\\\\ r_{2}\\\\ \\vdots \\\\ r_{s}\\\\ 0\\\\ \\vdots \\\\ 0\\end{array} \\right)  }_{\\mathrm{solucion} \\  \\mathrm{particular} } +\\left( \\begin{array}{c}-\\sum^{m-s}_{i=1} c^{\\left( s+i\\right)  }_{1}x_{s+i}\\\\ -\\sum^{m-s}_{i=1} c^{\\left( s+i\\right)  }_{2}x_{s+i}\\\\ \\vdots \\\\ -\\sum^{m-s}_{i=1} c^{\\left( s+i\\right)  }_{s}x_{s+i}\\\\ x_{s+1}\\\\ \\vdots \\\\ x_{m}\\end{array} \\right)$$\n",
    "<p style=\"text-align: right;\">$(1.40)$</p>\n",
    "\n",
    "En este caso, decimos que el sistema $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$ tiene un total de $m-s$ grados de libertad, y está representado por los valores que puede tomar el vector $\\mathbf{x}=(x_{s+1},x_{s+2},...,x_{m})\\in \\mathbb{R}^{m-s}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecfa007",
   "metadata": {},
   "source": [
    "## Espacios vectoriales.\n",
    "Al iniciar esta sección, caracterizaremos informalmente a los vectores como objetos que pueden sumarse con otros vectores y que pueden ser multiplicados por un escalar, siendo el resultado de dicha multiplicación también un vector. Habiendo repasado los conceptos básicos relativos a las matrices y sus operaciones elementales, y su aplicación inmediata a la resolución de sistemas de ecuaciones lineales, ya estamos listos para formalizar esta idea, partiendo por introducir el concepto de grupo, el cual está referido a un conjunto de elementos y una operación binaria definida sobre dichos elementos que mantiene intacta la estructura del conjunto completo.\n",
    "\n",
    "### Grupos.\n",
    "Los grupos juegan un rol fundamental en la ciencia de datos. Además de proveernos de un marco de referencia elemental para las operaciones sobre conjuntos determinados, se utilizan de manera intensiva en ramas tales como criptografía, teoría de la información y visualización.\n",
    "\n",
    "**<font color='blue'>Definición 1.12 – Grupo:</font>** Consideremos un conjunto $G$ y una operación $\\otimes :G\\times G\\longrightarrow G$ definida en $G$, denominada **operación binaria**. Llamaremos a la dupla $(G, \\otimes)$ **grupo**, si se cumplen las siguientes condiciones:\n",
    "\n",
    "- **(C1) – Clausura:** $\\forall x,y\\in G:x\\otimes y\\in G$.\n",
    "- **(C2) – Asociatividad:** $\\forall x,y,z\\in G:\\left( x\\otimes y\\right)  \\otimes z=x\\otimes \\left( y\\otimes z\\right)$.\n",
    "- **(C3) – Existencia de un elemento neutro:** $\\exists e\\in G\\  |\\  \\forall x\\in G:x\\otimes e=e\\otimes x=x$. El vector $e$ se conoce como **elemento neutro** del grupo (G, \\otimes).$$\n",
    "- **(C4) – Existencia de inversa:** $\\forall x\\in G\\ \\exists y\\in G:x\\otimes y=e\\wedge y\\otimes x=e$. El vector $y$ se conoce como **inversa** de $x$, y suele denotarse como $x^{-1}$. Notemos que dicha inversa está definida estrictamente con respecto a la operación binaria $\\otimes$ y, por lo tanto, no necesariamente significa $1/x$.\n",
    "\n",
    "Si, adicionalmente, $\\forall x,y\\in G$ se tiene que $x\\otimes y=y\\otimes x$ (llamada **propiedad de conmutatividad**), entonces la dupla $(G, \\otimes)$ será llamada **grupo abeliano o conmutativo**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81af3266",
   "metadata": {},
   "source": [
    "**Ejemplo 1.9:** Veamos algunos ejemplos de conjuntos y sus operaciones asociadas, verificando si las duplas respectivas constituyen un grupo.\n",
    "\n",
    "- La dupla $(\\mathbb{Z}, +)$ es un grupo abeliano.\n",
    "- La dupla $(\\mathbb{N}^{0}, +)$ no es un grupo: Aunque $(\\mathbb{N}^{0}, +)$ posee un elemento neutro (que es el 0), no tiene definido un elemento inverso.\n",
    "- La dupla $(\\mathbb{Z}, \\cdot)$ no es un grupo: Aunque $(\\mathbb{Z}, \\cdot)$ posee un elemento neutro (que es el 1), no tiene definido un elemento inverso para $z\\in \\mathbb{Z} | z\\neq \\pm 1$.\n",
    "- La dupla $(\\mathbb{R}, \\cdot)$ no es un grupo: El elemento 0 no tiene inversa.\n",
    "- La dupla $(\\mathbb{R}-\\left\\{ 0\\right\\}  , \\cdot)$ es un grupo abeliano.ç\n",
    "- Las duplas $(\\mathbb{R}^{n}, +)$ y $(\\mathbb{Z}^{n}, +)$, con $n\\in \\mathbb{N}$, son grupos abelianos, siempre que la operación $+$ se defina componente a componente. Es decir, para todo par $x,y\\in \\mathbb{R}^{n}\\vee \\mathbb{Z}^{n}$, debemos considerar que $\\left( x_{1},...,x_{n}\\right)  +\\left( y_{1},...,y_{n}\\right)  =\\left( x_{1}+y_{1},...,x_{n}+y_{n}\\right)$. Luego, $\\left( x_{1},...,x_{n}\\right)^{-1}  =\\left( -x_{1},...,-x_{n}\\right)$ es el elemento inverso y $e=(0,...,0)$ es el elemento neutro del grupo abeliano correspondiente.\n",
    "- La dupla $(\\mathbb{R}^{m\\times n}, +)$ es un grupo abeliano, si $+$ es la suma de matrices definida en (1.10).\n",
    "- Verifiquemos qué ocurre con la dupla $(\\mathbb{R}^{n\\times n}, \\cdot)$, siendo \"$\\cdot$\" la multiplicación matricial definida en (1.11). En efecto,\n",
    "    - La clausura y la asociatividad se verifican directamente de la definición de multiplicación matricial.\n",
    "    - Elemento neutro: La matriz identidad $\\mathbf{I}_{n}$ es el elemento neutro con respecto al operador \"$\\cdot$\".\n",
    "    - Elemento inverso: Si la inversa existe (en este caso, si $\\mathbf{A}\\in \\mathbb{R}^{n\\times n}$ es una matriz no singular), entonces la matriz $\\mathbf{A}^{-1}$ es el elemento inverso con respecto a la operación binaria \"$\\cdot$\". De esta manera, la dupla $(\\mathbb{R}^{n\\times n}, \\cdot)$ es efectivamente un grupo, que –de hecho– es llamado **grupo lineal general**.\n",
    "    \n",
    "◼︎\n",
    "\n",
    "**<font color='blue'>Definición 1.13 – Grupo lineal general:</font>** El conjunto de matrices no singulares $\\mathbf{A}\\in \\mathbb{R}^{n\\times n}$ es un grupo con respecto a la multiplicación matricial definida en la ecuación (1.11), y es llamado **grupo lineal general**, denotándose como $GL(n,\\mathbb{R})$. Sin embargo, debido a que la multiplicación matricial no es una operación conmutativa, dicho grupo no es abeliano.\n",
    "\n",
    "### Definición de un espacio vectorial.\n",
    "Cuando discutimos el concepto de grupo, consideramos conjuntos arbitrarios $G$ y operaciones binarias internas de $G$; es decir, aplicaciones $G\\times G\\longrightarrow G$ que únicamente operan sobre los elementos de $G$. A partir de ahora, consideraremos conjuntos que, en adición a una operación interna, contendrán además una **operación externa** (también binaria) que estará definida para elementos pertenecientes a un conjunto $\\mathbb{K}$ que podrán combinar a dichos elementos con los de $G$.\n",
    "\n",
    "**<font color='blue'>Definición 1.14 – Espacio vectorial  (general):</font>** Un conjunto $V$ será llamado $\\mathbb{K}$-espacio vectorial si se cumplen las siguientes condiciones:\n",
    "\n",
    "- **(C1):** $V$ admite una **operación interna**, que denominaremos \"$+$\", y que definiremos como $+:V\\times V\\longrightarrow V\\  |\\left( u,v\\right)  \\longrightarrow u+v$, y es tal que la dupla $(V, +)$ es un grupo abeliano.\n",
    "- **(C2):** $V$ admite una **operación externa**, que denominaremos \"$\\cdot$\", y que definiremos como $\\cdot :\\mathbb{K} \\times V\\longrightarrow V\\  |\\left( \\lambda ,v\\right)  \\longrightarrow \\lambda v$, donde $\\mathbb{K}$ es un cuerpo (que puede ser $\\mathbb{R}$ o $\\mathbb{C}$), tal que dicha operación externa tiene las siguientes propiedades:\n",
    "    - $\\lambda \\left( u+v\\right)  =\\lambda u+\\lambda v\\  ;\\  \\forall \\lambda \\in \\mathbb{K} \\wedge u,v\\in V$.\n",
    "    - $\\left( \\lambda +\\beta \\right)  u=\\lambda u+\\beta u\\  ;\\  \\forall \\lambda ,\\beta \\in \\mathbb{K} \\wedge \\forall u\\in V$.\n",
    "    - $\\left( \\lambda \\beta \\right)  u=\\lambda \\left( \\beta u\\right)  \\  ;\\  \\forall \\lambda ,\\beta \\in \\mathbb{K} \\wedge \\forall u\\in V$.\n",
    "    - $\\lambda u=O_{V}\\Longrightarrow \\lambda =O_{\\mathbb{K} }\\vee v=O_{V}\\  ;\\  \\forall \\lambda \\in \\mathbb{K} \\wedge \\forall v\\in V$.\n",
    "\n",
    "En la última propiedad, $O_{\\mathbb{K} }$ es el elemento neutro de $\\mathbb{K}$, y $O_{V}$ es el elemento neutro de $V$. Los elementos de $V$ son llamados **vectores** y la operación interna \"$+$\" es llamada **adición vectorial**. Los elementos $\\lambda \\in \\mathbb{K}$ son llamados escalares, y la operación \"$\\cdot$\" es llamada **multiplicación por un escalar**. En términos generales, el espacio vectorial resultante se denota por el cuarteto $(V,+,\\cdot,\\mathbb{K})$. Sin embargo, sin pérdida de generalidad y a menos que se indique lo contrario, usaremos únicamente la letra $V$ para referirnos al espacio vectorial así definido.\n",
    "\n",
    "**Ejemplo 1.10:** Veremos algunos ejemplos importantes de espacios vectoriales:\n",
    "\n",
    "- El conjunto de todos los puntos $\\mathbf{x}\\in \\mathbb{R}^{n}$ es un espacio vectorial, con las siguientes operaciones:\n",
    "    - Adición: $\\mathbf{x}+\\mathbf{y}=(x_{1},...,x_{n})+(y_{1},...,y_{n})$, para todo $\\mathbf{x},\\mathbf{y}\\in \\mathbb{R}^{n}$.\n",
    "    - Multiplicación por un escalar: $\\lambda \\mathbf{x} =\\lambda \\left( x_{1},...,x_{n}\\right)  =\\left( \\lambda x_{1},...,\\lambda x_{n}\\right)$, para todo $\\mathbf{x}\\in \\mathbb{R}^{n}$ y $\\lambda \\in \\mathbb{R}$.\n",
    "\n",
    "- El conjunto de todas las matrices $m\\times n$ ($\\mathbb{R}^{m\\times n}$) es un espacio vectorial con respecto a la adición matricial definida en (1.10) y la multiplicación por un escalar, definida como $\\lambda \\mathbf{A} =\\mathbf{K}$, donde $k_{ij}=\\lambda a_{ij}$, donde $\\lambda \\in \\mathbb{R}$.\n",
    "\n",
    "Si bien $\\mathbb{R}^{n}$, $\\mathbb{R}^{n\\times 1}$ y $\\mathbb{R}^{1\\times n}$ denotan conjuntos distintos, en términos de la propia información contenida en las estructuras características de cada uno, éstos simplemente difieren en la forma en la cual escribimos sus respectivos vectores. A partir de ahora, no haremos distinciones entre $\\mathbb{R}^{n}$ y $\\mathbb{R}^{n\\times 1}$, lo que nos permitirá escribir tuplas de dimensión $n$ como vectores columna:\n",
    "\n",
    "$$\\mathbf{x} =\\left( \\begin{matrix}x_{1}\\\\ \\vdots \\\\ x_{n}\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(1.41)$</p>\n",
    "\n",
    "Esto permite simplificar la notación relativa a las operaciones sobre este tipo de vectores (y su interrelación con las mismas matrices). Sin embargo, sí haremos la distinción entre $\\mathbb{R}^{n\\times 1}$ y $\\mathbb{R}^{1\\times n}$ , a fin de evitar confusión cuando se presenten multiplicaciones matriciales entre elementos de estas dimensiones y estructuras. Por defecto, escribiremos $\\mathbf{x}$ para denotar a un vector columna, y $\\mathbf{x}^{\\top}$ para referirnos a un vector fila con los mismos elementos y tamaño que $\\mathbf{x}$.\n",
    "\n",
    "**Ejemplo 1.11:** Revisemos otro ejemplo de espacio vectorial (menos intuitivo, quizás): El conjunto $\\mathbb{R}_{n}[x]$, que describe a todos los polinomios con coeficientes en el cuerpo $\\mathbb{R}$ y grado menor o igual que 𝑛, es un espacio vectorial, ya que:\n",
    "\n",
    "- Operación interna: $\\sum\\nolimits^{n}_{k=0} a_{k}x^{k}+\\sum\\nolimits^{n}_{k=0} b_{k}x^{k}=\\sum\\nolimits^{n}_{k=0} \\left( a_{k}+b_{k}\\right)  x^{k}$.\n",
    "- Operación externa: $\\lambda \\sum\\nolimits^{n}_{k=0} a_{k}x^{k}=\\sum\\nolimits^{n}_{k=0} \\lambda a_{k}x^{k}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40caea2d",
   "metadata": {},
   "source": [
    "◼︎\n",
    "### Subespacios.\n",
    "A continuación, introduciremos el concepto de **subespacio vectorial**. De manera intuitiva, podemos decir que éstos corresponden a conjuntos que están contenidos en un espacio vectorial con la propiedad de que, cuando aplicamos operaciones propias de dicho espacio sobre elementos que pertenecen al subespacio, nunca saldremos de él. En este sentido, los subespacios vectoriales son una especie de conjuntos *cerrados*. Estos subespacios conforman una de las ideas fundamentales de muchos conceptos propios del aprendizaje automatizado (machine learning), como veremos más adelante.\n",
    "\n",
    "**<font color='blue'>Definición 1.15 – Subespacio vectorial  (general):</font>** Sea $(V,+,\\cdot,\\mathbb{K})$ un $\\mathbb{K}$-espacio vectorial y $U\\subseteq V$, tal que $U\\neq \\emptyset$. Entonces el cuarteto $(U,+,\\cdot,\\mathbb{K})$ se denomina $\\mathbb{K}$-subespacio vectorial de $V$, si $U$ es un espacio vectorial con respecto a las operaciones interna y externa de $V$, restringiendo los dominios de dichas operaciones a los conjuntos $U\\times U$ y $\\mathbb{K}\\times U$, respectivamente. Si $U$ es un subespacio de $V$, denotamos este hecho como $U\\leq V$.\n",
    "\n",
    "**<font color='crimson'>Teorema 1.2 – Caracterización de un subespacio vectorial:</font>** *Consideremos un espacio vectorial, y sea 𝑈 un subconjunto de 𝑉. Luego tenemos:*\n",
    "\n",
    "$$U\\leq V\\Longleftrightarrow \\begin{cases}\\left( i\\right)  &U\\neq \\emptyset \\\\ \\left( ii\\right)  &u\\in U\\wedge v\\in V\\Longrightarrow \\left( u+v\\right)  \\in U\\\\ \\left( iii\\right)  &u\\in U\\wedge \\lambda \\in \\mathbb{K} \\Longrightarrow \\lambda u\\in U\\end{cases}$$\n",
    "<p style=\"text-align: right;\">$(1.42)$</p>\n",
    "◆\n",
    "\n",
    "**Ejemplo 1.12:** Vamos a usar el teorema (1.2) para mostrar que $U=\\left\\{ \\left( x,y,z\\right)  \\in \\mathbb{R}^{3} :x+y-z=0\\right\\}$ es un subespacio de $\\mathbb{R}^{3}$.\n",
    "\n",
    "En efecto, primero debemos mostrar que $U\\neq \\emptyset$. Por simple inspección, podemos comprobar que $(0, 0, 0)\\in U$, ya que $0+0-0=0$. Luego mostramos que, si $u,v\\in U$, entonces la suma $u+v$ también pertenece a $U$. De esta manera tenemos que,\n",
    "\n",
    "$$\\begin{array}{llll}u\\in U&\\Longleftrightarrow &u=\\left( u_{1},u_{2},u_{3}\\right)  \\in \\mathbb{R}^{3} \\wedge u_{1}+u_{2}-u_{3}=0&\\left( \\bigstar \\right)  \\\\ v\\in U&\\Longleftrightarrow &v=\\left( v_{1},v_{2},v_{3}\\right)  \\in \\mathbb{R}^{3} \\wedge v_{1}+v_{2}-v_{3}=0&\\left( \\spadesuit \\right)  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(1.43)$</p>\n",
    "\n",
    "Entonces,\n",
    "\n",
    "$$\\begin{array}{llll}u+v&=&\\left( u_{1},u_{2},u_{3}\\right)  +\\left( v_{1},v_{2},v_{3}\\right)  &\\left[ \\mathrm{ver} \\  \\left( \\bigstar \\right)  \\  \\mathrm{y} \\  \\left( \\spadesuit \\right)  \\right]  \\\\ &=&\\left( u_{1}+v_{1},u_{2}+v_{2},u_{3}+v_{3}\\right)  &\\left[ \\mathrm{suma} \\  \\mathrm{en} \\  \\mathbb{R}^{3} \\right]  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(1.44)$</p>\n",
    "\n",
    "Luego $(u+v)\\in \\mathbb{R}^{3}$. Por lo tanto, sólo resta verificar que $(u+v)$ satisfaga la condición que caracteriza a $U$. De este modo,\n",
    "\n",
    "$$\\begin{array}{lll}\\left( u_{1}+v_{1}\\right)  +\\left( u_{2}+v_{2}\\right)  -\\left( u_{3}+v_{3}\\right)  &=&u_{1}+v_{1}+u_{2}+v_{2}-u_{3}-v_{3}\\\\ &=&\\left( u_{1}+u_{2}-u_{3}\\right)  +\\left( v_{1}+v_{2}-v_{3}\\right)  \\\\ &=&0+0\\\\ &=&0\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(1.45)$</p>\n",
    "\n",
    "Así que, en efecto, $(u+v)\\in U$. Por lo tanto, $U$ es un subespacio de $V$. ◼︎\n",
    "\n",
    "**Ejemplo 1.13:** Veamos algunos casos más intuitivos:\n",
    "\n",
    "- Para cada espacio vectorial $V$, los llamados **subespacios triviales** corresponden a $V$ propiamente tal y a $O_{V}$ (el elemento nulo de $V$).\n",
    "- En la Fig. (1.5), sólo el caso (D) corresponde a un subespacio de $\\mathbb{R}^{2}$ (con las operaciones interna y externa usuales). En (A) y (C) ) se viola la propiedad de clausura y en (B) el conjunto respectivo no contiene al elemento neutro de $\\mathbb{R}^{2}$ (el vector $(0,0)$).\n",
    "- El conjunto solución de un sistema lineal homogéneo de ecuaciones ($\\mathbf{A}\\mathbf{x}=\\mathbf{0}$) con $n$ incógnitas $\\mathbf{x}=(x_{1},...,x_{n})^{\\top}$ es un subespacio de $\\mathbb{R}^{n}$. El recíproco también es cierto: Cada subespacio $U\\subseteq (\\mathbb{R}^{n},+,\\cdot)$ es la solución de un sistema lineal homogéneo con $n$ incógnitas.\n",
    "- El conjunto solución de un sistema lineal no homogéneo de ecuaciones ($\\mathbf{A}\\mathbf{x}=\\mathbf{b}$) con $n$ incógnitas $\\mathbf{x}=(x_{1},...,x_{n})^{\\top}$ no es un subespacio de $\\mathbb{R}^{n}$.\n",
    "- La intersección de una colección arbitraria de subespacios vectoriales es también un subsespacio vectorial.\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"figures/fig_1_5.png\" width=\"900\"></p>\n",
    "<p style=\"text-align: center;\">Fig. (1.5): Algunos ejemplos de conjuntos que no cumplen (salvo (D)) con ser subespacios de $\\mathbb{R}^{2}$</p>\n",
    "◼︎"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c757877b",
   "metadata": {},
   "source": [
    "## Independencia lineal.\n",
    "\n",
    "### Generadores.\n",
    "Recordemos que partimos esta primera sección estableciendo que queríamos construir un cierto conjunto de reglas para manipular objetos (vectores) operando con ellos. Ya hemos construido una definición del marco de referencia requerido para definir estos objetos y las operaciones que podemos hacer entre ellos. Sin embargo, aún debemos definir ciertas reglas para construir todos los objetos posibles de un determinado espacio vectorial, a fin de definir la dimensión del mismo (o, en líneas más informales, sus posibles grados de libertad).\n",
    "\n",
    "Queremos, por lo tanto, determinar el número mínimo de objetos necesarios para generar todos los objetos de un espacio vectorial determinado. Como orientación debemos recordar que un espacio vectorial, o tiene un elemento, o tiene infinitos elementos. Por tanto, el punto es generar un proceso que transforme en una ”idea finita”, lo que en realidad es, en concreto, no finito. Con esto en mente:\n",
    "\n",
    "**(a)** Consideremos el espacio $\\mathbb{R}^{3}$, que sabemos que es un $\\mathbb{R}$-espacio vectorial. Intentemos mirar de manera diferente a dicho espacio, interpretando de manera un tanto diferente las operaciones interna y externa que definen al mismo. De esta manera, podemos considerar el conjunto $U_{1}$, definido como\n",
    "\n",
    "$$U_{1}=\\left\\{ \\left( x,y,z\\right)  \\in \\mathbb{R}^{3} :x+4y-2z=0\\right\\}$$\n",
    "<p style=\"text-align: right;\">$(1.46)$</p>\n",
    "\n",
    "Si usamos la definición del conjunto $U_{1}$, podemos escribir\n",
    "\n",
    "$$\\begin{array}{lll}u\\in U_{1}&\\Longleftrightarrow &u=\\left( x,y,z\\right)  \\in \\mathbb{R}^{3} \\wedge x+4y-2x=0\\\\ &\\Longleftrightarrow &u=\\left( x,y,z\\right)  \\in \\mathbb{R}^{3} \\wedge x=2z-4y\\\\ &\\Longleftrightarrow &u=\\left( 2z-4y,y,z\\right)  \\wedge y\\in \\mathbb{R} ,z\\in \\mathbb{R} \\\\ &\\Longleftrightarrow &u=\\left( 2z,0,z\\right)  +\\left( -4y,y,0\\right)  \\wedge y\\in \\mathbb{R} ,z\\in \\mathbb{R} \\\\ &\\Longleftrightarrow &u=z\\left( 2,0,1\\right)  +y\\left( -4,1,0\\right)  \\wedge y\\in \\mathbb{R} ,z\\in \\mathbb{R} \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(1.47)$</p>\n",
    "\n",
    "Así que podemos redefinir al conjunto $U_{1}$ como:\n",
    "\n",
    "$$U_{1}=\\left\\{ \\lambda_{1} \\left( 2,0,1\\right)  +\\lambda_{2} \\left( -4,1,0\\right)  ;\\lambda_{1} ,\\lambda_{2} \\in \\mathbb{R} \\right\\}$$\n",
    "<p style=\"text-align: right;\">$(1.48)$</p>\n",
    "\n",
    "En conclusión,\n",
    "\n",
    "$$u\\in U_{1}\\Longleftrightarrow \\mathrm{La} \\  \\mathrm{ecuacion} \\  u=\\lambda_{1} \\left( 2,0,1\\right)  +\\lambda_{2} \\left( -4,1,0\\right)  \\  \\mathrm{tiene} \\  \\mathrm{solucion} \\  \\mathrm{en} \\  \\mathbb{R}$$\n",
    "<p style=\"text-align: right;\">$(1.49)$</p>\n",
    "\n",
    "**(b)** Vamos a aplicar el mismo enfoque anterior para el conjunto $U_{2}$, definido como\n",
    "\n",
    "$$U_{2}=\\left\\{ \\left( x,y,z\\right)  \\in \\mathbb{R}^{3} :x=y\\wedge z=0\\right\\}$$\n",
    "<p style=\"text-align: right;\">$(1.50)$</p>\n",
    "\n",
    "Nuevamente ingresamos al conjunto:\n",
    "\n",
    "$$\\begin{array}{lll}u\\in U_{2}&\\Longleftrightarrow &u=\\left( x,y,z\\right)  \\in \\mathbb{R}^{3} \\  ;\\  x=y\\wedge z=0\\\\ &\\Longleftrightarrow &u=\\left( x,x,0\\right)  \\wedge x\\in \\mathbb{R} \\\\ &\\Longleftrightarrow &u=x\\left( 1,1,0\\right)  \\wedge x\\in \\mathbb{R} \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(1.51)$</p>\n",
    "\n",
    "De este modo,\n",
    "\n",
    "$$U_{2}=\\left\\{ \\lambda \\left( 1,1,0\\right)  ;\\lambda \\in \\mathbb{R} \\right\\}$$\n",
    "<p style=\"text-align: right;\">$(1.52)$</p>\n",
    "\n",
    "La conclusión es, por lo tanto, lo siguiente:\n",
    "\n",
    "$$u\\in U_{2}\\Longleftrightarrow \\mathrm{La} \\  \\mathrm{ecuacion} \\  u=\\lambda \\left( 1,1,0\\right)  \\  \\mathrm{tiene} \\  \\mathrm{solucion} \\  \\mathrm{en} \\  \\mathbb{R}^{3}$$\n",
    "<p style=\"text-align: right;\">$(1.53)$</p>\n",
    "\n",
    "**(c)** Ahora tratemos de aplicar lo que hemos aprendido en el conjunto $U_{1}+U_{2}=\\left\\{ u_{1}+u_{2}\\  ;\\  u_{1}\\in U_{1}\\wedge u_{2}\\in U_{2}\\right\\}$. En este caso, primero debemos demostrar que $U_{1}+U_{2}\\neq \\emptyset$. En efecto, $O_{\\mathbb{R}^{3} }=\\left( 0,0,0\\right)  =0\\left( 2,0,1\\right)  +0\\left( -4,1,0\\right)  +0\\left( 1,1,0\\right)  \\Longrightarrow O_{\\mathbb{R}^{3} }\\in U_{1}+U_{2}$. Más aún, $O_{\\mathbb{R}^{3} }\\in U_{1}$, puesto que $(0,0,0)=0(2,0,1)+0(-4,1,0)$, y $O_{\\mathbb{R}^{3} }\\in U_{2}$, ya que igualmente $(0,0,0)=0(1,1,0)$. Así que $O_{\\mathbb{R}^{3} }\\in U_{1}\\cap U_{2}$, lo que implica que $U_{1}+U_{2}\\neq \\emptyset$. Por otro lado, se tiene\n",
    "\n",
    "$$\\begin{array}{lll}u\\in U_{1}\\cap U_{2}&\\Longleftrightarrow &u\\in U_{1}\\wedge u\\in U_{2}\\\\ &\\Longleftrightarrow &u=\\left( x,y,z\\right)  \\in \\mathbb{R}^{3} \\wedge x+4y-2z=0\\wedge x=y\\wedge z=0\\\\ &\\Longleftrightarrow &u=\\left( x,x,0\\right)  \\in \\mathbb{R}^{3} \\wedge x+4x=0\\\\ &\\Longleftrightarrow &u=\\left( x,x,0\\right)  \\in \\mathbb{R}^{3} \\wedge x=0\\\\ &\\Longleftrightarrow &u=\\left( 0,0,0\\right)  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(1.54)$</p>\n",
    "\n",
    "Por lo tanto, $U_{1}\\cap U_{2}=\\left\\{ \\left( 0,0,0\\right)  \\right\\}  =\\left\\{ O_{\\mathbb{R}^{3} }\\right\\}$. Ahora, para $u\\in \\mathbb{R}^{3}$ arbitrario, trataremos de resolver la ecuación\n",
    "\n",
    "$$u=x_{1}\\left( 2,0,1\\right)  +x_{2}\\left( -4,1,0\\right)  +x_{3}\\left( 1,1,0\\right)$$\n",
    "<p style=\"text-align: right;\">$(1.55)$</p>\n",
    "\n",
    "Si lo logramos, entonces se verificará que (1): $\\mathbb{R}^{3}=U_{1}+U_{2}$, y (2): Habremos encontrado una especie de *fórmula* para expresar todos los elementos de $\\mathbb{R}^{3}$ en términos de los vectores de $U_{1}$ y $U_{2}$. En efecto, de la ecuación (1.55), tenemos que\n",
    "\n",
    "$$\\begin{array}{lll}\\left( x,y,z\\right)  =x_{1}\\left( 2,0,1\\right)  +x_{2}\\left( -4,1,0\\right)  +x_{3}\\left( 1,1,0\\right)  &\\Longleftrightarrow &\\left( x,y,z\\right)  =\\left( 2x_{1}-4x_{2}+x_{3},x_{2}+x_{3},x_{1}\\right)  \\\\ &\\Longleftrightarrow &\\begin{cases}\\begin{array}{rcl}2x_{1}-4x_{2}+x_{3}&=&x\\\\ x_{2}+x_{3}&=&y\\\\ x_{1}&=&z\\end{array} &\\end{cases} \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(1.56)$</p>\n",
    "\n",
    "El sistema lineal resultante de (1.56) puede resolverse por medio del método de eliminación Gaussiana presentado con anterioridad, y su solución es\n",
    "\n",
    "$$\\left( x_{1},x_{2},x_{3}\\right)^{\\top }  =\\left( z,\\frac{-x+y+2z}{5} ,\\frac{x+4y-2z}{5} \\right)^{\\top }$$\n",
    "<p style=\"text-align: right;\">$(1.57)$</p>\n",
    "\n",
    "Lo que implica que:\n",
    "\n",
    "$$\\left( x,y,z\\right)  =z\\left( 2,0,1\\right)  +\\frac{-x+y+2z}{5} \\left( -4,1,0\\right)  +\\frac{x+4y-2z}{5} \\left( 1,1,0\\right)$$\n",
    "<p style=\"text-align: right;\">$(1.58)$</p>\n",
    "\n",
    "Finalmente, adoptando las siguientes notaciones:\n",
    "\n",
    "$$\\alpha =\\left\\{ \\left( 2,0,1\\right)  ,\\left( -4,1,0\\right)  ,\\left( 1,1,0\\right)  \\right\\}  \\wedge \\left[ \\left( x,y,z\\right)  \\right]_{\\alpha }  =\\left( \\begin{array}{c}z\\\\ \\displaystyle \\frac{-x+y+2z}{5} \\\\ \\displaystyle \\frac{x+4y-2z}{5} \\end{array} \\right)_{\\alpha }  \\Longleftrightarrow \\underbrace{z\\left( 2,0,1\\right)  +\\frac{-x+y+2z}{5} \\left( -4,1,0\\right)  }_{\\in U_{1}} +\\underbrace{\\frac{x+4y-2z}{5} \\left( 1,1,0\\right)  }_{\\in U_{2}}$$\n",
    "<p style=\"text-align: right;\">$(1.59)$</p>\n",
    "\n",
    "Donde la notación $\\left[ \\left( x,y,z\\right)  \\right]_{\\alpha }$ se lee *\"el vector $(x,y,z)$ que es generado por $\\alpha$\"*. De esta manera, hemos mostrado que $\\mathbb{R}^{3}$ puede escribirse como una especie de suma entre los conjuntos $U_{1}$ y $U_{2}$ (es decir, $\\mathbb{R}^{3}=U_{1}+U_{2}$). Además, reemplazando los *vectores base* previamente definidos en las **expresiones generadoras** que los acompañan, obtenemos\n",
    "\n",
    "$$\\begin{array}{rcl}\\left[ \\left( 2,0,1\\right)  \\right]_{\\alpha }  &=&\\begin{pmatrix}1\\\\ 0\\\\ 0\\end{pmatrix} \\\\ \\left[ \\left( -4,1,0\\right)  \\right]_{\\alpha }  &=&\\begin{pmatrix}0\\\\ 1\\\\ 0\\end{pmatrix} \\\\ \\left[ \\left( 1,1,0\\right)  \\right]_{\\alpha }  &=&\\begin{pmatrix}0\\\\ 0\\\\ 1\\end{pmatrix} \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(1.60)$</p>\n",
    "\n",
    "Por lo tanto, hemos demostrado que, para el caso de $\\alpha =\\left\\{ \\left( 2,0,1\\right)  ,\\left( -4,1,0\\right)  ,\\left( 1,1,0\\right)  \\right\\}$, $\\mathbb{R}^{3}$ se puede descomponer en la suma del plano $U_{1}$ y la recta $U_{2}$, de manera similar a lo que ocurre en el conjunto $\\mathbb{R}^{2}$, que sabemos *intuitivamente* que puede descomponerse entre los ejes $X$ e $Y$, que representan a las rectas $x=0$ e $y=0$. Esto motiva la siguiente definición.\n",
    "\n",
    "**<font color='blue'>Definición 1.16 – Suma directa de subespacios:</font>** Sea $V$ un $\\mathbb{K}$-espacio vectorial, y $U_{1}$ y $U_{2}$ dos subespacios de $V$. Diremos que $V$ es la **suma directa** de los subespacios $U_{1}$ y $U_{2}$, lo que denotamos como $V=U_{1}\\oplus U_{2}$, si se cumple que:\n",
    "\n",
    "- **(P1):** $V=U_{1}+U_{2}$.\n",
    "- **(P2):** $U_{1}\\cap U_{2}=\\left\\{ O_{V}\\right\\}$.\n",
    "\n",
    "De esta manera, el caso que estudiamos previamente es un ejemplo de una suma directa cuyo resultado es el espacio $\\mathbb{R}^{3}$.\n",
    "\n",
    "**<font color='crimson'>Teorema 1.3:</font>** *Sea $V$ un $\\mathbb{K}$-espacio vectorial y $\\left\\{ v_{1},...,v_{n}\\right\\}  \\subset V$. Entonces tenemos que*\n",
    "\n",
    "$$U=\\left< \\left\\{ v_{1},...,v_{n}\\right\\}  \\right>  =\\left\\{ \\sum^{n}_{i=1} r_{i}v_{i}\\  |\\  r_{i}\\in \\mathbb{K} \\  ;\\  1\\leq i\\leq n\\right\\}  \\leq V$$\n",
    "<p style=\"text-align: right;\">$(1.61)$</p>\n",
    "◆\n",
    "\n",
    "Este teorema motiva la siguiente definición.\n",
    "\n",
    "**<font color='blue'>Definición 1.17 – Generador:</font>** Sea $V$ un espacio vectorial (que asumiremos sobre el cuerpo $\\mathbb{R}$). Entonces,\n",
    "\n",
    "- **(P1):** El conjunto $U=\\left< \\left\\{ v_{1},...,v_{n}\\right\\}  \\right>$ se llamará **subespacio generado** por $\\alpha =\\left\\{ v_{1},...,v_{n}\\right\\}$, donde cada elemento $v_{1}\\in \\alpha$ para $1\\leq i\\leq n$ será llamado **generador**.\n",
    "- **(P2):** Llamaremos a $u\\in V$ **combinación lineal** de $\\alpha =\\left\\{ v_{1},...,v_{n}\\right\\}$ si $u\\in U$. Es decir, si existen $n$ escalares $\\left\\{ a_{1},...,a_{n}\\right\\}$ tal que\n",
    "\n",
    "$$u=\\sum^{n}_{i=1} a_{i}v_{i}$$\n",
    "<p style=\"text-align: right;\">$(1.62)$</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18484adb",
   "metadata": {},
   "source": [
    "### Sistemas de generadores.\n",
    "Si $V$ es un espacio vectorial (que asumiremos, sin pérdida de generalidad, sobre el cuerpo $\\mathbb{R}$) y $\\alpha =\\left\\{ v_{1},...,v_{n}\\right\\}\\subset V$, $\\alpha$ será llamado **sistema de generadores** para $V$ si $V=<\\alpha>=\\left< \\left\\{ v_{1},...,v_{n}\\right\\}  \\right>$. Equivalentemente, $\\alpha$ es un sistema de generadores si, para cada $v\\in V$, existe una colección de $n$ escalares, digamos $a_{1},...,a_{n}$, tales que $v$ es una combnación lineal de $\\alpha$. Es decir, $v=a_{1}v_{1}+\\cdots +a_{n}v_{n}$.\n",
    "\n",
    "**Ejemplo 1.14:** El conjunto $\\alpha_{n} =\\left\\{ 1,x,x^{2},...,x^{n}\\right\\}$ agrupa a todos los generadores *canónicos* del espacio vectorial $\\mathbb{R}_{n}[x]$, constituido por todos los polinomios de grado menor igual que $n$. En efecto,\n",
    "\n",
    "$$p\\left( x\\right)  =a_{0}\\cdot 1+a_{1}\\cdot x+a_{2}\\cdot x^{2}+\\cdots +a_{n}\\cdot x^{n}=\\sum^{n}_{k=0} a_{k}x^{k}$$\n",
    "<p style=\"text-align: right;\">$(1.63)$</p>\n",
    "\n",
    "Así que $\\mathbb{R}_{n} \\left[ x\\right]  =\\left< \\left\\{ 1,x,x^{2},...,x^{n}\\right\\}  \\right>$. ︎︎︎◼︎\n",
    "\n",
    "Antes de proseguir, vamos a estudiar algunas situaciones que nos permiten construir sistemas de generadores.\n",
    "\n",
    "**(1)** Sea $V$ un espacio vectorial y $\\alpha=\\left\\{ v_{1},v_{2}\\right\\}$. Entonces,\n",
    "\n",
    "$$V=\\left< \\left\\{ v_{1},v_{2}\\right\\}  \\right>  \\Longrightarrow V=\\left< \\left\\{ v_{1},v_{1}+v_{2}\\right\\}  \\right>$$\n",
    "<p style=\"text-align: right;\">$(1.64)$</p>\n",
    "\n",
    "En efecto,\n",
    "\n",
    "**(1a)** En este caso, tenemos que demostrar que $V=\\left< \\left\\{ v_{1},v_{1}+v_{2}\\right\\}  \\right>$. Es decir, debemos mostrar que la ecuación vectorial\n",
    "\n",
    "$$v=a_{1}v_{1}+a_{2}\\left( v_{1}+v_{2}\\right)$$\n",
    "<p style=\"text-align: right;\">$(1.65)$</p>\n",
    "\n",
    "tiene solución para cada $v\\in V$.\n",
    "\n",
    "**(1b)** Analizamos entonces los datos. Como $V=\\left< \\left\\{ v_{1},v_{2}\\right\\}  \\right>$ y $v\\in V$, entonces tiene solución la ecuación vectorial\n",
    "\n",
    "$$v=b_{1}v_{1}+b_{2}v_{2}$$\n",
    "<p style=\"text-align: right;\">$(1.66)$</p>\n",
    "\n",
    "Es decir, existen $b_{1}$ y $b_{2}$ en $\\mathbb{R}$ tal que la ecuación (1.66) es una identidad.\n",
    "\n",
    "**(1c)** Supongamos entonces, por un instante, que la ecuación (1.65) tiene solución. Entonces debemos tener\n",
    "\n",
    "$$\\begin{array}{lll}v&=&a_{1}v_{1}+a_{2}\\left( v_{1}+v_{2}\\right)  \\\\ &=&a_{1}v_{1}+a_{2}v_{1}+a_{2}v_{2}\\\\ &=&\\left( a_{1}+a_{2}\\right)  v_{1}+a_{2}v_{2}\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(1.67)$</p>\n",
    "\n",
    "Luego, basta que tomemos:\n",
    "\n",
    "$$\\begin{array}{rll}a_{1}+a_{2}&=&b_{1}\\\\ a_{2}&=&b_{2}\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(1.68)$</p>\n",
    "\n",
    "De donde obtenemos que $a_{1}=b_{1}-b_{2}$ y $a_{2}=b_{2}$. Entonces, hemos mostrado que\n",
    "\n",
    "$$v=b_{1}v_{1}+b_{2}v_{2}\\Longrightarrow v=\\left( b_{1}-b_{2}\\right)  v_{1}+b_{2}\\left( v_{1}+v_{2}\\right)$$\n",
    "<p style=\"text-align: right;\">$(1.69)$</p>\n",
    "\n",
    "Y concluimos que $V=\\left< \\left\\{ v_{1},v_{1}+v_{2}\\right\\}  \\right>$.\n",
    "\n",
    "**Ejemplo 1.15:** Vamos a demostrar que los conjuntos de polinomios\n",
    "\n",
    "$$\\begin{array}{lll}U&=&\\left\\{ p\\left( x\\right)  =a_{0}+a_{1}x+a_{2}x^{2}+a_{3}x^{3}\\in \\mathbb{R}_{3} \\left[ x\\right]  \\  :\\  a_{0}-a_{1}=0\\wedge a_{2}-a_{3}=0\\right\\}  \\\\ W&=&\\left\\{ q\\left( x\\right)  =b_{0}+b_{1}x+b_{2}x^{2}+b_{3}x^{3}\\in \\mathbb{R}_{3} \\left[ x\\right]  \\  :\\  b_{0}+b_{1}=0\\wedge b_{2}+b_{3}=0\\right\\}  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(1.70)$</p>\n",
    "\n",
    "conforman un sistema de generadores para el espacio vectorial $\\mathbb{R}_{3} \\left[ x\\right]$. En primer lugar, vamos a demostrar que tanto $U$ como $W$ son subespacios de $\\mathbb{R}_{3} \\left[ x\\right]$. De esta manera, usando lo que hemos aprendido en relación a los generadores, tenemos que\n",
    "\n",
    "$$\\begin{array}{lll}p\\left( x\\right)  \\in W&\\Longleftrightarrow &p\\left( x\\right)  =a_{0}+a_{1}x+a_{2}x^{2}+a_{3}x^{3}\\in \\mathbb{R}_{3} \\left[ x\\right]  \\wedge a_{0}-a_{1}=0\\  ;\\  a_{2}-a_{3}=0\\\\ &\\Longleftrightarrow &p\\left( x\\right)  =a_{0}+a_{1}x+a_{2}x^{2}+a_{3}x^{3}\\in \\mathbb{R}_{3} \\left[ x\\right]  \\wedge a_{0}=a_{1}\\  ;\\  a_{2}=a_{3}\\\\ &\\Longleftrightarrow &p\\left( x\\right)  =a_{0}+a_{0}x+a_{2}x^{2}+a_{2}x^{3}\\wedge a_{0}\\in \\mathbb{R} \\  ;\\  a_{2}\\in \\mathbb{R} \\\\ &\\Longleftrightarrow &p\\left( x\\right)  =a_{0}\\left( 1+x\\right)  +a_{2}\\left( x^{2}+x^{3}\\right)  \\wedge a_{0}\\in \\mathbb{R} \\  ;\\  a_{2}\\in \\mathbb{R} \\\\ &\\Longleftrightarrow &p\\left( x\\right)  \\in \\left< \\left\\{ 1+x,x^{2}+x^{3}\\right\\}  \\right>  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(1.71)$</p>\n",
    "\n",
    "Así que $U=\\left< \\left\\{ 1+x,x^{2}+x^{3}\\right\\}  \\right>  \\leq \\mathbb{R}_{3} \\left[ x\\right]$. De la misma forma,\n",
    "\n",
    "$$\\begin{array}{lll}q\\left( x\\right)  \\in W&\\Longleftrightarrow &q\\left( x\\right)  =b_{0}+b_{1}x+b_{2}x^{2}+b_{3}x^{3}\\in \\mathbb{R}_{3} \\left[ x\\right]  \\wedge b_{0}+b_{1}=0\\  ;\\  b_{2}+b_{3}=0\\\\ &&q\\left( x\\right)  =b_{0}+b_{1}x+b_{2}x^{2}+b_{3}x^{3}\\in \\mathbb{R}_{3} \\left[ x\\right]  \\wedge -b_{0}=b_{1}\\  ;\\  -b_{2}=b_{3}\\\\ &&q\\left( x\\right)  =b_{0}-b_{0}x+b_{2}x^{2}-b_{2}x^{3}\\wedge b_{0}\\in \\mathbb{R} \\  ;\\  b_{2}\\in \\mathbb{R} \\\\ &&q\\left( x\\right)  =b_{0}\\left( 1-x\\right)  +b_{2}\\left( x^{2}-x^{3}\\right)  \\wedge b_{0}\\in \\mathbb{R} \\  ;\\  b_{2}\\in \\mathbb{R} \\\\ &&q\\left( x\\right)  \\in \\left< \\left\\{ 1-x,x^{2}-x^{3}\\right\\}  \\right>  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(1.72)$</p>\n",
    "\n",
    "Por lo tanto, $W=\\left< \\left\\{ 1-x,x^{2}-x^{3}\\right\\}  \\right>  \\leq \\mathbb{R}_{3} \\left[ x\\right]$.\n",
    "\n",
    "Ahora debemos demostrar $\\mathbb{R}_{3}$ es la suma directa de $U$ y $W$. Es decir, que $\\mathbb{R}_{3}=U\\oplus W$. Conforme la definición (1.16), debemos resolver la ecuación,\n",
    "\n",
    "$$\\begin{array}{lll}p\\left( x\\right)  &=&u+w\\  ;\\  u\\in U\\wedge w\\in W\\\\ &=&\\underbrace{c_{1}\\left( 1-x\\right)  +c_{2}\\left( x^{2}-x^{3}\\right)  }_{w} +\\underbrace{c_{3}\\left( 1+x\\right)  +c_{4}\\left( x^{2}+x^{3}\\right)  }_{u} \\  ;\\  p\\left( x\\right)  \\in \\mathbb{R}_{3} \\left[ x\\right]  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(1.73)$</p>\n",
    "\n",
    "Manos a la obra entonces:\n",
    "\n",
    "$$\\begin{array}{lll}a_{0}+a_{1}x+a_{2}x^{2}+a_{3}x^{3}&=&c_{1}\\left( 1-x\\right)  +c_{2}\\left( x^{2}-x^{3}\\right)  +c_{3}\\left( 1+x\\right)  +c_{4}\\left( x^{2}+x^{3}\\right)  \\\\ &=&c_{1}-c_{1}x+c_{2}x^{2}-c_{2}x^{3}+c_{3}+c_{3}x+c_{4}x^{2}+c_{4}x^{3}\\\\ &=&c_{1}+c_{3}+c_{3}x-c_{1}x+c_{4}x^{2}+c_{2}x^{2}-c_{2}x^{3}+c_{4}x^{3}\\\\ &=&c_{1}+c_{3}+\\left( c_{3}-c_{1}\\right)  x+\\left( c_{2}+c_{4}\\right)  x^{2}+\\left( c_{4}-c_{2}\\right)  x^{3}\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(1.74)$</p>\n",
    "\n",
    "Lo anterior nos lleva al siguiente sistema lineal de ecuaciones, con la siguiente solución\n",
    "\n",
    "$$\\begin{array}{rll}c_{1}+c_{3}&=&a_{0}\\\\ -c_{1}+c_{3}&=&a_{1}\\\\ c_{2}+c_{4}&=&a_{2}\\\\ -c_{2}+c_{4}&=&a_{3}\\end{array} \\  \\Longrightarrow \\  \\begin{array}{lll}c_{1}&=&\\frac{a_{0}-a_{1}}{2} \\\\ c_{2}&=&\\frac{a_{0}+a_{1}}{2} \\\\ c_{3}&=&\\frac{a_{2}-a_{3}}{2} \\\\ c_{4}&=&\\frac{a_{2}+a_{3}}{2} \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(1.75)$</p>\n",
    "\n",
    "Por lo tanto,\n",
    "\n",
    "$$p\\left( x\\right)  =\\underbrace{\\frac{a_{0}-a_{1}}{2} \\left( 1-x\\right)  +\\frac{a_{2}-a_{3}}{2} \\left( x^{2}-x^{3}\\right)  }_{\\in W} +\\underbrace{\\frac{a_{0}+a_{1}}{2} \\left( 1+x\\right)  +\\frac{a_{2}+a_{3}}{2} \\left( x^{2}+x^{3}\\right)  }_{\\in U}$$\n",
    "<p style=\"text-align: right;\">$(1.76)$</p>\n",
    "\n",
    "Hemos mostrado pues que $\\mathbb{R}_{3}[x]=U+W$. Ahora debemos demostrar que $U\\cap W=\\left\\{ O_{\\mathbb{R}_{3} [x]}\\right\\}$. Para esto, procedemos como sigue:\n",
    "\n",
    "$$\\begin{array}{lll}p\\left( x\\right)  \\in W&\\Longleftrightarrow &p\\left( x\\right)  \\in W\\wedge p\\left( x\\right)  \\in U\\\\ &\\Longleftrightarrow &p\\left( x\\right)  =b_{1}\\left( 1-x\\right)  +b_{2}\\left( x^{2}-x^{3}\\right)  \\wedge p\\left( x\\right)  =b_{3}\\left( 1+x\\right)  +b_{4}\\left( x^{2}+x^{3}\\right)  \\\\ &\\Longleftrightarrow &b_{1}-b_{1}x+b_{2}x^{2}-b_{2}x^{3}=b_{3}+b_{3}x+b_{4}x^{2}+b_{4}x^{3}\\\\ &\\Longleftrightarrow &\\left( b_{1}=b_{3}\\right)  \\wedge \\left( -b_{1}=b_{3}\\right)  \\wedge \\left( b_{2}=b_{4}\\right)  \\wedge \\left( -b_{2}=b_{4}\\right)  \\\\ &\\Longleftrightarrow &b_{1}=b_{3}=0\\wedge b_{2}=b_{4}=0\\\\ &\\Longleftrightarrow &p\\left( x\\right)  =O_{\\mathbb{R}_{3} \\left[ x\\right]  }\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(1.77)$</p>\n",
    "\n",
    "De donde concluimos que, efectivamente, $U\\cap W=\\left\\{ O_{\\mathbb{R}_{3} \\left[ x\\right]  }\\right\\}$. Hemos pues obtenido un sistema de generadores para $\\mathbb{R}_{3} \\left[ x\\right]$, ya que\n",
    "\n",
    "$$\\left[ a_{0}+a_{1}x+a_{2}x^{2}+a_{3}x^{3}\\right]_{\\alpha }  =\\left( \\begin{array}{c}\\frac{a_{0}-a_{1}}{2} \\\\ \\frac{a_{2}-a_{3}}{2} \\\\ \\frac{a_{0}+a_{1}}{2} \\\\ \\frac{a_{2}+a_{3}}{2} \\end{array} \\right)$$\n",
    "<p style=\"text-align: right;\">$(1.78)$</p>\n",
    "◼︎"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae2b6b1",
   "metadata": {},
   "source": [
    "### Definición de independencia lineal.\n",
    "Ya contamos con una excelente representación de un espacio vectorial en virtud del concepto de sistema de generadores. Sin embargo, queremos garantizar que dicha representación sea segura; es decir, que sea una condición necesaria y suficiente para la definición de un espacio vectorial como tal, a partir de un determinado sistema. Para ello, iniciamos nuestro análisis considerando un sistema de generadores $\\alpha =\\left\\{ v_{1},...,v_{n}\\right\\}$ de un espacio vectorial $V$. En símbolos,\n",
    "\n",
    "$$V=\\left< \\left\\{ v_{1},...,v_{n}\\right\\}  \\right>$$\n",
    "<p style=\"text-align: right;\">$(1.79)$</p>\n",
    "\n",
    "Equivalentemente, para cada $v\\in V$, existe una colección de escalares $c_{1},...,c_{n}$ tal que $v$ es una combinación lineal de tales escalares y el sistema de generadores $\\alpha$. Es decir,\n",
    "\n",
    "$$v=\\sum^{n}_{i=1} c_{i}v_{i}$$\n",
    "<p style=\"text-align: right;\">$(1.80)$</p>\n",
    "\n",
    "Lo anterior motiva la siguiente definición.\n",
    "\n",
    "**<font color='blue'>Definición 1.18:</font>** Sea $\\alpha =\\left\\{ v_{1},...,v_{n}\\right\\}  \\subset V$. Entonces definimos la relación $\\left[ \\right]_{\\alpha }  :V\\longrightarrow \\mathbb{R}^{n\\times 1}$ como sigue\n",
    "\n",
    "$$\\left[ v\\right]_{\\alpha }  =\\left( \\begin{matrix}c_{1}\\\\ c_{2}\\\\ \\vdots \\\\ c_{n}\\end{matrix} \\right)  \\in \\mathbb{R}^{n\\times 1} \\Longleftrightarrow v=\\sum^{n}_{k=1} c_{k}v_{k}\\in V$$\n",
    "<p style=\"text-align: right;\">$(1.81)$</p>\n",
    "\n",
    "**Ejemplo 1.16:** Si $\\alpha =\\left\\{ v_{1},...,v_{n}\\right\\}\\subset V$ es un sistema de generadores para $V$, entonces para $w\\in V$, $\\beta=\\left\\{ v_{1},...,v_{n},w\\right\\}$ también genera $V$, y\n",
    "\n",
    "$$\\left[ w\\right]_{\\beta }  =\\left( \\begin{matrix}c_{1}\\\\ c_{2}\\\\ \\vdots \\\\ c_{n}\\\\ 0\\end{matrix} \\right)  \\Longleftrightarrow w=\\sum^{n}_{k=1} c_{k}v_{k}+0\\cdot w$$\n",
    "<p style=\"text-align: right;\">$(1.82)$</p>\n",
    "\n",
    "ya que $\\alpha$ genera $V$. Por otra parte, parece existir un problema de ambigüedad en nuestra definición de generador, ya que existe una representación diferente para $w$, pues\n",
    "\n",
    "$$\\left[ w\\right]_{\\beta }  =\\left( \\begin{matrix}0\\\\ 0\\\\ \\vdots \\\\ 0\\\\ 1\\end{matrix} \\right)  \\Longleftrightarrow w=\\sum^{n}_{k=1} 0\\cdot v_{k}+1\\cdot w$$\n",
    "<p style=\"text-align: right;\">$(1.83)$</p>\n",
    "\n",
    "Para caracterizar esta ambigüedad, supongamos que $\\alpha =\\left\\{ v_{1},...,v_{n}\\right\\}$ es un sistema de generadores para $V$ y que $u\\in V$ tiene dos representaciones diferentes en este sistema. Entonces, debemos tener\n",
    "\n",
    "- **(C1):** $u=\\sum^{n}_{k=1} a_{k}v_{k}\\wedge u=\\sum^{n}_{k=1} b_{k}v_{k}$.\n",
    "- **(C2):** Si estas representaciones son distintas, entonces existe $i$, con $1\\leq i\\leq n$, tal que $a_{i}\\neq b_{i}$.\n",
    "- **(C3):** Luego tenemos\n",
    "\n",
    "$$\\begin{array}{lll}u=\\sum^{n}_{k=1} a_{k}v_{k}=\\sum^{n}_{k=1} b_{k}v_{k}&\\Longrightarrow &\\sum^{n}_{k=1} \\left( a_{k}-b_{k}\\right)  v_{k}=O_{V}\\\\ &\\Longrightarrow &\\left( a_{1}-b_{1}\\right)  v_{1}+\\left( a_{2}-b_{2}\\right)  v_{2}+\\cdots +\\underbrace{\\left( a_{i}-b_{i}\\right)  v_{i}}_{\\neq 0} +\\cdots +\\left( a_{n}-b_{n}\\right)  v_{n}\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(1.84)$</p>\n",
    "\n",
    "En conclusión, si un vector $u\\in V$ se representa de más de una manera, como combinación lineal de los elementos de $\\alpha$, entonces el vector nulo $O_{V}$ se representa de más de una manera, como combinación lineal de los elementos de $\\alpha$. Luego, en virtud de nuestra lógica previamente desarrollada, podemos decir que, si $O_{V}$ se escribe de una única forma como combinación lineal de los elementos de $\\alpha$, entonces todo vector $u\\in V$ posee la misma propiedad. ◼︎\n",
    "\n",
    "El ejemplo anterior motiva la siguiente definición.\n",
    "\n",
    "**<font color='blue'>Definición 1.19 – Independencia lineal:</font>** Sea $V$ un espacio vectorial y $\\alpha =\\left\\{ v_{1},...,v_{n}\\right\\}  \\subset V$. Diremos que $\\alpha$ es un conjunto **linealmente independiente** (LI) en $V$, si $O_{V}$ se escribe de forma única como combinación lineal de los elementos de $\\alpha$. En caso contrario, diremos que $\\alpha$ es **linealmente dependiente** (LD) en $V$. Por lo tanto, en símbolos,\n",
    "\n",
    "$$\\alpha \\  \\mathrm{es} \\  \\mathrm{LI} \\  \\mathrm{en} \\  V\\Longleftrightarrow \\sum^{n}_{k=1} a_{k}v_{k}=O_{V}\\Longrightarrow a_{k}=0$$\n",
    "<p style=\"text-align: right;\">$(1.85)$</p>\n",
    "\n",
    "**Ejemplo 1.17:** Vamos a demostrar que el conjunto $\\alpha =\\left\\{ 1,1+x,1+x+x^{2}\\right\\}  \\subset \\mathbb{R}_{2} \\left[ x\\right]$ es LI en $\\mathbb{R}_{2} \\left[ x\\right]$. En efecto, de acuerdo a la definición (1.19), si suponemos que $a_{1}+a_{2}\\left( 1+x\\right)  +a_{3}\\left( 1+x+x^{2}\\right)  =O_{\\mathbb{R}_{2} \\left[ x\\right]  }$, entonces debemos demostrar que $a_{1}=a_{2}=a_{3}=0$. Por lo tanto,\n",
    "\n",
    "$$\\begin{array}{lll}a_{1}+a_{2}\\left( 1+x\\right)  +a_{3}\\left( 1+x+x^{2}\\right)  =O_{\\mathbb{R}_{2} \\left[ x\\right]  }&\\Longrightarrow &a_{1}+a_{2}+a_{2}x+a_{3}+a_{3}x+a_{3}x^{2}=0+0x+0x^{2}\\\\ &\\Longrightarrow &\\left( a_{1}+a_{2}+a_{3}\\right)  +\\left( a_{2}+a_{3}\\right)  x+a_{3}x^{2}=0+0x+0x^{2}\\\\ &\\Longrightarrow &\\begin{cases}\\begin{array}{rcl}a_{1}+a_{2}+a_{3}&=&0\\\\ a_{2}+a_{3}&=&0\\\\ a_{3}&=&0\\end{array} &\\Longrightarrow \\  a_{1}=a_{2}=a_{3}=0\\end{cases} \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(1.86)$</p>\n",
    "\n",
    "Así que $\\alpha$ es LI en $\\mathbb{R}_{2} \\left[ x\\right]$. ◼︎\n",
    "\n",
    "La independencia lineal es uno de los conceptos más importantes en el álgebra lineal. Intuitivamente, un conjunto de vectores linealmente independientes es tal que dichos vectores no son redundantes entre sí. O en palabras menos formales, ningún vector es múltiplo escalar del otro."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63128306",
   "metadata": {},
   "source": [
    "## Base y dimensión.\n",
    "Ahora tenemos construidas y listas para usar las piezas para garantizar la existencia y unicidad de una representación como combinación lineal para cada vector de un espacio vectorial. Partimos, entonces, con la siguiente definición.\n",
    "\n",
    "**<font color='blue'>Definición 1.20 – Base:</font>** Sea $V$ un espacio vectorial y $\\alpha =\\left\\{ v_{1},...,v_{n}\\right\\}  \\subset V$. Diremos que $\\alpha$ es una base para el espacio vectorial $V$ si se cumplen las siguientes condiciones:\n",
    "\n",
    "- **(C1):** $\\alpha$ es un sistema de generadores para $V$.\n",
    "- **(C2):** $\\alpha$ es un conjunto linealmente independiente en $V$.\n",
    "\n",
    "**Ejemplo 1.18:** Si $V\\in \\mathbb{R}^{n\\times s}$, entonces $\\alpha =\\left\\{ e_{ij}\\  :\\  e_{ij}=1\\  ;\\  \\forall a_{kl}\\in \\mathbb{R} \\  |\\  i=k,j=l\\wedge e_{ij}=0\\  ;\\  \\forall a_{kl}\\in \\mathbb{R} \\  |\\  i\\neq k,j\\neq l\\right\\}$ es llamada **base canónica** de $\\mathbb{R}^{n\\times s}$. Por ejemplo, para $n=s=3$:\n",
    "\n",
    "$$\\begin{array}{lll}\\left( \\begin{matrix}a_{11}&a_{12}&a_{13}\\\\ a_{21}&a_{22}&a_{23}\\\\ a_{31}&a_{32}&a_{33}\\end{matrix} \\right)  &=&a_{11}e_{11}+a_{12}e_{12}+a_{13}e_{13}+a_{21}e_{21}+a_{22}e_{22}+a_{23}e_{23}+a_{31}e_{31}+a_{32}e_{32}+a_{33}e_{33}\\\\ &=&a_{11}\\left( \\begin{matrix}1&0&0\\\\ 0&0&0\\\\ 0&0&0\\end{matrix} \\right)  +a_{12}\\left( \\begin{matrix}0&1&0\\\\ 0&0&0\\\\ 0&0&0\\end{matrix} \\right)  +a_{13}\\left( \\begin{matrix}0&0&1\\\\ 0&0&0\\\\ 0&0&0\\end{matrix} \\right)  +a_{21}\\left( \\begin{matrix}0&0&0\\\\ 1&0&0\\\\ 0&0&0\\end{matrix} \\right)  +a_{22}\\left( \\begin{matrix}0&0&0\\\\ 0&1&0\\\\ 0&0&0\\end{matrix} \\right)  +a_{23}\\left( \\begin{matrix}0&0&0\\\\ 0&0&1\\\\ 0&0&0\\end{matrix} \\right)  +a_{31}\\left( \\begin{matrix}0&0&0\\\\ 0&0&0\\\\ 1&0&0\\end{matrix} \\right)  +a_{32}\\left( \\begin{matrix}0&0&0\\\\ 0&0&0\\\\ 0&1&0\\end{matrix} \\right)  +a_{33}\\left( \\begin{matrix}0&0&0\\\\ 0&0&0\\\\ 0&0&1\\end{matrix} \\right)  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(1.87)$</p>\n",
    "◼︎\n",
    "\n",
    "**Ejemplo 1.19:** Si $V=\\mathbb{R}_{n}[x]$, entonces el conjunto $\\alpha =\\left\\{ 1,x,x^{2},...,x^{n}\\right\\}$ es llamado **base canónica** de $\\mathbb{R}_{n}[x]$, pues genéricamente un polinomio se escribe como\n",
    "\n",
    "$$p\\left( x\\right)  =a_{0}\\cdot 1+a_{1}\\cdot x+a_{2}\\cdot x^{2}+\\cdots +a_{n}\\cdot x^{n}$$\n",
    "<p style=\"text-align: right;\">$(1.88)$</p>\n",
    "◼︎\n",
    "\n",
    "**Ejemplo 1.20:** En $\\mathbb{R}^{n}$, sea $\\alpha =\\left\\{ v_{1},...,v_{n}\\right\\}$ una base. Entonces:\n",
    "\n",
    "- $c\\alpha =\\left\\{ cv_{1},...,cv_{n}\\right\\}$ es una nueva base para $\\mathbb{R}^{n}$, para cada $c\\neq 0$.\n",
    "- $\\alpha^{+} =\\left\\{ v_{1},v_{1}+v_{2},v_{1}+v_{2}+v_{3},...,\\sum^{n}_{k=1} v_{k}\\right\\}$ es una nueva base para $\\mathbb{R}^{n}$.\n",
    "- En general, $\\beta =\\left\\{ w_{1},...,w_{n}\\right\\}$, donde $w_{j}=\\sum^{j}_{i=1} a_{i}v_{i}$ para $(a_{1},...,a_{n})\\in \\mathbb{R}^{n}$ fijo, es una base de $\\mathbb{R}^{n}$. \n",
    "\n",
    "◼︎\n",
    "\n",
    "**<font color='crimson'>Teorema 1.4:</font>** *Sea $V$ un espacio vectorial y $\\alpha =\\left\\{ v_{1},...,v_{n}\\right\\}$ una base de $V$. Entonces cualquier subconjunto de $V$ que posea más de $n$ elementos es linealmente dependiente en $V$.* ◆\n",
    "\n",
    "Este teorema nos lleva al siguiente corolario.\n",
    "\n",
    "**<font color='magenta'>Corolario 1.1:</font>** *Todas las bases de un espacio vectorial tienen el mismo número de vectores.* ◆\n",
    "\n",
    "**<font color='blue'>Definición 1.21 – Dimensión de un espacio vectorial:</font>** Si $V$ es un $\\mathbb{K}$-espacio vectorial, llamaremos **dimensión** de $V$ sobre el cuerpo de escalares $\\mathbb{K}$ al número de elementos que constituyen una base de $V$, y denotaremos ese valor como $\\dim \\left( V\\right)$.\n",
    "\n",
    "**Ejemplo 1.21:** Usando la información desarrollada en los ejemplos y definiciones anteriores, tenemos que:\n",
    "\n",
    "- $\\dim (\\mathbb{R}^{n})=n$, pues toda base de $\\mathbb{R}^{n}$ tiene $n$ elementos. De hecho, su base canónica es $\\alpha =\\left\\{ \\left( 1,0,...,0\\right)  ,\\left( 0,1,...,0\\right)  ,...,\\left( 0,0,...,1\\right)  \\right\\}  =\\left< \\left\\{ \\mathbf{e}_{n} \\right\\}  \\right>$, la que evidentemente consta de $n$ vectores.\n",
    "- $\\dim (\\mathbb{C})=2$, pues todo número complejo $z\\in \\mathbb{C}$ tiene una base $\\alpha=(u, v)$ tal que $z=u+iv$, la que consta de 2 elementos.\n",
    "- $\\dim (\\mathbb{R}_{n}[x])=n+1$, pues la base canónica de $\\mathbb{R}_{n}[x]$ es $\\alpha =\\left\\{ x^{0},x^{1},...,x^{n}\\right\\}  =\\left\\{ 1,x,...,x^{n}\\right\\}$, la que consta de un total de $n+1$ elementos.\n",
    "- $\\dim (\\mathbb{R}^{m\\times n})=mn$, pues la base canónica de $\\mathbb{R}^{m\\times n}$ tiene un total de $mn$ elementos, como vimos en el ejemplo (1.18).\n",
    "\n",
    "◼︎"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3bb5d0",
   "metadata": {},
   "source": [
    "## Transformaciones lineales.\n",
    "A continuación, estudiaremos aplicaciones que mapean objetos entre espacios vectoriales y que preservan su estructura, las cuales nos permitirán, una vez definidas, construir el concepto de *coordenada*. Al principio de esta sección, dijimos que los vectores son objetos que pueden sumarse entre sí y multiplicarse por un escalar, siendo el resultado de ambas operaciones también un vector. Queremos preservar esta propiedad cuando implementemos estas aplicaciones: Consideremos, para ello, dos espacios vectoriales $V$ y $W$. Una aplicación $T:V\\longrightarrow W$ preserva la estructura de un espacio vectorial si se cumple que\n",
    "\n",
    "$$T\\left( u+v\\right)  =T\\left( u\\right)  +T\\left( v\\right)  \\wedge T\\left( \\lambda u\\right)  =\\lambda T\\left( u\\right)$$\n",
    "<p style=\"text-align: right;\">$(1.89)$</p>\n",
    "\n",
    "Para todo $u,v\\in V$ y $\\lambda \\in \\mathbb{R}$. Esto motiva la siguiente definición.\n",
    "\n",
    "**<font color='blue'>Definición 1.22 – Transformación lineal (general):</font>** Sean $V$ y $W$ dos $\\mathbb{K}$-espacios vectoriales y $T:V\\longrightarrow W$ una función. Diremos que $T$ es una **transformación lineal** entre $V$ y $W$ si esta función verifica las siguientes propiedades:\n",
    "\n",
    "- **(P1):** $T(u+v)=T(u)+T(v);\\forall u,v\\in V$.\n",
    "- **(P2):** $T(\\lambda u)=\\lambda T(u);\\forall u\\in V\\wedge \\lambda \\in \\mathbb{R}$.\n",
    "\n",
    "Una notación utilizada muchísimo en álgebra lineal para denotar las transformaciones lineales es\n",
    "\n",
    "$$\\mathbb{L}_{\\mathbb{K}}(V,W)=\\left\\{ T:V\\longrightarrow W\\  |\\  T\\  \\mathrm{es} \\  \\mathrm{una} \\  \\mathrm{transformacion} \\  \\mathrm{lineal} \\right\\}$$\n",
    "<p style=\"text-align: right;\">$(1.90)$</p>\n",
    "\n",
    "**Ejemplo 1.22:** Sea $T:\\mathbb{R}^{3}\\longrightarrow \\mathbb{R}^{2}$ una función definida como $T(x,y,z)=(x-y+z,x+z)$. Vamos a demostrar que $T$ es una transformación lineal. En efecto, conforme a la definición (1.22), en primer lugar, introducimos e interpretamos los datos del dominio de $T$:\n",
    "\n",
    "- $u\\in \\mathbb{R}^{3}\\Longleftrightarrow u=(u_{1},u_{2},u_{3})$, donde $u_{i}\\in \\mathbb{R}$ para $i=1,2,3$.\n",
    "- $v\\in \\mathbb{R}^{3}\\Longleftrightarrow v=(v_{1},v_{2},v_{3})$, donde $v_{i}\\in \\mathbb{R}$ para $i=1,2,3$.\n",
    "- $u+v=(u_{1}+v_{1},u_{2}+v_{2},u_{3}+v_{3})\\in \\mathbb{R}^{3}$.\n",
    "\n",
    "Ahora debemos mostrar que $T(u+v)=T(u)+T(v)$. De lo anterior,\n",
    "\n",
    "$$\\begin{array}{lll}T\\left( u+v\\right)  &=&T\\left( \\left( u_{1}+v_{1},u_{2}+v_{2},u_{3}+v_{3}\\right)  \\right)  \\\\ &=&\\left( \\left( u_{1}+v_{1}\\right)  -\\left( u_{2}+v_{2}\\right)  +\\left( u_{3}+v_{3}\\right)  ,\\left( u_{1}+v_{1}\\right)  +\\left( u_{3}+v_{3}\\right)  \\right)  \\\\ &=&\\left( u_{1}+v_{1}-u_{2}-v_{2}+u_{3}+v_{3},u_{1}+v_{1}+u_{3}+v_{3}\\right)  \\\\ &=&\\left( \\left[ u_{1}-u_{2}+u_{3}\\right]  +\\left[ v_{1}-v_{2}+v_{3}\\right]  ,\\left[ u_{1}+u_{3}\\right]  +\\left[ v_{1}+v_{3}\\right]  \\right)  \\\\ &=&\\left( u_{1}-u_{2}+u_{3},u_{1}+u_{3}\\right)  +\\left( v_{1}-v_{2}+v_{3},v_{1}+v_{3}\\right)  \\\\ &=&T\\left( \\left( u_{1},u_{2},u_{3}\\right)  \\right)  +T\\left( \\left( v_{1},v_{2},v_{3}\\right)  \\right)  \\\\ &=&T\\left( u\\right)  +T\\left( v\\right)  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(1.91)$</p>\n",
    "\n",
    "Finalmente, debemos demostrar que $T(\\lambda u)=\\lambda T(u)$. En efecto,\n",
    "\n",
    "$$\\begin{array}{lll}T\\left( \\lambda u\\right)  &=&T\\left( \\left( \\lambda u_{1},\\lambda u_{2},\\lambda u_{3}\\right)  \\right)  \\\\ &=&\\left( \\lambda u_{1}-\\lambda u_{2}+\\lambda u_{3},\\lambda u_{1}+\\lambda u_{3}\\right)  \\\\ &=&\\lambda \\left( u_{1}-u_{2}+u_{3},u_{1}+u_{3}\\right)  \\\\ &=&\\lambda T\\left( \\left( u_{1},u_{2},u_{3}\\right)  \\right)  \\\\ &=&\\lambda T\\left( u\\right)  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(1.92)$</p>\n",
    "\n",
    "Así que, efectivamente, $T$ es una transformación lineal. ◼︎\n",
    "\n",
    "### Propiedades de las transformaciones lineales.\n",
    "Sea $T:V\\longrightarrow W$ una transformación lineal entre los espacios vectoriales $V$ y $W$. La función $T$ será llamada:\n",
    "\n",
    "- **Inyectiva**, si $\\forall u,v\\in V:T\\left( u\\right)  =T\\left( v\\right)  \\Longrightarrow u=v$.\n",
    "- **Sobreyectiva**, si $T(V)=W$.\n",
    "- **Biyectiva**; si $T$ es inyectiva y sobreyectiva a la vez.\n",
    "\n",
    "Si $T$ es sobreyectiva, entonces cada elemento en $W$ puede ser *recuperado* desde $V$ usando la función $T$. Por definición, si $T$ es biyectiva, entonces existe una función $\\Omega: W\\longrightarrow V$ tal que $\\Omega (T(u))=u$. Tal función $\\Omega$ es llamada **inversa** de la transformación lineal $T$, y se representa como $T^{-1}$.\n",
    "\n",
    "Vamos a verificar otras **propiedades cualitativas** de las transformaciones lineales que son esenciales para su completo entendimiento. En este contexto, sea $T:V\\longrightarrow W$ una transformación lineal entre los espacios vectoriales $V$ y $W$, tal que $\\dim(V)=n$. Entonces:\n",
    "\n",
    "- **(P1):** $T(O_{V})=O_{W}$. En efecto,\n",
    "\n",
    "$$T\\left( O_{V}\\right)  =T\\left( O_{V}+O_{V}\\right)  =T\\left( O_{V}\\right)  +T\\left( O_{V}\\right)  \\Longrightarrow T\\left( O_{V}\\right)  =O_{W}$$\n",
    "<p style=\"text-align: right;\">$(1.93)$</p>\n",
    "\n",
    "- **(P2):** $T$ inyectiva $\\Longrightarrow n\\geq \\dim(W)$. En efecto,\n",
    "\n",
    "a) Si $\\alpha =\\left\\{ v_{1},...,v_{n}\\right\\}$ es una base de $V$, entonces $T(\\alpha):=\\left\\{ T\\left( v_{1}\\right)  ,...,T\\left( v_{n}\\right)  \\right\\}  \\subset W$.\n",
    "\n",
    "b) La imagen $T(\\alpha)$ puede ser LI o LD en $W$. Verifiquemos cuál es su condición:\n",
    "\n",
    "$$\\begin{array}{lll}\\sum^{n}_{i=1} \\alpha_{i} T\\left( v_{i}\\right)  =O_{W}&\\Longrightarrow &T\\left( \\sum^{n}_{i=1} \\alpha_{i} v_{i}\\right)  =O_{W}\\\\ &\\Longrightarrow &T\\left( \\sum^{n}_{i=1} \\alpha_{i} v_{i}\\right)  =T\\left( O_{V}\\right)  \\  \\left( \\mathrm{por} \\  \\mathrm{la} \\  \\mathrm{propiedad} \\  \\mathrm{(P1)} \\right)  \\\\ &\\Longrightarrow &\\sum^{n}_{i=1} \\alpha_{i} v_{i}=O_{V}\\  \\left( \\mathrm{ya} \\  \\mathrm{que} \\  T\\  \\mathrm{es} \\  \\mathrm{inyectiva} \\right)  \\\\ &\\Longrightarrow &\\alpha_{i} =0;\\forall i=1,...,n\\  \\left( \\mathrm{pues} \\  \\alpha \\  \\mathrm{es} \\  \\mathrm{LI} \\  \\mathrm{en} \\  W\\right)  \\\\ &\\Longrightarrow &T\\left( \\alpha \\right)  \\  \\mathrm{es} \\  \\mathrm{LI} \\  \\mathrm{en} \\  W\\\\ &\\Longrightarrow &\\dim \\left( W\\right)  \\leq n\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(1.94)$</p>\n",
    "\n",
    "- **(P3):** $T$ sobreyectiva $\\Longrightarrow n\\leq \\dim(W)$. En efecto,\n",
    "\n",
    "a) Como $T$ es sobreyectiva, entonces $\\mathrm{im}(T)=W$; es decir,\n",
    "\n",
    "$$w\\in W\\Longrightarrow \\left( \\exists u;u\\in V\\right)  :T\\left( u\\right)  =w$$\n",
    "<p style=\"text-align: right;\">$(1.95)$</p>\n",
    "\n",
    "b) Como $\\dim(V)=n$, entonces podemos considerar la existencia de una base $\\alpha =\\left\\{ v_{1},...,v_{n}\\right\\}$ de $V$ y, entonces, considerando la ecuación (1.95), tenemos que\n",
    "\n",
    "$$\\begin{array}{lll}u\\in W&\\Longleftrightarrow &\\left( \\exists u;u\\in V\\right)  :u=\\sum^{n}_{i=1} \\alpha_{i} v_{i}\\wedge T\\left( \\sum^{n}_{i=1} \\alpha_{i} v_{i}\\right)  =w\\\\ &\\Longleftrightarrow &\\left( \\exists u;u\\in V\\right)  :u=\\sum^{n}_{i=1} \\alpha_{i} v_{i}\\wedge \\sum^{n}_{i=1} \\alpha_{i} T\\left( v_{i}\\right)  =w\\\\ &\\Longleftrightarrow &w\\in \\left< \\left\\{ T\\left( v_{1}\\right)  ,...,T\\left( v_{n}\\right)  \\right\\}  \\right>  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(1.96)$</p>\n",
    "\n",
    "Luego $W=\\left< \\left\\{ T\\left( v_{1}\\right)  ,...,T\\left( v_{n}\\right)  \\right\\}  \\right>  \\Longrightarrow \\dim \\left( W\\right)  \\leq n$. Es conveniente recordar que la dimensión de un espacio vectorial es el número mínimo de vectores generadores soportado por dicho espacio.\n",
    "\n",
    "- **(P4):** $T$ biyectiva $\\Longrightarrow \\dim(V)=\\dim(W)$. En efecto,\n",
    "\n",
    "$$\\begin{array}{lll}T\\  \\mathrm{inyectiva} &\\Longrightarrow &n\\geq \\dim \\left( W\\right)  \\\\ T\\  \\mathrm{sobreyectiva} &\\Longrightarrow &n\\leq \\dim \\left( W\\right)  \\end{array} \\  \\Longrightarrow \\dim \\left( V\\right)  =\\dim \\left( W\\right)$$\n",
    "<p style=\"text-align: right;\">$(1.97)$</p>\n",
    "\n",
    "**<font color='blue'>Definición 1.23 – Isomorfismo (general) de espacios vectoriales:</font>** Sean $V$ y $W$ dos $\\mathbb{K}$-espacios vectoriales y $T:V\\longrightarrow W$ una función. Diremos que $T$ es un **isomorfismo de espacios vectoriales** si se cumplen las siguientes condiciones:\n",
    "\n",
    "- **(C1):** $T$ es una transformación lineal entre $V$ y $W$.\n",
    "- **(C2):** $T$ es una función biyectiva.\n",
    "\n",
    "En tal caso, diremos que los $\\mathbb{K}$-espacios vetoriales $V$ y $W$ son **isomorfos** y lo denotaremos como $V\\cong W$.\n",
    "\n",
    "**<font color='crimson'>Teorema 1.5:</font>** *Sean $V$ y $W$ dos $\\mathbb{K}$-espacios vetoriales. Entonces tenemos que:*\n",
    "\n",
    "**(a)** $V\\cong W \\Longrightarrow \\dim_{\\mathbb{K}}(V)=\\dim_{\\mathbb{K}}(W)$.\n",
    "**(b)** *Recíprocamente, si $\\dim_{\\mathbb{K}}(V)=\\dim_{\\mathbb{K}}(W)$, entonces existe un isomorfismo de espacios vectoriales que depende, naturalmente, de las bases de cada uno de los $\\mathbb{K}$-espacios vectoriales $V$ y $W$. ◆ \n",
    "\n",
    "### Teorema de la dimensión.\n",
    "El teorema de la dimensión corresponde a uno de los resultados más importantes del álgebra lineal en relación a las transformaciones lineales y, puntualmente, en relación a los isomorfismos entre espacios vectoriales. Para poder enunciarlo, primero, necesitamos definir algunos conceptos previos relativos a ciertos conjuntos que son característicos de las transformaciones lineales y que representan, de alguna manera, sus rangos definitorios.\n",
    "\n",
    "**<font color='blue'>Definición 1.24 – Kernel de una transformación lineal (general):</font>** Sean $V$ y $W$ dos $\\mathbb{K}$-espacios vectoriales y sea $T:V\\longrightarrow W$ una transformación lineal entre ambos. Llamaremos **núcleo** o **kernel** de la función $T$ al conjunto\n",
    "\n",
    "$$\\ker \\left( T\\right)  =\\left\\{ u\\in V:T\\left( u\\right)  =O_{W}\\right\\}$$\n",
    "<p style=\"text-align: right;\">$(1.98)$</p>\n",
    "\n",
    "**<font color='blue'>Definición 1.25 – Imagen de una transformación lineal (general):</font>** Sean $V$ y $W$ dos $\\mathbb{K}$-espacios vectoriales y sea $T:V\\longrightarrow W$ una transformación lineal entre ambos. Diremos que la **imagen** o **rango** de la función $T$ corresponde al conjunto contenido en $W$ conformado por todos los valores que puede llegar a tomar $T$. Este conjunto se denota como $\\mathrm{im}(T)$ y, en símbolos, podemos definirlo como\n",
    "\n",
    "$$\\mathrm{im} \\left( T\\right)  =\\left\\{ w\\in W:\\exists v\\in V,f\\left( u\\right)  =w\\right\\}$$\n",
    "<p style=\"text-align: right;\">$(1.99)$</p>\n",
    "\n",
    "Con las definiciones anteriores, ya estamos en condiciones de enunciar el siguiente teorema.\n",
    "\n",
    "**<font color='crimson'>Teorema 1.6 – Teorema de la dimensión:</font>** *Sean $V$ y $W$ dos espacios vectoriales tales que $\\dim (V)=n$, con $n\\in \\mathbb{N}$, y sea $T$ una transformación lineal entre $V$ y $W$. Entonces tenemos que*\n",
    "\n",
    "$$\\dim \\left( V\\right)  =\\dim \\left( \\ker \\left( T\\right)  \\right)  +\\dim \\left( \\mathrm{im} \\left( T\\right)  \\right)$$\n",
    "<p style=\"text-align: right;\">$(1.100)$</p>\n",
    "◆ "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8c0d44c",
   "metadata": {},
   "source": [
    "### Representación matricial de las transformaciones lineales.\n",
    "Cualquier espacio vectorial de dimensión $n$ es isomorfo a $\\mathbb{R}^{n}$ (por el teorema (1.5)). Consideremos una base $\\alpha =\\left\\{ v_{1},...,v_{n}\\right\\}$ de un espacio vectorial $V$. En lo que sigue, el **orden** de los elementos dispuestos en una base como $\\alpha$ será de importancia. Por lo tanto, escribiremos $\\alpha=(v_{1},...,v_{n})$ y llamaremos a esta tupla **base ordenada** de $V$.\n",
    "\n",
    "**<font color='blue'>Definición 1.26 – Representación coordenada:</font>** Consideremos un espacio vectorial $V$ (que supondremos, sin pérdida de generalidad, sobre el cuerpo de escalares $\\mathbb{R}$) y una base ordenada $\\alpha=(v_{1},...,v_{n})$ de $V$. Para cualquier $v\\in V$ obtenemos una única representación (combinación lineal)\n",
    "\n",
    "$$v=a_{1}v_{1}+\\cdots +a_{n}v_{n}$$\n",
    "\n",
    "de $v$ respecto de $\\alpha$. Entonces llamaremos a los escalares $a_{1},...,a_{n}$ **coordenadas** de $v$ con respecto a la base ordenada $\\alpha$, y el vector $\\mathbf{a}=(a_{1},...,a_{n})\\in \\mathbb{R}^{n}$ será llamado **vector coordenado** o **representación coordenada** de la base ordenada $\\alpha$.\n",
    "\n",
    "Una base define efectivamente un sistema de coordenadas. Estamos familiarizados con el sistema de coordenadas cartesianas o rectangulares, el cual es generado a partir de los vectores coordenados unitarios que, comúnmente, solemos representar mediante lo símbolos como $\\mathbf{e}_{1}$ y $\\mathbf{e}_{2}$. En este sistema, un vector $\\mathbf{x}\\in \\mathbb{R}^{2}$ dispone de una representación que nos explicita como combinar $\\mathbf{e}_{1}$ con $\\mathbf{e}_{2}$ para obtener $\\mathbf{x}$. Sin embargo, cualquier base de $\\mathbb{R}^{2}$ define un sistema coordenado y el mismo vector $\\mathbf{x}$ de antes puede tener diferentes representaciones coordenadas dependiendo de la base que utilicemos. En la Fig. (1.6a), las coordenadas de $\\mathbf{x}$ con respecto a la base canónica ($\\mathbf{e}_{1}$, $\\mathbf{e}_{2}$) son representadas por el vector $(2, 2)$. Sin embargo, en el sistema de coordenadas mostrado en la Fig. (1.6b), el mismo vector $\\mathbf{x}$ se representa mediante las coordenadas $(1.09, 0.72)$. Por lo tanto, en este sistema, con base $(\\mathbf{b}_{1},\\mathbf{b}_{2})$, $\\mathbf{x}$ se representa como $\\mathbf{x}=1.09\\mathbf{b}_{1}+0.72\\mathbf{b}_{2}$. Más adelante, explicaremos como obtener estas representaciones.\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"figures/fig_1_6.png\" width=\"500\"></p>\n",
    "<p style=\"text-align: center;\">Fig. (1.6): Dos sistemas coordenados distintos definidos por dos juegos de vectores coordenados. Esto pone de manifiesto que un vector $\\mathbf{x}$ tiene diferentes representaciones coordenadas dependiendo del sistema elegido (imagen tomada del fantástico libro \"Mathematics for Machine Learning\", (Deinsenroth, M., Faisal, A. & Soon Ong, C.; 2021))</p>\n",
    "\n",
    "En síntesis, para un espacio vectorial $V$ de dimensión $n$ y una base ordenada $\\alpha$ de $V$, la función $T:\\mathbb{R}^{n}\\longrightarrow V$ definida como $T(\\mathbf{e}_{i})=\\mathbf{b}_{i}$ para $1\\leq i\\leq n$, es lineal (y, por el teorema (1.5), es un isomorfismo de espacios vectoriales), donde $(\\mathbf{e}_{1},...,\\mathbf{e}_{n})$ es la base canónica de $\\mathbb{R}^{n}$.\n",
    "\n",
    "Ahora ya estamos listos para formular una conexión explícita entre las matrices y las transformaciones lineales entre espacios vectoriales de dimensión finita.\n",
    "\n",
    "**<font color='blue'>Definición 1.27 – Matriz de cambio de base:</font>** Consideremos los espacios vectoriales $V$ y $W$ (de dimensión $n$ y $m$, respectivamente) con sus correspondientes bases ordenadas $\\alpha=(v_{1},...,v_{n})$ y $\\beta=(w_{1},...,w_{n})$. Consideremos ademas una transformación lineal $T:V\\longrightarrow W$. Para $j\\in \\left\\{ 1,...,n\\right\\}$, diremos que\n",
    "\n",
    "$$T\\left( v_{j}\\right)  =a_{1j}w_{1}+\\cdots +a_{mj}v_{m}=\\sum^{m}_{i=1} a_{ij}w_{i}$$\n",
    "<p style=\"text-align: right;\">$(1.101)$</p>\n",
    "\n",
    "es la representación única de $T(v_{j})$ con respecto a $\\beta$. Dado lo anterior, la matriz de $m\\times n$ denotada como $[\\mathbf{I}]_{\\alpha}^{\\beta}$, cuyos elementos se definen como $[\\mathbf{I}]_{\\alpha}^{\\beta}(i,j)=a_{ij}$, será llamada **matriz de cambio de base** (o **matriz de transformación**) entre (las bases de) los espacios vectoriales $V$ y $W$.\n",
    "\n",
    "Las coordenadas de $T(v_{j})$ con respecto a la base ordenada $\\beta$ de $W$ se corresponden con la $j$-ésima columna de $[\\mathbf{I}]_{\\alpha}^{\\beta}$. Consideremos los espacios vectoriales $V$ y $W$ (de dimensión finita) con bases ordenadas $\\alpha$ y $\\beta$ y una transformación lineal $T:V\\longrightarrow W$ con matriz de cambio de base $[\\mathbf{I}]_{\\alpha}^{\\beta}$. Si $\\hat{\\mathbf{v}}$ es el vector coordenado de $v\\in V$ con respecto a $\\alpha$, y $\\hat{\\mathbf{w}}$ es el vector coordenado de $w=T(v)\\in W$ con respecto a $\\beta$, entonces\n",
    "\n",
    "$$\\hat{\\mathbf{w} } =\\left[ \\mathbf{I} \\right]^{\\beta }_{\\alpha }  \\hat{\\mathbf{v} }$$\n",
    "<p style=\"text-align: right;\">$(1.102)$</p>\n",
    "\n",
    "Esto significa que la matriz de cambio de base $[\\mathbf{I}]_{\\alpha}^{\\beta}$ puede ser utilizada para aplicar coordenadas con respecto a una base ordenada de $V$ sobre coordenadas con respecto a una base ordenada de $W$.\n",
    "\n",
    "**<font color='crimson'>Teorema 1.7 – Cambio de base:</font>** *Sean $V$ y $W$ dos espacios vectoriales de dimensión $n$ y sean $\\alpha=(v_{1},...,v_{n})$ y $\\beta=(w_{1},...,w_{n})$ dos bases ordenadas de $V$ y $W$, respectivamente. Para todo $u\\in V$, pongamos $[u]_{\\alpha}=\\sum^{n}_{i=1} a_{i}v_{i}$ y $\\left[ u\\right]_{\\beta }  =\\sum^{n}_{j=1} b_{i}w_{i}$ para representar la generación del vector $u$ por medio de ambas bases ordenadas. Entonces tenemos que*\n",
    "\n",
    "$$\\left[ \\mathbf{I} \\right]^{\\beta }_{\\alpha }  \\left[ u\\right]_{\\alpha }  =\\left[ u\\right]_{\\beta }$$\n",
    "<p style=\"text-align: right;\">$(1.103)$</p>\n",
    "◆ \n",
    "\n",
    "**Ejemplo 1.23:** Consideremos la función $T:\\mathbb{R}^{3\\times 1}\\longrightarrow \\mathbb{R}^{2}$, definida como\n",
    "\n",
    "$$T\\left( \\begin{matrix}x\\\\ y\\\\ z\\end{matrix} \\right)  =\\left( x-\\lambda y+4z,\\lambda x-y+3z\\right)$$\n",
    "<p style=\"text-align: right;\">$(1.104)$</p>\n",
    "\n",
    "- **(a):** Vamos a demostrar que $T$ es una transformación lineal entre $\\mathbb{R}^{3\\times 1}$ y $\\mathbb{R}^{2}$, para todo $\\lambda \\in \\mathbb{R}$.\n",
    "- **(b):** Vamos a determinar el conjunto $S=\\left\\{ \\lambda \\in \\mathbb{R} :\\ker \\left( T\\right)  =\\left\\{ O_{\\mathbb{R}^{3\\times 1} }\\right\\}  \\right\\}$.\n",
    "\n",
    "En efecto, para resolver **(a)**, debemos verificar que, para $\\mathbf{A}, \\mathbf{B}\\in \\mathbb{R}^{3\\times 1}$, se tiene que $T(\\mathbf{A}+\\mathbf{B})=T(\\mathbf{A})+T(\\mathbf{B})$. De esta manera, sean entonces\n",
    "\n",
    "$$\\mathbf{A} ,\\mathbf{B} \\in \\mathbb{R}^{3\\times 1} \\Longleftrightarrow \\mathbf{A} =\\left( \\begin{matrix}x_{1}\\\\ y_{1}\\\\ z_{1}\\end{matrix} \\right)  \\wedge \\mathbf{B} =\\left( \\begin{matrix}x_{2}\\\\ y_{2}\\\\ z_{2}\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(1.105)$</p>\n",
    "\n",
    "Luego tenemos,\n",
    "\n",
    "$$\\begin{array}{lll}T\\left( \\mathbf{A} +\\mathbf{B} \\right)  &=&T\\left( \\begin{matrix}x_{1}+x_{2}\\\\ y_{1}+y_{2}\\\\ z_{1}+z_{2}\\end{matrix} \\right)  \\\\ &=&\\left( x_{1}+x_{2}-\\lambda \\left( y_{1}+y_{2}\\right)  +4\\left( z_{1}+z_{2}\\right)  ,\\lambda \\left( x_{1}+x_{2}\\right)  -\\left( y_{1}+y_{2}\\right)  +3\\left( z_{1}+z_{2}\\right)  \\right)  \\\\ &=&\\left( x_{1}+x_{2}-\\lambda y_{1}-\\lambda y_{2}+4z_{1}+4z_{2},\\lambda x_{1}+\\lambda x_{2}-y_{1}-y_{2}+3x_{1}+3z_{2}\\right)  \\\\ &=&\\left( x_{1}-\\lambda y_{1}+4z_{1},\\lambda x_{1}-y_{1}+3z_{1}\\right)  +\\left( x_{2}+\\lambda y_{2}+4z_{2},\\lambda x_{2}-y_{2}+3z_{2}\\right)  \\\\ &=&T\\left( \\begin{matrix}x_{1}\\\\ y_{1}\\\\ z_{1}\\end{matrix} \\right)  +T\\left( \\begin{matrix}x_{2}\\\\ y_{2}\\\\ z_{2}\\end{matrix} \\right)  \\\\ &=&T\\left( \\mathbf{A} \\right)  +T\\left( \\mathbf{B} \\right)  \\end{array} $$\n",
    "<p style=\"text-align: right;\">$(1.106)$</p>\n",
    "\n",
    "Así que, efectivamente, $T\\left( \\mathbf{A} +\\mathbf{B} \\right)=T\\left( \\mathbf{A} \\right)  +T\\left( \\mathbf{B} \\right)$. Ahora debemos demostrar que, para $\\mu \\in \\mathbb{R}$ y $\\mathbf{A}\\in \\mathbb{R}^{3\\times 1}$, $T(\\mu \\mathbf{A}) = \\mu T(\\mathbf{A})$. De esta manera,\n",
    "\n",
    "$$\\begin{array}{lll}T\\left( \\mu \\mathbf{A} \\right)  &=&T\\left( \\begin{matrix}\\mu x_{1}\\\\ \\mu x_{2}\\\\ \\mu x_{3}\\end{matrix} \\right)  \\\\ &=&\\left( \\mu x_{1}-\\lambda \\mu y_{1}+4\\mu z_{1},\\lambda \\mu x_{1}-\\mu y_{1}+3\\mu z_{1}\\right)  \\\\ &=&\\mu \\left( x_{1}-\\lambda y_{1}+4z_{1},\\lambda x_{1}-y_{1}+3z_{1}\\right)  \\\\ &=&\\mu T\\left( \\begin{matrix}x_{1}\\\\ y_{1}\\\\ z_{1}\\end{matrix} \\right)  =\\mu T\\left( \\mathbf{A} \\right)  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(1.107)$</p>\n",
    "\n",
    "Así que, efectivamente, $T(\\mu \\mathbf{A}) = \\mu T(\\mathbf{A})$.\n",
    "\n",
    "Toca ahora resolver **(b)**. De esta manera, determinaremos el conjunto $S=\\left\\{ \\lambda \\in \\mathbb{R} :\\ker \\left( T\\right)  =\\left\\{ O_{\\mathbb{R}^{3\\times 1} }\\right\\}  \\right\\}$. La forma más sencilla de resolver este problema es mediante la aplicación del teorema (1.6), ya que\n",
    "\n",
    "$$\\dim_{\\mathbb{R} } \\left( \\mathbb{R}^{3\\times 1} \\right)  =3=\\dim_{\\mathbb{R} } \\left( \\ker \\left( T\\right)  \\right)  +\\dim_{\\mathbb{R} } \\left( \\mathrm{im} \\left( T\\right)  \\right)$$\n",
    "<p style=\"text-align: right;\">$(1.108)$</p>\n",
    "\n",
    "Pero como $\\dim_{\\mathbb{R}}(\\mathrm{im}(T))\\leq \\dim_{\\mathbb{R}}(\\mathbb{R}^{2})=2$, entonces $\\dim_{\\mathbb{R}}(\\ker (T))=1$ 0 $\\dim_{\\mathbb{R}}(\\ker (T))=2$. Sin embargo, dado que $\\dim_{\\mathbb{R}}(\\mathbb{R}^{3})$, se tiene que $S=\\emptyset$.\n",
    "\n",
    "Siempre podemos proceder usando la definición de kernel en cualquier caso. De este modo, basta con considerar\n",
    "\n",
    "$$\\lambda \\in S\\Longleftrightarrow \\lambda \\in \\mathbb{R} \\wedge \\ker \\left( T\\right)  =\\left\\{ O_{\\mathbb{R}^{3\\times 1} }\\right\\}$$\n",
    "<p style=\"text-align: right;\">$(1.109)$</p>\n",
    "\n",
    "Luego debemos estudiar $\\ker(T)$. Así que,\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{A} \\in \\ker \\left( T\\right)  &\\Longleftrightarrow &\\mathbf{A} \\in \\mathbb{R}^{3\\times 1} \\wedge T\\left( \\mathbf{A} \\right)  =O_{\\mathbb{R}^{2} }\\\\ &\\Longleftrightarrow &\\mathbf{A} =\\left( \\begin{matrix}x\\\\ y\\\\ z\\end{matrix} \\right)  \\in \\mathbb{R}^{3\\times 1} \\wedge T\\left( \\begin{matrix}x\\\\ y\\\\ z\\end{matrix} \\right)  =\\left( 0,0\\right)  \\\\ &\\Longleftrightarrow &\\mathbf{A} =\\left( \\begin{matrix}x\\\\ y\\\\ z\\end{matrix} \\right)  \\in \\mathbb{R}^{3\\times 1} \\wedge \\left( x-\\lambda y+4z,\\lambda x-y+3z\\right)  =\\left( 0,0\\right)  \\\\ &\\Longleftrightarrow &\\mathbf{A} =\\left( \\begin{matrix}x\\\\ y\\\\ z\\end{matrix} \\right)  \\in \\mathbb{R}^{3\\times 1} \\wedge \\begin{cases}\\begin{array}{rcl}x-\\lambda y+4z&=&0\\\\ \\lambda x-y+3z&=&0\\end{array} &\\end{cases} \\\\ &\\Longleftrightarrow &\\mathbf{A} =\\left( \\begin{matrix}x\\\\ y\\\\ z\\end{matrix} \\right)  \\in \\mathbb{R}^{3\\times 1} \\wedge \\underbrace{\\left( \\begin{matrix}1&-\\lambda &4\\\\ \\lambda &-1&3\\end{matrix} \\right)  }_{\\mathbf{B} } \\left( \\begin{matrix}x\\\\ y\\\\ z\\end{matrix} \\right)  =\\left( \\begin{matrix}0\\\\ 0\\\\ 0\\end{matrix} \\right)  \\\\ &\\Longleftrightarrow &\\mathbf{A} =\\left( \\begin{matrix}x\\\\ y\\\\ z\\end{matrix} \\right)  \\in \\mathbb{R}^{3\\times 1} \\wedge \\rho \\left( \\mathbf{B} \\right)  \\leq 2;\\forall \\lambda \\in \\mathbb{R} \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(1.110)$</p>\n",
    "\n",
    "Luego, el sistema, por ser homogéneo, siempre tiene solución, pero nunca tendrá solución única, ya que, en ese caso, su rango debería ser igual a 3, y entonces $\\ker \\left( T\\right)  \\neq \\left\\{ O_{\\mathbb{R}^{3\\times 1} }\\right\\}$. Por lo tanto, $S=\\emptyset$. ◼︎\n",
    "\n",
    "**Ejemplo 1.24:** Consideremos la función $T:\\mathbb{R}^{3}\\longrightarrow \\mathbb{R}_{2}[x]$ definida explícitamente como $T\\left( a,b,c\\right)  =\\left( a+2b+3c\\right)  +\\left( a-3b+c\\right)  x+\\left( a+b+c\\right)  x^{2}$. Vamos a demostrar que $T$ es un isomorfismo de espacios vectoriales.\n",
    "\n",
    "En primer lugar, debemos demostrar que $T$ es una transformación lineal entre $\\mathbb{R}^{3}$ y $\\mathbb{R}_{2}[x]$. Para ello, consideramos\n",
    "\n",
    "$$\\begin{array}{lll}u_{1}\\in \\mathbb{R}^{3} &\\Longleftrightarrow &u_{1}=\\left( a_{1},b_{1},c_{1}\\right)  \\\\ u_{2}\\in \\mathbb{R}^{3} &\\Longleftrightarrow &u_{2}=\\left( a_{2},b_{2},c_{2}\\right)  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(1.111)$</p>\n",
    "\n",
    "Luego tenemos,\n",
    "\n",
    "$$\\begin{array}{lll}T\\left( u_{1}+u_{2}\\right)  &=&T\\left( a_{1}+a_{2},b_{1}+b_{2},c_{1}+c_{2}\\right)  \\\\ &=&\\left( a_{1}+a_{2}+2\\left( b_{1}+b_{2}\\right)  +3\\left( c_{1}+c_{2}\\right)  \\right)  +\\left( a_{1}+a_{2}-3\\left( b_{1}-b_{2}\\right)  +c_{1}+c_{2}\\right)  x+\\left( a_{1}+a_{2}+b_{1}+b_{2}+c_{1}+c_{2}\\right)  x^{2}\\\\ &=&\\left( a_{1}+a_{2}+2b_{1}+2b_{2}+3c_{1}+3c_{2}\\right)  +\\left( a_{1}+a_{2}-3b_{1}-3b_{2}+c_{1}+c_{2}\\right)  x+\\left( a_{1}+a_{2}+b_{1}+b_{2}+c_{1}+c_{2}\\right)  x^{2}\\\\ &=&\\left( a_{1}+2b_{1}+3c_{1}\\right)  +\\left( a-3b_{1}+c_{1}\\right)  x+\\left( a_{1}+b_{1}+c_{1}\\right)  x^{2}+\\left( a_{2}+2b_{2}+3c_{2}\\right)  +\\left( a_{2}-3b_{2}+c_{2}\\right)  x+\\left( a_{2}+b_{2}+c_{2}\\right)  x^{2}\\\\ &=&T\\left( a_{1},b_{1},c_{1}\\right)  +T\\left( a_{2},b_{2},c_{2}\\right)  \\\\ &=&T\\left( u_{1}\\right)  +T\\left( u_{2}\\right)  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(1.112)$</p>\n",
    "\n",
    "Además, para $\\lambda \\in \\mathbb{R}$,\n",
    "\n",
    "$$\\begin{array}{lll}T\\left( \\lambda u_{1}\\right)  &=&T\\left( \\lambda a_{1},\\lambda b_{1},\\lambda c_{1}\\right)  \\\\ &&\\left( \\lambda a_{1}+2\\lambda b_{1}+3\\lambda c_{1}\\right)  +\\left( \\lambda a_{1}-3\\lambda b_{1}+\\lambda c_{1}\\right)  x+\\left( \\lambda a_{1}+\\lambda b_{1}+\\lambda c_{1}\\right)  x^{2}\\\\ &&\\lambda \\left( a_{1}+2b_{1}+3c_{1}\\right)  +\\left( \\lambda \\left( a_{1}-3b_{1}+c_{1}\\right)  \\right)  x+\\left( \\lambda \\left( a_{1}+b_{1}+c_{1}\\right)  \\right)  x^{2}\\\\ &&\\lambda \\left( \\left( a_{1}+2b_{1}+3c_{1}\\right)  +\\left( a_{1}-3b_{1}+c_{1}\\right)  x+\\left( a_{1}+b_{1}+c_{1}\\right)  x^{2}\\right)  \\\\ &&\\lambda T\\left( a_{1},b_{1},c_{1}\\right)  \\\\ &&\\lambda T\\left( u_{1}\\right)  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(1.113)$</p>\n",
    "\n",
    "Así que, en efecto, $T\\in \\mathbb{L}_{\\mathbb{R}}(\\mathbb{R}^{3},\\mathbb{R}_{2}[x])$.\n",
    "\n",
    "Ahora, para demostrar que $T$ es un isomorfismo, debemos verificar que $T$ es biyectiva. Sin embargo, debido a que $\\dim \\left( \\mathbb{R}^{3} \\right)  =\\dim \\left( \\mathbb{R}_{2} \\left[ x\\right]  \\right)  =3$, bastará determinar, por el teorema (1.6), la dimensión de la imagen o el kernel de $T$ para determinar si $T$ es biyectiva. Estudiemos, pues, el kernel de $T$:\n",
    "\n",
    "$$\\begin{array}{lll}u\\in \\ker \\left( T\\right)  &\\Longleftrightarrow &u\\in \\mathbb{R}^{3} \\wedge T\\left( u\\right)  =O_{\\mathbb{R}_{2} \\left[ x\\right]  }\\\\ &\\Longleftrightarrow &u=\\left( a,b,c\\right)  \\in \\mathbb{R}^{3} \\wedge T\\left( a,b,c\\right)  =0+0x+0x^{2}\\\\ &\\Longleftrightarrow &u=\\left( a,b,c\\right)  \\in \\mathbb{R}^{3} \\wedge \\left( a+2b+3c\\right)  +\\left( a-3b+c\\right)  x+\\left( a+b+c\\right)  x^{2}=0+0x+0x^{2}\\\\ &\\Longleftrightarrow &u=\\left( a,b,c\\right)  \\in \\mathbb{R}^{3} \\wedge \\begin{cases}\\begin{array}{c}\\left( 1\\right)  \\\\ \\left( 2\\right)  \\\\ \\left( 3\\right)  \\end{array} &\\begin{array}{rcl}a+2b+3c&=&0\\\\ a-3b+c&=&0\\\\ a+b+c&=&0\\end{array} \\end{cases} \\\\ &\\overbrace{\\Longleftrightarrow }^{\\left( 3\\right)  -\\left( 2\\right)  } &u=\\left( a,b,c\\right)  \\in \\mathbb{R}^{3} \\wedge \\begin{cases}\\begin{array}{c}\\left( 1\\right)  \\\\ \\left( 2\\right)  \\\\ \\left( 4\\right)  \\end{array} &\\begin{array}{rcl}a+2b+3c&=&0\\\\ a-3b+c&=&0\\\\ 4b&=&0\\end{array} \\end{cases} \\\\ &\\overbrace{\\Longleftrightarrow }^{\\mathrm{por} \\  \\left( 4\\right)  } &u=\\left( a,b,c\\right)  \\in \\mathbb{R}^{3} \\wedge \\begin{cases}\\begin{array}{c}\\left( 6\\right)  \\\\ \\left( 7\\right)  \\\\ \\left( 5\\right)  \\end{array} &\\begin{array}{rcl}a+3c&=&0\\\\ a+c&=&0\\\\ b&=&0\\end{array} \\end{cases} \\\\ &\\overbrace{\\Longleftrightarrow }^{\\left( 6\\right)  -\\left( 7\\right)  } &u=\\left( a,b,c\\right)  \\in \\mathbb{R}^{3} \\wedge \\begin{cases}\\begin{array}{c}\\left( 6\\right)  \\\\ \\left( 7\\right)  \\\\ \\left( 5\\right)  \\end{array} &\\begin{array}{rcl}2c&=&0\\\\ a+c&=&0\\\\ b&=&0\\end{array} \\end{cases} \\\\ &\\Longleftrightarrow &u=\\left( a,b,c\\right)  \\in \\mathbb{R}^{3} \\wedge a=b=c=0\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(1.114)$</p>\n",
    "\n",
    "Así que $T$ es inyectiva y, conforme lo visto previamente, es un isomorfismo. ◼︎\n",
    "\n",
    "**Ejemplo 1.25:** Sea $\\alpha =\\left\\{ x,x^{2}+3,2x^{2}+x\\right\\}$ una base de $\\mathbb{R}_{2}[x]$. Vamos a estudiar la posibilidad de determinar otra base $\\beta$ de $\\mathbb{R}_{2}[x]$, tal que\n",
    "\n",
    "$$\\left[ \\mathbf{I} \\right]^{\\beta }_{\\alpha }  =\\left( \\begin{matrix}2&2&0\\\\ 3&-1&1\\\\ 0&1&2\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(1.115)$</p>\n",
    "\n",
    "En efecto, primero definimos el *esqueleto* de la base $\\beta$. Es decir, $\\beta =\\left\\{ p_{1}\\left( x\\right)  ,p_{2}\\left( x\\right)  ,p_{3}\\left( x\\right)  \\right\\}$. En tal caso, partiendo de la definición (1.27), tenemos que\n",
    "\n",
    "$$\\left[ \\mathbf{I} \\right]^{\\beta }_{\\alpha }  =\\left( \\left[ x\\right]_{\\beta }  ,\\left[ x^{2}+3\\right]_{\\beta }  ,\\left[ 2x^{2}+x\\right]_{\\beta }  \\right)  =\\left( \\begin{matrix}2&2&0\\\\ 3&-1&1\\\\ 0&1&2\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(1.116)$</p>\n",
    "\n",
    "Así que, a partir de lo anterior, tenemos que:\n",
    "\n",
    "$$\\begin{array}{rcl}\\left[ x\\right]_{\\beta }  =\\left( \\begin{matrix}2\\\\ 3\\\\ 0\\end{matrix} \\right)  &\\Longleftrightarrow &x=2p_{1}\\left( x\\right)  +3p_{2}\\left( x\\right)  \\\\ \\left[ x^{2}+3\\right]_{\\beta }  =\\left( \\begin{matrix}2\\\\ -1\\\\ 1\\end{matrix} \\right)  &\\Longleftrightarrow &x^{2}+3=2p_{1}\\left( x\\right)  -p_{2}\\left( x\\right)  +p_{3}\\left( x\\right)  \\\\ \\left[ 2x^{2}+x\\right]_{\\beta }  =\\left( \\begin{matrix}0\\\\ 1\\\\ 2\\end{matrix} \\right)  &\\Longleftrightarrow &2x^{2}+x=p_{2}\\left( x\\right)  +2p_{3}\\left( x\\right)  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(1.117)$</p>\n",
    "\n",
    "Luego,\n",
    "\n",
    "$$\\begin{array}{crcl}&\\begin{array}{lrcl}\\left( 1\\right)  &2p_{1}\\left( x\\right)  +3p_{2}\\left( x\\right)  &=&x\\\\ \\left( 2\\right)  &2p_{1}\\left( x\\right)  -p_{2}\\left( x\\right)  +p_{3}\\left( x\\right)  &=&x^{2}+3\\\\ \\left( 3\\right)  &p_{2}\\left( x\\right)  +2p_{3}\\left( x\\right)  &=&2x^{2}+x\\end{array} &\\overbrace{\\Longrightarrow }^{\\left( 1\\right)  -\\left( 2\\right)  } &\\begin{array}{lrcl}\\left( 4\\right)  &4p_{2}\\left( x\\right)  -p_{3}\\left( x\\right)  &=&-x^{2}+x-3\\\\ \\left( 2\\right)  &2p_{1}\\left( x\\right)  -p_{2}\\left( x\\right)  +p_{3}\\left( x\\right)  &=&x^{2}+3\\\\ \\left( 3\\right)  &p_{2}\\left( x\\right)  +2p_{3}\\left( x\\right)  &=&2x^{2}+x\\end{array} \\\\ \\Longrightarrow &\\begin{array}{lrcl}\\left( 5\\right)  &8p_{2}\\left( x\\right)  -2p_{3}\\left( x\\right)  &=&-2x^{2}+2x-6\\\\ \\left( 2\\right)  &2p_{1}\\left( x\\right)  -p_{2}\\left( x\\right)  +p_{3}\\left( x\\right)  &=&x^{2}+3\\\\ \\left( 3\\right)  &p_{2}\\left( x\\right)  +2p_{3}\\left( x\\right)  &=&2x^{2}+x\\end{array} &\\overbrace{\\Longrightarrow }^{\\left( 5\\right)  +\\left( 3\\right)  } &\\begin{array}{lrcl}\\left( 5\\right)  &9p_{2}\\left( x\\right)  &=&3x-6\\\\ \\left( 2\\right)  &2p_{1}\\left( x\\right)  -p_{2}\\left( x\\right)  +p_{3}\\left( x\\right)  &=&x^{2}+3\\\\ \\left( 3\\right)  &p_{2}\\left( x\\right)  +2p_{3}\\left( x\\right)  &=&2x^{2}+x\\end{array} \\\\ \\Longrightarrow &\\begin{array}{lrcl}\\left( 6\\right)  &p_{2}\\left( x\\right)  &=&\\frac{1}{3} x-\\frac{2}{3} \\\\ \\left( 2\\right)  &2p_{1}\\left( x\\right)  -p_{2}\\left( x\\right)  +p_{3}\\left( x\\right)  &=&x^{2}+3\\\\ \\left( 3\\right)  &p_{2}\\left( x\\right)  +2p_{3}\\left( x\\right)  &=&2x^{2}+x\\end{array} &\\overbrace{\\Longrightarrow }^{\\begin{matrix}\\left( 3\\right)  -\\left( 6\\right)  \\\\ \\left( 2\\right)  +\\left( 6\\right)  \\end{matrix} } &\\begin{array}{lrcl}\\left( 6\\right)  &p_{2}\\left( x\\right)  &=&\\frac{1}{3} x-\\frac{2}{3} \\\\ \\left( 7\\right)  &2p_{1}\\left( x\\right)  +p_{3}\\left( x\\right)  &=&x^{2}+\\frac{1}{3} x+\\frac{7}{3} \\\\ \\left( 8\\right)  &p_{3}\\left( x\\right)  &=&x^{2}+\\frac{1}{3} x+\\frac{1}{3} \\end{array} \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(1.117)$</p>\n",
    "\n",
    "Por lo tanto, los polinomios $p_{1}(x), p_{2}(x)$ y $p_{3}(x)$ que conforman la base $\\beta$ son $p_{1}(x)=1$, $p_{2}(x)=\\frac{1}{3}x-\\frac{2}{3}$ y $p_{3}(x)=x^{2}+\\frac{1}{3}x+\\frac{1}{3}$. Por lo tanto, la base $\\beta$, definida como\n",
    "\n",
    "$$\\beta =\\left\\{ 1,\\frac{1}{3} x-\\frac{2}{3} ,x^{2}+\\frac{1}{3} x+\\frac{1}{3} \\right\\}$$\n",
    "<p style=\"text-align: right;\">$(1.118)$</p>\n",
    "\n",
    "cumple con los requisitos pedidos en el enunciado de nuestro ejemplo. ◼︎\n",
    "\n",
    "Vemos que los conceptos de kernel e imagen son absolutamente fundamentales a la hora de definir si una transformación lineal $T$ cumple con los requisitos para ser un isomorfismo de espacios vectoriales. En la Fig. (1.7), se muestra una esquematización de estos conceptos.\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"figures/fig_1_7.png\" width=\"600\"></p>\n",
    "<p style=\"text-align: center;\">Fig. (1.7): Un esquema que ilustra el kernel y la imagen de una transformación lineal $T:V\\longrightarrow W$</p>\n",
    "\n",
    "En estos apuntes, hacemos una distinción entre los distintos tópicos que son propios del álgebra lineal (por ejemplo, vectores, matrices, independencia lineal, bases, dimensión, etc.) y otros tópicos referidos a la geometría de un espacio vectorial. En la siguiente sección, introduciremos el concepto de producto interno, el cual induce una norma. Dicho concepto nos permitirá definir ángulos, distancias y longitudes, que serán elementos que utilizaremos para definir el concepto de proyección ortogonal, el cual será clave en una serie de algoritmos de aprendizaje, tales como el modelo de regresión lineal y el análisis de componentes principales, que veremos más adelante."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
