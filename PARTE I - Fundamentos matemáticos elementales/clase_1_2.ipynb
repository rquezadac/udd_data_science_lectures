{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5aa20e36",
   "metadata": {},
   "source": [
    "# CLASE 1.2: Espacios vectoriales con producto interno\n",
    "---\n",
    "\n",
    "## Introducción.\n",
    "En la sección anterior, estudiamos los conceptos de vectores, espacios vectoriales y transformaciones lineales en términos generales, pero además, completamente abstractos. En esta sección, añadiremos a estos conceptos abstractos algunas interpretaciones geométricas a fin de construir un cierto nivel de intuición respecto de estos conceptos. En particular, visualizaremos vectores desde una perspectiva geométrica y calcularemos sus longitudes y distancias o ángulos con respecto a otros vectores. Para poder hacer esto, vamos a equipar a los espacios vectoriales con una operación especial conocida como producto interno, cuya propiedad fundamental será la inducción de la geometría relativa al espacio vectorial respectivo.\n",
    "\n",
    "Los productos internos y sus normas correspondientes y métricas nos permiten capturar las nociones intuitivas de distancia y similitud, las que resultan fundamentales en la construcción de uno de los modelos de machine learning más importantes que existen: Las máquinas de soporte vectorial (support vector machines, SVM). Luego, utilizaremos los conceptos de longitud y ángulo entre vectores para discutir las proyecciones ortogonales, las que jugarán un papel fundamental cuando estudiemos dos de los modelos de aprendizaje más elementales en machine learning: El análisis de componentes principales (que es un modelo de aprendizaje no supervisado) y el modelo de regresión lineal (que es un modelo de aprendizaje supervisado)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "14b08a52",
   "metadata": {},
   "source": [
    "## Producto interno.\n",
    "Cuando pensamos en vectores desde una perspectiva puramente geométrica; es decir, líneas dirigidas que parten desde el origen y terminan en un punto determinado, siempre hemos asociado a estos vectores el concepto de longitud del mismo en términos de la distancia entre el origen y dicho punto. A continuación, formalizaremos esta noción intuitiva mediante el concepto de **norma**.\n",
    "\n",
    "**<font color='blue'>Definición 2.1 – Norma:</font>** Sea $V$ un espacio vectorial que supondremos (sin pérdida de generalidad) definido sobre el cuerpo $\\mathbb{R}$. Para todo $v\\in V$ definimos la función\n",
    "\n",
    "$$\\begin{array}{ll}\\| \\  \\cdot \\  \\| :&V\\longrightarrow \\mathbb{R} \\\\ &v\\longrightarrow \\left\\Vert v\\right\\Vert  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(2.1)$</p>\n",
    "\n",
    "y que será llamada **norma**. Esta función asigna a $v\\in V$ su **longitud** $\\left\\Vert v\\right\\Vert\\in \\mathbb{R}$, y es tal que, para todo $\\lambda \\in \\mathbb{R}$ y para cualquier otro vector $u\\in V$, cumple con las siguientes propiedades:\n",
    "\n",
    "- **(P1) – Homogeneidad absoluta:** $\\left\\Vert \\lambda v\\right\\Vert  =\\left| \\lambda \\right|  \\left\\Vert v\\right\\Vert$.\n",
    "- **(P2) – Desigualdad triangular:** $\\left\\Vert u+v\\right\\Vert  \\leq \\left\\Vert u\\right\\Vert  +\\left\\Vert v\\right\\Vert$.\n",
    "- **(P3) – Definida positiva:** $\\left\\Vert v\\right\\Vert  \\geq 0\\wedge \\left\\Vert v\\right\\Vert  =0\\Longleftrightarrow v=O_{V}$.\n",
    "\n",
    "En términos geométricos, por ejemplo, la desigualdad triangular puede interpretarse por medio de vectores en $\\mathbb{R}^{n}$ para $n\\leq 3$, estableciendo que, para un triángulo cualquiera, la suma de sus longitudes de dos de sus lados debe ser mayor o igual que la longitud del lado restante (lo que se ilustra en la Fig. (2.1)). \n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"figures/fig_2_1.png\" width=\"400\"></p>\n",
    "<p style=\"text-align: center;\">Fig. (2.1): Una interpretación geométrica sencilla de la desigualdad triangular</p>\n",
    "\n",
    "La definición (2.1) es válida para cualquier espacio vectorial, pero, para efectos prácticos, bastará con que consideremos únicamente a aquellos con dimensión finita (y, puntualmente, nos limitaremos en muchos casos simplemente a $\\mathbb{R}^{n}$.\n",
    "\n",
    "**Ejemplo 2.1 – La norma $\\ell_{1}$:** La definición (2.1) establece las condiciones que debe cumplir una función (denotada como $\\left\\Vert \\cdot \\right\\Vert$) para ser considerada una norma sobre un determinado espacio vectorial (ya que opera con los elementos de dicho espacio). Por esa razón es que existen varios tipos de normas que son utilizadas en muchos campos de las matemáticas. Un ejemplo es la **norma $\\ell_{1}$**, llamada comúnmente **norma Manhattan**, que se define para cualquier vector $\\mathbf{x}\\in \\mathbb{R}^{n}$ (donde $\\mathbf{x}=(x_{1},...,x_{n})$) como\n",
    "\n",
    "$$\\left\\Vert \\mathbf{x} \\right\\Vert_{1}  :=\\sum^{n}_{i=1} \\left| x_{i}\\right|$$\n",
    "<p style=\"text-align: right;\">$(2.2)$</p>\n",
    "\n",
    "Donde $\\left| x_{i}\\right|$ es el valor absoluto de la *componente* $x_{i}$. En la Fig. (2.2a) se muestran todos los puntos en el plano $\\mathbb{R}^{2}$ tales que $\\left\\Vert \\mathbf{x} \\right\\Vert_{1}  =1$. ◼︎\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"figures/fig_2_2.png\" width=\"700\"></p>\n",
    "<p style=\"text-align: center;\">Fig. (2.2): (a) Todos los puntos $\\mathbf{x}$ en el plano tales que $\\left\\Vert \\mathbf{x} \\right\\Vert_{1}  =1$ ; (b) Todos los puntos $\\mathbf{x}$ en el plano tales que $\\left\\Vert \\mathbf{x} \\right\\Vert_{2}  =1$</p>\n",
    "\n",
    "**Ejemplo 2.2 – La norma $\\ell_{2}$:** Otro tipo de norma muy común en las matemáticas (y en el análisis en $\\mathbb{R}^{n}$) corresponde a la **norma $\\ell_{2}$:**, conocida igualmente como **norma Euclidiana**, la que se define para cualquier vector $\\mathbf{x}\\in \\mathbb{R}^{n}$ (donde $\\mathbf{x}=(x_{1},...,x_{n})$) como\n",
    "\n",
    "$$\\left\\Vert \\mathbf{x} \\right\\Vert_{2}  :=\\left( \\sum^{n}_{i=1} x^{2}_{i}\\right)^{\\frac{1}{2} }  =\\sqrt{\\mathbf{x}^{\\top } \\mathbf{x} }$$\n",
    "<p style=\"text-align: right;\">$(2.3)$</p>\n",
    "\n",
    "Esta norma permite calcular la distancia Euclidiana del vector $\\mathbf{x}\\in \\mathbb{R}^{n}$ con respecto al origen del sistema de coordenadas rectangulares. En la Fig. (2.2b) se muestran todos los puntos en el plano $\\mathbb{R}^{2}$ tales que $\\left\\Vert \\mathbf{x} \\right\\Vert_{2}  =1$. La norma $\\ell_{2}$ es una norma que usaremos a menudo durante esta asignatura, y será frecuente que la denotamos como la opción por defecto de la función de norma (poniendo simplemente $\\left\\Vert \\mathbf{x} \\right\\Vert$ en vez de $\\left\\Vert \\mathbf{x} \\right\\Vert_{2}$), salvo que especifiquemos lo contrario. ◼︎\n",
    "\n",
    "La norma es un caso particular de una operación importante en álgebra conocida como **producto interno**, y que definiremos a continuación.\n",
    "\n",
    "**<font color='blue'>Definición 2.2 – Producto interno (general):</font>** Sea $V$ un $\\mathbb{K}$-espacio vectorial. Diremos que la función\n",
    "\n",
    "$$\\begin{array}{ll}\\left< \\  ,\\  \\right>  :&V\\times V\\longmapsto \\mathbb{K} \\\\ &\\left( u,v\\right)  \\longmapsto \\left< u,v\\right>  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(2.4)$</p>\n",
    "\n",
    "es llamada **producto interno** definido sobre $V$. Esta función cumple con las siguientes propiedades:\n",
    "\n",
    "- **(P1):** $\\left< v,v\\right>  \\geq O_{\\mathbb{K} };\\forall v\\in V\\wedge \\left< v,v\\right>  =O_{\\mathbb{K} }\\Longleftrightarrow v=O_{V}$.\n",
    "- **(P2):** $\\left< u+v,w\\right>  =\\left< u,w\\right>  +\\left< v,w\\right>  ;\\forall u,v,w\\in V$.\n",
    "- **(P3):** $\\left< u,v+w\\right>  =\\left< u,v\\right>  +\\left< v,w\\right>  ;\\forall u,v,w\\in V$.\n",
    "- **(P4):** $\\left< \\lambda u,v\\right>  =\\lambda \\left< u,v\\right>  ;\\forall u,v\\in V\\wedge \\lambda \\in \\mathbb{K}$.\n",
    "- **(P5):** $\\left< u,\\lambda v\\right>  =\\bar{\\lambda } \\left< u,v\\right>  ;\\forall u,v\\in V\\wedge \\lambda \\in \\mathbb{K}$.\n",
    "- **(P6):** $\\left< u,v\\right>  =\\overline{\\left< v,u\\right>  } ;\\forall u,v\\in V$.\n",
    "\n",
    "Cuando un $\\mathbb{K}$-espacio vectorial $V$ está *equipado* con un producto interno, se denomina **espacio vectorial normado o prehilbertiano**.\n",
    "\n",
    "**Ejemplo 2.3:** En $\\mathbb{K}^{n}$ (donde $\\mathbb{K}$ puede ser $\\mathbb{R}$ o $\\mathbb{C}$), definimos, para $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{K}^{n}$, con $\\mathbf{x}=(x_{1},...,x_{n})$ y $\\mathbf{y}=(y_{1},...,y_{n})$,\n",
    "\n",
    "$$\\left< \\mathbf{x} ,\\mathbf{y} \\right>  =\\sum^{n}_{k=1} x_{k}\\overline{y}_{k}$$\n",
    "<p style=\"text-align: right;\">$(2.5)$</p>\n",
    "\n",
    "y lo denominaremos **producto interno canónico en $\\mathbb{K}^{n}$**. Notemos que, cuando $\\mathbb{K}=\\mathbb{R}$, se tiene que $\\overline{y}_{k}=y_{k}$ para todo $k=1,...,n$. En un contexto más geométrico, donde estos vectores suelen describir cantidades físicas, tal producto interno suele denominarse como *producto punto*. ◼︎\n",
    "\n",
    "**Ejemplo 2.4:** En $\\mathbb{R}^{n\\times n}$ definimos, para $\\mathbf{A} =\\left\\{ a_{ij}\\right\\}  \\in \\mathbb{R}^{n\\times n} \\wedge \\mathbf{B} =\\left\\{ b_{ij}\\right\\}  \\in \\mathbb{R}^{n\\times n}$,\n",
    "\n",
    "$$\\left< \\mathbf{A} ,\\mathbf{B} \\right>  =\\mathrm{tr} \\left( \\mathbf{B}^{\\top } \\mathbf{A} \\right)$$\n",
    "<p style=\"text-align: right;\">$(2.6)$</p>\n",
    "\n",
    "y lo denominaremos **producto interno canónico de $\\mathbb{R}^{n\\times n}$**. ◼︎\n",
    "\n",
    "**Ejemplo 2.5:** Definimos el conjunto $C^{k}([a,b])$ como el conjunto de todas las funciones $k$ veces diferenciables sobre el intervalo cerrado $[a,b]$ (es decir, funciones de clase $C^{k}$ en el intervalo cerrado $[a,b]\\in \\mathbb{R}$). La operación definida como\n",
    "\n",
    "$$\\left< f,g\\right>  =\\left< f\\left( x\\right)  ,g\\left( x\\right)  \\right>  =\\int^{b}_{a} f\\left( x\\right)  g\\left( x\\right)  dx$$\n",
    "<p style=\"text-align: right;\">$(2.7)$</p>\n",
    "\n",
    "es llamada **producto interno de las funciones $f$ y $g$ para cada $x\\in [a,b]$**. ◼︎"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3ba00cc6",
   "metadata": {},
   "source": [
    "## Longitud y distancia.\n",
    "La norma, en general, corresponde a un caso particular de aplicación del producto interno, en el cual el argumento respectivo es siempre el mismo vector. Por esta razón, decimos que un producto interno sobre un espacio vectorial $V$ siempre induce una norma en $V$. En este caso, podemos re-definir la norma en relación a cualquier producto interno, ya que, para todo $v\\in V$, se tendrá que\n",
    "\n",
    "$$\\left\\Vert v\\right\\Vert  :=\\sqrt{\\left< v,v\\right>  }$$\n",
    "<p style=\"text-align: right;\">$(2.8)$</p>\n",
    "\n",
    "La norma corresponde a un concepto que, por tanto, se desprende de forma natural para cualquier espacio vectorial con producto interno. Sin embargo, no todas las normas son inducidas a partir de un producto interno; la norma $\\ell_{1}$ es un ejemplo de norma que no se corresponde con un producto interno y que resulta importante en procedimientos fundamentales propios de muchos algoritmos de machine learning tales como la regularización de hiperparámetros (donde, mediante un procedimiento iterativo, intentamos evitar que nuestros modelos sobreajusten o aprendan de memoria un patrón extremadamente variable dado un conjunto de datos que deseamos representar). Sin embargo, para definir conceptos geométricos claves, como longitudes, distancias y ángulos, nos limitaremos momentáneamente al uso de normas inducidas. Para ello, partiremos con un importante teorema, que generaliza la desigualdad triangular vista en la definición (2.1).\n",
    "\n",
    "**<font color='crimson'>Teorema 2.1 – Desigualdad de Cauchy-Schwarz:</font>** *Sea $V$ un espacio vectorial normado. Para todo par de vectores $u,v\\in V$ se tiene que*\n",
    "\n",
    "$$\\left| \\left< u,v\\right>  \\right|^{2}  \\leq \\left< u,u\\right>  \\left< v,v\\right>  \\Longleftrightarrow \\left| \\left< u,v\\right>  \\right|^{2}  \\leq \\left\\Vert u\\right\\Vert  \\left\\Vert v\\right\\Vert$$\n",
    "<p style=\"text-align: right;\">$(2.9)$</p>\n",
    "◆ \n",
    "\n",
    "**Ejemplo 2.6:** En el campo de la geometría analítica, con frecuencia, estamos interesados en la longitud de un vector. Podemos utilizar el producto interno para calcular tales longitudes por medio de la ecuación (2.8). Por ejemplo, consideremos el vector $\\mathbf{u}=(1, 1)^{\\top}\\in \\mathbb{R}^{2}$. En este caso, a partir de la definición de norma inducida y, en este caso, utilizando la norma $\\ell_{2}$ (que es, de hecho, el producto interno definido en el ejemplo (2.3)), obenemos\n",
    "\n",
    "$$\\left\\Vert \\mathbf{u} \\right\\Vert  =\\sqrt{\\left< \\mathbf{u} ,\\mathbf{u} \\right>  } =\\sqrt{1^{2}+1^{2}} =\\sqrt{2}$$\n",
    "<p style=\"text-align: right;\">$(2.10)$</p>\n",
    "\n",
    "y que corresponde a la longitud del vector $\\mathbf{u}$. Por otro lado, es posible demostrar que la expresión\n",
    "\n",
    "$$\\mathbf{u}^{\\top } \\mathbf{A} \\mathbf{v} ;\\forall \\mathbf{u} ,\\mathbf{v} \\in V\\wedge \\mathbf{A} \\in \\mathbb{R}^{n\\times n}$$\n",
    "<p style=\"text-align: right;\">$(2.11)$</p>\n",
    "\n",
    "donde $V$ es un espacio vectorial normado, es de hecho un producto interno siempre que la matriz $\\mathbf{A} =\\left\\{ a_{ij}\\right\\}$ sea definida positiva; es decir, si las submatrices $\\tilde{\\mathbf{A} } =\\left\\{ \\tilde{a}_{ij} \\right\\}$, con $i=1,...,n-r$ y $j=1,...,n-r$, para $r = 1,...,n-1$, son todas invertibles. Una matriz que cumple con este criterio es\n",
    "\n",
    "$$\\mathbf{A} =\\left( \\begin{array}{rr}1&-\\frac{1}{2} \\\\ -\\frac{1}{2} &1\\end{array} \\right)$$\n",
    "<p style=\"text-align: right;\">$(2.12)$</p>\n",
    "\n",
    "El producto interno (2.11) induce la norma $\\| \\mathbf{u} \\| =\\mathbf{u}^{\\top } \\mathbf{A} \\mathbf{u}$ para todo $\\mathbf{u}\\in V$. Con esta norma, obtenemos $\\| \\mathbf{u} \\|=\\sqrt{1}=1$. Por lo tanto, la longitud del vector $\\mathbf{u}$ será dependiente de la norma con la cual se defina. Esto abre la posibilidad de pensar en que la geometría no necesariamente tiene que ser Euclidiana, ya que los conceptos de distancia, como veremos un poco más adelante, dependerán de la métrica con la cual equipemos al espacio donde estamos trabajando. ◼︎\n",
    "\n",
    "**<font color='blue'>Definición 2.3 – Distancia y métrica:</font>** Sea $V$ un espacio vectorial normado y sean $u,v\\in V$. Definimos la **distancia** entre los vectores $u$ y $v$ como\n",
    "\n",
    "$$d\\left( u,v\\right)  :=\\left\\Vert u-v\\right\\Vert  =\\sqrt{\\left< u-v,u-v\\right>  }$$\n",
    "<p style=\"text-align: right;\">$(2.13)$</p>\n",
    "\n",
    "Si $V=\\mathbb{R}^{n}$, y utilizamos la norma $\\ell_{2}$, la distancia así definida será llamada **distancia Euclidiana** entre los vectores respectivos. Por otro lado, la función $d$ definida como\n",
    "\n",
    "$$\\begin{array}{ll}d:&V\\times V\\longmapsto \\mathbb{R} \\\\ &\\left( u,v\\right)  \\longmapsto d\\left( u,v\\right)  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(2.14)$</p>\n",
    "\n",
    "será llamada **métrica** del espacio vectorial $V$.\n",
    "\n",
    "De manera similar al concepto de longitud de un vector, la distancia entre dos vectores no requiere de un producto interno para ser definida. Bastará siempre con el concepto de norma, independiente de si ésta fue inducida por un producto interno o no. En cualquier caso, si la norma sí es inducida, el valor de la distancia variará en función del producto interno utilizado.\n",
    "\n",
    "Una métrica $d$ satisface las siguiente condiciones:\n",
    "\n",
    "- **(C1):** La métrica $d$ es **definida positiva**. Es decir, $d(u,v)\\geq 0$ para todo $u,v\\in V$, siendo $d(u,v)=0 \\Longleftrightarrow u=v$.\n",
    "- **(C2):** La métrica $d$ es **simétrica**. Es decir, $d(u,v)=d(v,u); \\forall u,v\\in V$.\n",
    "- **(C3):** La métrica $d$ satisface la **desigualdad triangular**. Es decir, $d(u,w)\\leq d(u,v)+d(v,w); \\forall u,v,w \\in V$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "43cecf9c",
   "metadata": {},
   "source": [
    "## Ortogonalidad.\n",
    "\n",
    "### Ángulo entre vectores.\n",
    "En adición a la posibilidad de definir longitudes de vectores y la distancia entre ellos, los productos internos (y, puntualmente, las normas) también permiten capturar la noción de geometría de un espacio vectorial mediante la definición del *ángulo* $\\omega$ entre dos vectores. Este concepto tiene una interpretación geométrica que resulta natural cuando $V=\\mathbb{R}^{2}$ o $V=\\mathbb{R}^{3}$ y, para definirla, utilizaremos la desigualdad de Cauchy-Schwarz.\n",
    "\n",
    "**<font color='blue'>Definición 2.4 – Ángulo (entre dos vectores):</font>** Sea $V$ un $\\mathbb{K}$-espacio vectorial normado y $u,v\\in V$ tales que $u\\neq O_{V}$ y $v\\neq O_{V}$. Conforme la desigualdad de Cauchy-Schwarz, es posible establecer que\n",
    "\n",
    "$$-1\\leq \\frac{\\left< u,v\\right>  }{\\left\\Vert u\\right\\Vert  \\left\\Vert v\\right\\Vert  } \\leq 1$$\n",
    "<p style=\"text-align: right;\">$(2.15)$</p>\n",
    "\n",
    "Definimos el **ángulo** entre los vectores $u$ y $v$ como el único valor $\\omega$ que satisface la ecuación\n",
    "\n",
    "$$\\cos \\left( \\omega \\right)  =\\frac{\\left< u,v\\right>  }{\\left\\Vert u\\right\\Vert  \\left\\Vert v\\right\\Vert  }$$\n",
    "<p style=\"text-align: right;\">$(2.16)$</p>\n",
    "\n",
    "Intuitivamente, cuando $V=\\mathbb{R}^{2}$ o $V=\\mathbb{R}^{3}$, el concepto de ángulo nos permite entender qué tan similares son las orientaciones de los vectores $\\mathbf{u}$ y $\\mathbf{v}$, cuando $\\mathbf{u}, \\mathbf{v}\\in \\mathbb{R}^{n}$ ($n=2, 3$). Cuando $v$ es un vector arbitrario, para todo espacio vectorial abstracto, el concepto de ángulo es más general y sirve como base para construir elementos más representativos de la geometría de dicho espacio. Volveremos a retomar estos conceptos más adelante, cuando estudiemos las *bases ortogonales*.\n",
    "\n",
    "**Ejemplo 2.7:** Vamos a calcular el ángulo entre los vectores $\\mathbf{x}=(1, 1)^{\\top}\\in \\mathbb{R}^{2}$ e $\\mathbf{y}=(1,2)^{\\top}\\in \\mathbb{R}^{2}$, los que se ilustran en la Fig. (2.3). Utilizando la norma $\\ell_{2}$ y el producto interno canónico de $\\mathbb{R}^{2}$, obtenemos\n",
    "\n",
    "$$\\cos \\left( \\omega \\right)  =\\frac{\\left< \\mathbf{x} ,\\mathbf{y} \\right>  }{\\left\\Vert \\mathbf{x} \\right\\Vert  \\left\\Vert \\mathbf{y} \\right\\Vert  } =\\frac{3}{\\sqrt{10} } \\Longleftrightarrow \\omega =\\arccos \\left( \\frac{3}{\\sqrt{10} } \\right)  \\approx 0.23\\  \\mathrm{rad}$$\n",
    "<p style=\"text-align: right;\">$(2.17)$</p>\n",
    "\n",
    "Así que el ángulo $\\omega$ entre los vectores $\\mathbf{x}$ e $\\mathbf{y}$ es igual a 0.23 radianes, equivalente a unos 18º aproximadamente. ◼︎\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"figures/fig_2_3.png\" width=\"150\"></p>\n",
    "<p style=\"text-align: center;\">Fig. (2.3): El ángulo $\\omega$ entre los vectores $\\mathbf{x}$ e $\\mathbf{y}$</p>\n",
    "\n",
    "Un último concepto clave en relación a la caracterización de espacios vectoriales corresponde al concepto de ortogonalidad, el cual también, como sabemos, admite una interpretación geométrica. En esta primera aproximación, construiremos este concepto aprovechando la definición de ángulo vista previamente, pero más adelante lo formalizaremos cuando estudiemos las *bases ortogonales*.\n",
    "\n",
    "**<font color='blue'>Definición 2.5 – Ortogonalidad:</font>** Sea $V$ un espacio vectorial normado y sean $u,v\\in V$. Diremos que los vectores $u$ y $v$ son **ortogonales** si y sólo si $\\left< u,v\\right>  =O_{V}$. En tal caso, escribiremos $u\\bot v$ para denotar la condición de ortogonalidad entre ambos vectores.\n",
    "\n",
    "Notemos que también podemos establecer que dos vectores son ortogonales si el ángulo $\\omega$ entre ellos es tal que $\\cos(\\omega)=0$.\n",
    "\n",
    "Debemos observar que dos vectores son ortogonales siempre respecto de una determinada definición de producto interno."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cf48a33a",
   "metadata": {},
   "source": [
    "### Bases ortogonales.\n",
    "Sea $\\alpha =\\left( \\mathbf{v}_{1} ,\\mathbf{v}_{2} \\right)$ una base ordenada de $\\mathbb{R}^{2}$ tal que $\\left< \\mathbf{v}_{1} ,\\mathbf{v}_{2} \\right>  =0$. Entonces, como $\\alpha$ es una base de $\\mathbb{R}^{2}$, para $\\mathbf{v}\\in \\mathbb{R}^{2}$, existen escalares únicos $a_{1}, a_{2}\\in \\mathbb{R}$ tales que $\\mathbf{v}=a_{1}\\mathbf{v}_{1} +a_{2}\\mathbf{v}_{2}$. Como $\\left< \\mathbf{v}_{1} ,\\mathbf{v}_{2} \\right>  =0$, entonces podemos *multiplicar* la combinación lineal que genera $\\mathbf{v}$ (usando el producto interno) por $\\mathbf{v}_{1}$, para obtener\n",
    "\n",
    "$$\\left< \\mathbf{v} ,\\mathbf{v}_{1} \\right>  =\\left< a_{1}\\mathbf{v}_{1} +a_{2}\\mathbf{v}_{2} ,\\mathbf{v}_{1} \\right>  =a_{1}\\left< \\mathbf{v}_{1} ,\\mathbf{v}_{1} \\right>  +a_{2}\\underbrace{\\left< \\mathbf{v}_{2} ,\\mathbf{v}_{1} \\right>  }_{=0} =a_{1}\\left< \\mathbf{v}_{1} ,\\mathbf{v}_{1} \\right>$$\n",
    "<p style=\"text-align: right;\">$(2.18)$</p>\n",
    "\n",
    "De donde se tiene que\n",
    "\n",
    "$$a_{1}=\\frac{\\left< \\mathbf{v} ,\\mathbf{v}_{1} \\right>  }{\\left< \\mathbf{v}_{1} ,\\mathbf{v}_{1} \\right>  }$$\n",
    "<p style=\"text-align: right;\">$(2.19)$</p>\n",
    "\n",
    "Notemos que, como $\\mathbf{v}_{1}$ es un vector no nulo, se tiene que $\\left< \\mathbf{v}_{1} ,\\mathbf{v}_{1} \\right> > 0$. Análogamente, siguiendo un procedimiento similar, podemos obtener que\n",
    "\n",
    "$$a_{2}=\\frac{\\left< \\mathbf{v} ,\\mathbf{v}_{2} \\right>  }{\\left< \\mathbf{v}_{2} ,\\mathbf{v}_{2} \\right>  }$$\n",
    "<p style=\"text-align: right;\">$(2.20)$</p>\n",
    "\n",
    "Hemos demostrado pues que, si $\\alpha =(\\mathbf{v}_{1},\\mathbf{v}_{2})$ es una base ordenada de $\\mathbb{R}^{2}$ tal que $\\left< \\mathbf{v}_{1} ,\\mathbf{v}_{2} \\right>  =0$, entonces el vector $\\mathbf{v}\\in \\mathbb{R}^{2}$ puede expresarse como una combinación lineal del tipo\n",
    "\n",
    "$$\\mathbf{v} =\\frac{\\left< \\mathbf{v} ,\\mathbf{v}_{1} \\right>  }{\\left< \\mathbf{v}_{1} ,\\mathbf{v}_{1} \\right>  } \\mathbf{v}_{1} +\\frac{\\left< \\mathbf{v} ,\\mathbf{v}_{2} \\right>  }{\\left< \\mathbf{v}_{2} ,\\mathbf{v}_{2} \\right>  } \\mathbf{v}_{2}$$\n",
    "<p style=\"text-align: right;\">$(2.21)$</p>\n",
    "\n",
    "Vamos a intentar generalizar el resultado anterior para un vector en $\\mathbb{R}^{n}$. Supongamos entonces que $\\alpha = (\\mathbf{v}_{1},...,\\mathbf{v}_{n})$ es una base ordenada de $\\mathbb{R}^{n}$ tal que $\\left< \\mathbf{v}_{i} ,\\mathbf{v}_{j} \\right>  =0$ para $i\\neq j$. Entonces, a partir del hecho de que $\\alpha$ es una base de $\\mathbb{R}^{n}$, se tiene que existe una colección de escalares $\\left\\{ a_{i}\\right\\}^{n}_{i=1}$ tales que, para todo $\\mathbf{u}\\in \\mathbb{R}^{n}$, se tendrá que $\\mathbf{u} =\\sum^{n}_{i=1} a_{i}\\mathbf{v}_{i}$. Por lo tanto, podemos escribir\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{u} =\\displaystyle \\sum^{n}_{i=1} a_{i}\\mathbf{v}_{i} &\\Longrightarrow &\\left< \\mathbf{u} ,\\mathbf{v}_{j} \\right>  =\\left< \\displaystyle \\sum^{n}_{i=1} a_{i}\\mathbf{v}_{i} ,\\mathbf{v}_{j} \\right>  =\\displaystyle \\sum^{n}_{i=1} a_{i}\\overbrace{\\left< \\mathbf{v}_{i} ,\\mathbf{v}_{j} \\right>  }^{=0\\Longleftrightarrow i\\neq j} \\\\ &\\Longrightarrow &\\left< \\mathbf{u} ,\\mathbf{v}_{j} \\right>  =a_{j}\\left< \\mathbf{v}_{j} ,\\mathbf{v}_{j} \\right>  \\\\ &\\Longrightarrow &a_{j}=\\displaystyle \\frac{\\left< \\mathbf{u} ,\\mathbf{v}_{j} \\right>  }{\\left< \\mathbf{v}_{j} ,\\mathbf{v}_{j} \\right>  } ;j=1,...,n\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(2.22)$</p>\n",
    "\n",
    "Hemos demostrado, pues, el siguiente resultado fundamental:\n",
    "\n",
    "$$\\mathbf{u} =\\sum^{n}_{i=1} \\frac{\\left< \\mathbf{u} ,\\mathbf{v}_{i} \\right>  }{\\left< \\mathbf{v}_{i} ,\\mathbf{v}_{i} \\right>  } \\mathbf{v}_{i} \\Longleftrightarrow \\left[ \\mathbf{u} \\right]_{\\alpha }  =\\left( \\begin{array}{c}\\displaystyle \\frac{\\left< \\mathbf{u} ,\\mathbf{v}_{1} \\right>  }{\\left< \\mathbf{v}_{1} ,\\mathbf{v}_{1} \\right>  } \\\\ \\displaystyle \\frac{\\left< \\mathbf{u} ,\\mathbf{v}_{2} \\right>  }{\\left< \\mathbf{v}_{2} ,\\mathbf{v}_{2} \\right>  } \\\\ \\vdots \\\\ \\displaystyle \\frac{\\left< \\mathbf{u} ,\\mathbf{v}_{n} \\right>  }{\\left< \\mathbf{v}_{n} ,\\mathbf{v}_{n} \\right>  } \\end{array} \\right)$$\n",
    "<p style=\"text-align: right;\">$(2.23)$</p>\n",
    "\n",
    "Este desarrollo motiva la siguiente definición.\n",
    "\n",
    "**<font color='blue'>Definición 2.6 – Base ortogonal (general):</font>** Sea $V$ un $\\mathbb{K}$-espacio vectorial normado. Una base ordenada $\\alpha=(v_{1},...,v_{n})\\subset V$ será llamada **base ortogonal** de $V$ si se cumplen las siguientes condiciones:\n",
    "\n",
    "- **(C1):** $\\alpha$ es una base de $V$.\n",
    "- **(C2):** Si $i\\neq j$, entonces $\\left< v_{i},v_{j}\\right>  =0$.\n",
    "\n",
    "Para un vector arbitrario $u\\in V$, la **coordenada** respecto de la base ortogonal $\\alpha$, digamos $a_{i}=\\frac{\\left< u,v_{i}\\right>  }{\\left< v_{i},v_{i}\\right>  }$, será llamada $i$-ésimo coeficiente de Fourier del vector $u$.\n",
    "\n",
    "**Ejemplo 2.8:** La base canónica de $\\mathbb{R}^{n}$ definida como $\\mathbf{e} \\left( n\\right)  =\\left\\{ \\mathbf{e}_{1} ,...,\\mathbf{e}_{n} \\right\\}$, donde $\\mathbf{e}_{k} =\\left( e_{1},...,e_{n}\\right)^{\\top }$, y\n",
    "\n",
    "$$e_{j}=\\begin{cases}1&;\\  \\mathrm{si} \\  j=k\\\\ 0&;\\  \\mathrm{si} \\  j\\neq k\\end{cases}$$\n",
    "<p style=\"text-align: right;\">$(2.24)$</p>\n",
    "\n",
    "es también una base ortogonal, ya que\n",
    "\n",
    "$$\\left< \\mathbf{e}_{i} ,\\mathbf{e}_{j} \\right>  =\\begin{cases}1&;\\  \\mathrm{si} \\  i=j\\\\ 0&;\\  \\mathrm{si} \\  i\\neq j\\end{cases}$$\n",
    "<p style=\"text-align: right;\">$(2.25)$</p>\n",
    "◼︎\n",
    "\n",
    "**Ejemplo 2.9:** Sea $V=\\left< \\left\\{ 1,\\mathrm{sen} \\left( x\\right)  ,\\cos \\left( x\\right)  ,\\mathrm{sen} \\left( 2x\\right)  ,\\cos \\left( 2x\\right)  ,...,\\mathrm{sen} \\left( nx\\right)  ,\\cos \\left( nx\\right)  \\right\\}  \\right>$ y definimos en $V$ el producto interno\n",
    "\n",
    "$$\\left< f,g\\right>  =\\int^{\\pi }_{-\\pi } f\\left( x\\right)  g\\left( x\\right)  dx$$\n",
    "<p style=\"text-align: right;\">$(2.26)$</p>\n",
    "\n",
    "Donde $f$ y $g$ son funciones integrables en el intervalo cerrado $[-\\pi, \\pi]$. Entonces $\\alpha =\\left\\{ 1,\\mathrm{sen} \\left( x\\right)  ,\\cos \\left( x\\right)  ,\\mathrm{sen} \\left( 2x\\right)  ,\\cos \\left( 2x\\right)  ,...,\\mathrm{sen} \\left( nx\\right)  ,\\cos \\left( nx\\right)  \\right\\}$ es una base ortogonal de $V$ respecto de dicho producto interno, ya que\n",
    "\n",
    "$$\\begin{array}{lll}\\left< \\mathrm{sen} \\left( px\\right)  ,\\cos \\left( qx\\right)  \\right>  &=&\\displaystyle \\int^{\\pi }_{-\\pi } \\mathrm{sen} \\left( px\\right)  \\cos \\left( qx\\right)  dx\\  ;\\  p\\neq q\\wedge p,q\\in \\mathbb{N} +\\left\\{ 0\\right\\}  \\\\ &=&\\displaystyle \\frac{1}{2} \\displaystyle \\int^{\\pi }_{-\\pi } \\left[ \\mathrm{sen} \\left( p+q\\right)  x+\\mathrm{sen} \\left( p-q\\right)  x\\right]  dx\\\\ &=&-\\displaystyle \\frac{1}{2} \\left( \\left[ \\displaystyle \\frac{1}{p+q} \\cos \\left( p+q\\right)  x\\right]^{x=\\pi }_{x=-\\pi }  +\\left[ \\displaystyle \\frac{1}{p-q} \\cos \\left( p-q\\right)  x\\right]^{x=\\pi }_{x=-\\pi }  \\right)  \\\\ &=&0\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(2.27)$</p>\n",
    "\n",
    "Mediante procedimientos similares, podemos demostrar además que $\\left< \\mathrm{sen} \\left( px\\right)  ,\\mathrm{sen} \\left( qx\\right)  \\right>  =\\left< \\cos \\left( px\\right)  ,\\cos \\left( qx\\right)  \\right>  =0$ para $p\\neq q$. Luego, efectivamente, $\\alpha$ es una base ortogonal de $V$ con respecto al producto interno (2.26). ◼︎"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "38b1568e",
   "metadata": {},
   "source": [
    "### Proceso de ortogonalización de Gram-Schmidt.\n",
    "Observamos que la fortaleza de las bases ortogonales, a la hora de determinar las componentes (coordenadas) de un vector, radica en el hecho de que los productos internos entre dichas componentes es cero, salvo que dicho producto se aplique sobre la misma componente (en cuyo caso obtenemos la norma inducida por este producto interno). Cabe preguntarse, por lo tanto: ¿Qué significa, en términos geométricos, el hecho de que $\\left< v_{i},v_{j}\\right>  =0;\\forall i\\neq j$? Para responder esta pregunta, observemos la situación en la Fig. (2.4) y, a partir de dicha representación, utilicemos el producto interno canónico de $\\mathbb{R}^{2}$. De esta manera, tenemos\n",
    "\n",
    "$$\\begin{array}{lll}\\left< \\mathbf{v}_{1} ,\\mathbf{v}_{2} \\right>  =0&\\Longleftrightarrow &x_{1}x_{2}+y_{1}y_{2}=0\\  ;\\  \\left( \\mathrm{sea} \\  l\\left( \\mathbf{v}_{i} \\right)  \\  \\mathrm{la} \\  \\mathrm{longitud} \\  \\mathrm{del} \\  \\mathrm{vector} \\  \\mathbf{v}_{i} \\right)  \\\\ &\\Longleftrightarrow &l\\left( \\mathbf{v}_{1} \\right)  \\cos \\left( \\omega_{1} \\right)  l\\left( \\mathbf{v}_{2} \\right)  \\cos \\left( \\omega_{2} \\right)  +l\\left( \\mathbf{v}_{1} \\right)  \\mathrm{sen} \\left( \\omega_{1} \\right)  l\\left( \\mathbf{v}_{2} \\right)  \\mathrm{sen} \\left( \\omega_{2} \\right)  =0\\\\ &\\Longleftrightarrow &l\\left( \\mathbf{v}_{1} \\right)  l\\left( \\mathbf{v}_{2} \\right)  \\cos \\left( \\omega_{2} -\\omega_{1} \\right)  =0\\\\ &\\Longleftrightarrow &l\\left( \\mathbf{v}_{1} \\right)  l\\left( \\mathbf{v}_{2} \\right)  \\cos \\left( \\omega \\right)  =0\\Longleftrightarrow \\omega =\\frac{\\pi }{2} \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(2.28)$</p>\n",
    "\n",
    "Así que la condición, en el plano $\\mathbb{R}^{2}$, para que los vectores $\\mathbf{v}_{1}$ y $\\mathbf{v}_{2}$ tengan un producto interno nulo (si tal producto interno es el canónico), es que sus trazas en $\\mathbb{R}^{2}$ (que son rectas) sean perpendiculares, que es precisamente lo que establecimos unas líneas atrás.\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"figures/fig_2_4.png\" width=\"300\"></p>\n",
    "<p style=\"text-align: center;\">Fig. (2.4): Relaciones angulares entre los vectores $\\mathbf{v}_{1}, \\mathbf{v}_{2}$ y el origen del plano $\\mathbb{R}^{2}$</p>\n",
    "\n",
    "Uno podría preguntarse, por supuesto, si dados los beneficios que traen consigo las bases ortogonales para la caracterización conveniente de cualquier vector en un espacio vectorial $V$, si éstas son sencillas de construir o si son abundantes, en caso de que no sean posibles de construir.\n",
    "\n",
    "Para responder las preguntas anteriores, podemos utilizar el resultado anterior para establecer la condición de que, si un espacio vectorial no tiene una base $v  \\subset V$, entonces $\\left< v_{i},v_{j}\\right>  \\neq 0$ para $i\\neq j$.\n",
    "\n",
    "Vamos a intentar trabajar sobre ésto, considerando –nuevamente– el espacio $\\mathbb{R}^{2}$, a fin de tener una noción intuitiva de donde queremos llegar. Entonces, si $\\left< \\mathbf{v}_{1},\\mathbf{v}_{2}\\right>  \\neq 0$, de acuerdo a la Fig. (2.4), sabemos que $\\mathbf{v}_{1}$ y $\\mathbf{v}_{2}$ no son perpendiculares. Así que, en este contexto, podemos suponer que estos vectores son como los que se muestran en la Fig. (2.5).\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"figures/fig_2_5.png\" width=\"800\"></p>\n",
    "<p style=\"text-align: center;\">Fig. (2.5): Los vectores $\\mathbf{v}_{1}$ y $\\mathbf{v}_{2}$ que se han asumido como no perpendiculares</p>\n",
    "\n",
    "Entonces,\n",
    "\n",
    "$$\\mathbf{v}_{2} =\\mathbf{v}^{\\prime }_{2} +a\\mathbf{v}_{1}$$\n",
    "<p style=\"text-align: right;\">$(2.29)$</p>\n",
    "\n",
    "Lamentablemente, la ecuación (2.29) contiene tres variables, y sólo conocemos una de ellas. Sin embargo, en virtud de las propiedades del producto interno, podemos escribir\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{v}_{2} =\\mathbf{v}^{\\prime }_{2} +a\\mathbf{v}_{1} &\\Longrightarrow &\\left< \\mathbf{v}_{2} ,\\mathbf{v}_{2} \\right>  =\\left< \\mathbf{v}^{\\prime }_{2} ,\\mathbf{v}_{1} \\right>  +\\left< a\\mathbf{v}_{1} ,\\mathbf{v}_{1} \\right>  \\\\ &\\Longrightarrow &\\left< \\mathbf{v}_{2} ,\\mathbf{v}_{1} \\right>  =a\\left< \\mathbf{v}_{1} ,\\mathbf{v}_{1} \\right>  \\\\ &\\Longrightarrow &a=\\displaystyle \\frac{\\left< \\mathbf{v}_{2} ,\\mathbf{v}_{1} \\right>  }{\\left< \\mathbf{v}_{1} ,\\mathbf{v}_{1} \\right>  } \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(2.30)$</p>\n",
    "\n",
    "Y sustituyendo en la ecuación (2.29), obtenemos\n",
    "\n",
    "$$\\mathbf{v}^{\\prime }_{2} =\\mathbf{v}_{2} -\\frac{\\left< \\mathbf{v}_{2} ,\\mathbf{v}_{1} \\right>  }{\\left< \\mathbf{v}_{1} ,\\mathbf{v}_{1} \\right>  } \\mathbf{v}_{1}$$\n",
    "<p style=\"text-align: right;\">$(2.31)$</p>\n",
    "\n",
    "De esta manera, hemos probado un caso particular del siguiente teorema.\n",
    "\n",
    "**<font color='crimson'>Teorema 2.2 – Proceso de ortogonalización de Gram-Schmidt:</font>** *Sea $V$ un $\\mathbb{K}$-espacio vectorial equipado con un producto interno y $\\alpha =\\left\\{ v_{1},...,v_{n}\\right\\}$ una base de $V$. Entonces $\\alpha^{\\prime } =\\left\\{ v^{\\prime }_{1},...,v^{\\prime }_{n}\\right\\}$ es una base ortogonal para $V$, donde los elementos de $\\alpha^{\\prime }$ satisfacen todos la ecuación vectorial*\n",
    "\n",
    "$$\\begin{cases}v^{\\prime }_{1}=v_{1}&\\\\ v^{\\prime }_{j}=v_{j}-\\displaystyle \\frac{\\left< v_{j},v^{\\prime }_{j-1}\\right>  }{\\left< v^{\\prime }_{j-1},v^{\\prime }_{j-1}\\right>  } v^{\\prime }_{j-1}-\\cdots -\\displaystyle \\frac{\\left< v_{j},v^{\\prime }_{1}\\right>  }{\\left< v^{\\prime }_{1},v^{\\prime }_{1}\\right>  } v^{\\prime }_{1}&;\\  \\left( 2\\leq j\\leq n\\right)  \\end{cases}$$\n",
    "<p style=\"text-align: right;\">$(2.32)$</p>\n",
    "◆\n",
    "\n",
    "**Ejemplo 2.10:** Consideremos el subespacio $W=\\left\\{ \\left( x,y,z,t\\right)  \\in \\mathbb{R}^{4} :x+y+z+t=0\\right\\}$. Vamos a determinar una base ortogonal para $W$ utilizando el producto interno canónico de $\\mathbb{R}^{4}$. De esta manera, en primer lugar, determinamos una base para $W$:\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{u} \\in W&\\Longleftrightarrow &\\mathbf{u} =\\left( x,y,z,t\\right)  \\in \\mathbb{R}^{4} \\wedge x+y+z+t=0\\\\ &\\Longleftrightarrow &\\mathbf{u} =\\left( x,y,z,t\\right)  \\in \\mathbb{R}^{4} \\wedge t=-x-y-z\\\\ &\\Longleftrightarrow &\\mathbf{u} =\\left( x,y,z,-x-y-z\\right)  ;\\left( x,y,z\\right)  \\in \\mathbb{R}^{3} \\\\ &\\Longleftrightarrow &\\mathbf{u} =x\\left( 1,0,0,-1\\right)  +y\\left( 0,1,0,-1\\right)  +z\\left( 0,0,1,-1\\right)  ;\\left( x,y,z\\right)  \\in \\mathbb{R}^{3} \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(2.32)$</p>\n",
    "\n",
    "Luego $W=\\left< \\left\\{ \\left( 1,0,0,-1\\right)  ,\\left( 0,1,0,-1\\right)  ,\\left( 0,0,1,-1\\right)  \\right\\}  \\right>$. Por lo tanto, $\\alpha =\\left\\{ \\left( 1,0,0,-1\\right)  ,\\left( 0,1,0,-1\\right)  ,\\left( 0,0,1,-1\\right)  \\right\\}$ es una base de $W$, ya que sus componentes son LI (esto se deja como ejercicio al lector). Ahora construiremos una base ortogonal siguiendo el proceso de Gram-Schmidt:\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{v}^{\\prime }_{1} &=&\\left( 1,0,0,-1\\right)  \\\\ \\mathbf{v}^{\\prime }_{2} &=&\\left( 0,1,0,-1\\right)  -\\displaystyle \\frac{\\left< \\left( 0,1,0,-1\\right)  ,\\left( 1,0,0,-1\\right)  \\right>  }{\\left< \\left( 1,0,0,-1\\right)  ,\\left( 1,0,0,-1\\right)  \\right>  } \\left( 1,0,0,-1\\right)  \\\\ &=&\\left( 0,1,0,-1\\right)  -\\displaystyle \\frac{1}{2} \\left( 1,0,0,-1\\right)  \\\\ &=&\\left( -\\displaystyle \\frac{1}{2} ,1,0,-\\displaystyle \\frac{1}{2} \\right)  ;\\mathrm{tomaremos} \\  \\mathbf{v}^{\\prime }_{2} =\\left( -1,2,0,-1\\right)  \\\\ \\mathbf{v}^{\\prime }_{3} &=&\\left( 0,1,0,-1\\right)  -\\displaystyle \\frac{\\left< \\left( 0,0,1,-1\\right)  ,\\left( -1,2,0,-1\\right)  \\right>  }{\\left< \\left( -1,2,0,-1\\right)  ,\\left( -1,2,0,-1\\right)  \\right>  } \\left( -1,2,0,-1\\right)  -\\displaystyle \\frac{\\left< \\left( 0,1,0,-1\\right)  ,\\left( 1,0,0,-1\\right)  \\right>  }{\\left< \\left( 1,0,0,-1\\right)  ,\\left( 1,0,0,-1\\right)  \\right>  } \\left( 1,0,0,-1\\right)  \\\\ &=&\\left( 0,1,0,-1\\right)  -\\displaystyle \\frac{1}{6} \\left( -1,2,0,-1\\right)  -\\displaystyle \\frac{1}{2} \\left( 1,0,0,-1\\right)  =\\left( -\\displaystyle \\frac{1}{3} ,-\\displaystyle \\frac{1}{3} ,1,-\\displaystyle \\frac{1}{3} \\right)  ;\\mathrm{tomaremos} \\  \\mathbf{v}^{\\prime }_{3} =\\left( -1,-1,3,-1\\right)  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(2.33)$</p>\n",
    "\n",
    "Así que $\\alpha^{\\prime } =\\left\\{ \\left( 1,0,0,-1\\right)  ,\\left( -1,2,0,-1\\right)  ,\\left( -1,-1,3,-1\\right)  \\right\\}$ es una base ortogonal de $W$. ◼︎\n",
    "\n",
    "Sea $\\alpha =\\left\\{ v_{1},...,v_{n}\\right\\}  \\subset V$ una base ortogonal de un $\\mathbb{K}$-espacio vectorial $V$. Para cada $u\\in V$ tenemos la representación única en términos de la base $\\alpha$\n",
    "\n",
    "$$u=\\sum^{n}_{i=1} \\frac{\\left< u,v_{i}\\right>  }{\\left< v_{i},v_{i}\\right>  } v_{i}$$\n",
    "<p style=\"text-align: right;\">$(2.34)$</p>\n",
    "\n",
    "Sin embargo, resulta interesante observar que el denominador del término común de esta sumatoria es, de hecho, el cuadrado de la norma inducida por el correspondiente producto interno (es decir, $\\left\\Vert v_{i}\\right\\Vert^{2}  =\\left< v_{i},v_{i}\\right>$). Podemos, por tanto, mejorar la definición de base ortogonal considerando el siguiente teorema.\n",
    "\n",
    "**<font color='crimson'>Teorema 2.3:</font>** *Sea $V$ un $\\mathbb{K}$-espacio vectorial y $\\alpha =\\left\\{ v_{1},...,v_{n}\\right\\}$ una base de $V$. Sea $\\beta =\\left\\{ v^{\\prime }_{1},...,v^{\\prime }_{n}\\right\\}$, donde $v^{\\prime }_{i}=v_{i}/\\left< v_{i},v_{i}\\right>$ para cada $i=1,...,n$. Entonces,*\n",
    "\n",
    "- **(T1):** $\\left< v^{\\prime }_{i},v^{\\prime }_{j}\\right>  =0\\Longleftrightarrow i\\neq j$.\n",
    "- **(T2):** $u=\\sum^{n}_{i=1} \\left< u,v_{i}\\right>  v^{\\prime }_{i}$.\n",
    "◆\n",
    "\n",
    "Lo anterior motiva la siguiente definición.\n",
    "\n",
    "**<font color='blue'>Definición 2.7 – Base ortonormal (general):</font>** Sea $V$ un $\\mathbb{K}$-espacio vectorial normado y sea $\\beta =\\left\\{ w_{1},...,w_{n}\\right\\}  \\subset V$. Diremos que $\\beta$ es una **base ortonormal** de $V$ si se cumplen con las siguientes condiciones:\n",
    "\n",
    "- **(C1):** $\\beta$ es una base ortogonal de $V$.\n",
    "- **(C2):** $\\left\\Vert v_{i}\\right\\Vert  =1;\\forall i,i=1,...,n$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
