{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "46f40333",
   "metadata": {},
   "source": [
    "# CLASE 1.2: Espacios vectoriales con producto interno\n",
    "---\n",
    "\n",
    "## Introducción.\n",
    "En la sección anterior, estudiamos los conceptos de vectores, espacios vectoriales y transformaciones lineales en términos generales, pero además, completamente abstractos. En esta sección, añadiremos a estos conceptos abstractos algunas interpretaciones geométricas a fin de construir un cierto nivel de intuición respecto de estos conceptos. En particular, visualizaremos vectores desde una perspectiva geométrica y calcularemos sus longitudes y distancias o ángulos con respecto a otros vectores. Para poder hacer esto, vamos a equipar a los espacios vectoriales con una operación especial conocida como producto interno, cuya propiedad fundamental será la inducción de la geometría relativa al espacio vectorial respectivo.\n",
    "\n",
    "Los productos internos y sus normas correspondientes y métricas nos permiten capturar las nociones intuitivas de distancia y similitud, las que resultan fundamentales en la construcción de uno de los modelos de machine learning más importantes que existen: Las máquinas de soporte vectorial (support vector machines, SVM). Luego, utilizaremos los conceptos de longitud y ángulo entre vectores para discutir las proyecciones ortogonales, las que jugarán un papel fundamental cuando estudiemos dos de los modelos de aprendizaje más elementales en machine learning: El análisis de componentes principales (que es un modelo de aprendizaje no supervisado) y el modelo de regresión lineal (que es un modelo de aprendizaje supervisado)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dff10b4c",
   "metadata": {},
   "source": [
    "## Producto interno.\n",
    "Cuando pensamos en vectores desde una perspectiva puramente geométrica; es decir, líneas dirigidas que parten desde el origen y terminan en un punto determinado, siempre hemos asociado a estos vectores el concepto de longitud del mismo en términos de la distancia entre el origen y dicho punto. A continuación, formalizaremos esta noción intuitiva mediante el concepto de **norma**.\n",
    "\n",
    "**<font color='blue'>Definición 2.1 – Norma:</font>** Sea $V$ un espacio vectorial que supondremos (sin pérdida de generalidad) definido sobre el cuerpo $\\mathbb{R}$. Para todo $v\\in V$ definimos la función\n",
    "\n",
    "$$\\begin{array}{ll}\\| \\  \\cdot \\  \\| :&V\\longrightarrow \\mathbb{R} \\\\ &v\\longrightarrow \\left\\Vert v\\right\\Vert  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(2.1)$</p>\n",
    "\n",
    "y que será llamada **norma**. Esta función asigna a $v\\in V$ su **longitud** $\\left\\Vert v\\right\\Vert\\in \\mathbb{R}$, y es tal que, para todo $\\lambda \\in \\mathbb{R}$ y para cualquier otro vector $u\\in V$, cumple con las siguientes propiedades:\n",
    "\n",
    "- **(P1) – Homogeneidad absoluta:** $\\left\\Vert \\lambda v\\right\\Vert  =\\left| \\lambda \\right|  \\left\\Vert v\\right\\Vert$.\n",
    "- **(P2) – Desigualdad triangular:** $\\left\\Vert u+v\\right\\Vert  \\leq \\left\\Vert u\\right\\Vert  +\\left\\Vert v\\right\\Vert$.\n",
    "- **(P3) – Definida positiva:** $\\left\\Vert v\\right\\Vert  \\geq 0\\wedge \\left\\Vert v\\right\\Vert  =0\\Longleftrightarrow v=O_{V}$.\n",
    "\n",
    "En términos geométricos, por ejemplo, la desigualdad triangular puede interpretarse por medio de vectores en $\\mathbb{R}^{n}$ para $n\\leq 3$, estableciendo que, para un triángulo cualquiera, la suma de sus longitudes de dos de sus lados debe ser mayor o igual que la longitud del lado restante (lo que se ilustra en la Fig. (2.1)). \n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"figures/fig_2_1.png\" width=\"400\"></p>\n",
    "<p style=\"text-align: center;\">Fig. (2.1): Una interpretación geométrica sencilla de la desigualdad triangular</p>\n",
    "\n",
    "La definición (2.1) es válida para cualquier espacio vectorial, pero, para efectos prácticos, bastará con que consideremos únicamente a aquellos con dimensión finita (y, puntualmente, nos limitaremos en muchos casos simplemente a $\\mathbb{R}^{n}$.\n",
    "\n",
    "**Ejemplo 2.1 – La norma $\\ell_{1}$:** La definición (2.1) establece las condiciones que debe cumplir una función (denotada como $\\left\\Vert \\cdot \\right\\Vert$) para ser considerada una norma sobre un determinado espacio vectorial (ya que opera con los elementos de dicho espacio). Por esa razón es que existen varios tipos de normas que son utilizadas en muchos campos de las matemáticas. Un ejemplo es la **norma $\\ell_{1}$**, llamada comúnmente **norma Manhattan**, que se define para cualquier vector $\\mathbf{x}\\in \\mathbb{R}^{n}$ (donde $\\mathbf{x}=(x_{1},...,x_{n})$) como\n",
    "\n",
    "$$\\left\\Vert \\mathbf{x} \\right\\Vert_{1}  :=\\sum^{n}_{i=1} \\left| x_{i}\\right|$$\n",
    "<p style=\"text-align: right;\">$(2.2)$</p>\n",
    "\n",
    "Donde $\\left| x_{i}\\right|$ es el valor absoluto de la *componente* $x_{i}$. En la Fig. (2.2a) se muestran todos los puntos en el plano $\\mathbb{R}^{2}$ tales que $\\left\\Vert \\mathbf{x} \\right\\Vert_{1}  =1$. ◼︎\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"figures/fig_2_2.png\" width=\"700\"></p>\n",
    "<p style=\"text-align: center;\">Fig. (2.2): (a) Todos los puntos $\\mathbf{x}$ en el plano tales que $\\left\\Vert \\mathbf{x} \\right\\Vert_{1}  =1$ ; (b) Todos los puntos $\\mathbf{x}$ en el plano tales que $\\left\\Vert \\mathbf{x} \\right\\Vert_{2}  =1$</p>\n",
    "\n",
    "**Ejemplo 2.2 – La norma $\\ell_{2}$:** Otro tipo de norma muy común en las matemáticas (y en el análisis en $\\mathbb{R}^{n}$) corresponde a la **norma $\\ell_{2}$:**, conocida igualmente como **norma Euclidiana**, la que se define para cualquier vector $\\mathbf{x}\\in \\mathbb{R}^{n}$ (donde $\\mathbf{x}=(x_{1},...,x_{n})$) como\n",
    "\n",
    "$$\\left\\Vert \\mathbf{x} \\right\\Vert_{2}  :=\\left( \\sum^{n}_{i=1} x^{2}_{i}\\right)^{\\frac{1}{2} }  =\\sqrt{\\mathbf{x}^{\\top } \\mathbf{x} }$$\n",
    "<p style=\"text-align: right;\">$(2.3)$</p>\n",
    "\n",
    "Esta norma permite calcular la distancia Euclidiana del vector $\\mathbf{x}\\in \\mathbb{R}^{n}$ con respecto al origen del sistema de coordenadas rectangulares. En la Fig. (2.2b) se muestran todos los puntos en el plano $\\mathbb{R}^{2}$ tales que $\\left\\Vert \\mathbf{x} \\right\\Vert_{2}  =1$. La norma $\\ell_{2}$ es una norma que usaremos a menudo durante esta asignatura, y será frecuente que la denotamos como la opción por defecto de la función de norma (poniendo simplemente $\\left\\Vert \\mathbf{x} \\right\\Vert$ en vez de $\\left\\Vert \\mathbf{x} \\right\\Vert_{2}$), salvo que especifiquemos lo contrario. ◼︎\n",
    "\n",
    "La norma es un caso particular de una operación importante en álgebra conocida como **producto interno**, y que definiremos a continuación.\n",
    "\n",
    "**<font color='blue'>Definición 2.2 – Producto interno (general):</font>** Sea $V$ un $\\mathbb{K}$-espacio vectorial. Diremos que la función\n",
    "\n",
    "$$\\begin{array}{ll}\\left< \\  ,\\  \\right>  :&V\\times V\\longmapsto \\mathbb{K} \\\\ &\\left( u,v\\right)  \\longmapsto \\left< u,v\\right>  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(2.4)$</p>\n",
    "\n",
    "es llamada **producto interno** definido sobre $V$. Esta función cumple con las siguientes propiedades:\n",
    "\n",
    "- **(P1):** $\\left< v,v\\right>  \\geq O_{\\mathbb{K} };\\forall v\\in V\\wedge \\left< v,v\\right>  =O_{\\mathbb{K} }\\Longleftrightarrow v=O_{V}$.\n",
    "- **(P2):** $\\left< u+v,w\\right>  =\\left< u,w\\right>  +\\left< v,w\\right>  ;\\forall u,v,w\\in V$.\n",
    "- **(P3):** $\\left< u,v+w\\right>  =\\left< u,v\\right>  +\\left< v,w\\right>  ;\\forall u,v,w\\in V$.\n",
    "- **(P4):** $\\left< \\lambda u,v\\right>  =\\lambda \\left< u,v\\right>  ;\\forall u,v\\in V\\wedge \\lambda \\in \\mathbb{K}$.\n",
    "- **(P5):** $\\left< u,\\lambda v\\right>  =\\bar{\\lambda } \\left< u,v\\right>  ;\\forall u,v\\in V\\wedge \\lambda \\in \\mathbb{K}$.\n",
    "- **(P6):** $\\left< u,v\\right>  =\\overline{\\left< v,u\\right>  } ;\\forall u,v\\in V$.\n",
    "\n",
    "Cuando un $\\mathbb{K}$-espacio vectorial $V$ está *equipado* con un producto interno, se denomina **espacio vectorial normado o prehilbertiano**.\n",
    "\n",
    "**Ejemplo 2.3:** En $\\mathbb{K}^{n}$ (donde $\\mathbb{K}$ puede ser $\\mathbb{R}$ o $\\mathbb{C}$), definimos, para $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{K}^{n}$, con $\\mathbf{x}=(x_{1},...,x_{n})$ y $\\mathbf{y}=(y_{1},...,y_{n})$,\n",
    "\n",
    "$$\\left< \\mathbf{x} ,\\mathbf{y} \\right>  =\\sum^{n}_{k=1} x_{k}\\overline{y}_{k}$$\n",
    "<p style=\"text-align: right;\">$(2.5)$</p>\n",
    "\n",
    "y lo denominaremos **producto interno canónico en $\\mathbb{K}^{n}$**. Notemos que, cuando $\\mathbb{K}=\\mathbb{R}$, se tiene que $\\overline{y}_{k}=y_{k}$ para todo $k=1,...,n$. En un contexto más geométrico, donde estos vectores suelen describir cantidades físicas, tal producto interno suele denominarse como *producto punto*. ◼︎\n",
    "\n",
    "**Ejemplo 2.4:** En $\\mathbb{R}^{n\\times n}$ definimos, para $\\mathbf{A} =\\left\\{ a_{ij}\\right\\}  \\in \\mathbb{R}^{n\\times n} \\wedge \\mathbf{B} =\\left\\{ b_{ij}\\right\\}  \\in \\mathbb{R}^{n\\times n}$,\n",
    "\n",
    "$$\\left< \\mathbf{A} ,\\mathbf{B} \\right>  =\\mathrm{tr} \\left( \\mathbf{B}^{\\top } \\mathbf{A} \\right)$$\n",
    "<p style=\"text-align: right;\">$(2.6)$</p>\n",
    "\n",
    "y lo denominaremos **producto interno canónico de $\\mathbb{R}^{n\\times n}$**. ◼︎\n",
    "\n",
    "**Ejemplo 2.5:** Definimos el conjunto $C^{k}([a,b])$ como el conjunto de todas las funciones $k$ veces diferenciables sobre el intervalo cerrado $[a,b]$ (es decir, funciones de clase $C^{k}$ en el intervalo cerrado $[a,b]\\in \\mathbb{R}$). La operación definida como\n",
    "\n",
    "$$\\left< f,g\\right>  =\\left< f\\left( x\\right)  ,g\\left( x\\right)  \\right>  =\\int^{b}_{a} f\\left( x\\right)  g\\left( x\\right)  dx$$\n",
    "<p style=\"text-align: right;\">$(2.7)$</p>\n",
    "\n",
    "es llamada **producto interno de las funciones $f$ y $g$ para cada $x\\in [a,b]$**. ◼︎"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9efeaa6b",
   "metadata": {},
   "source": [
    "## Longitud y distancia.\n",
    "La norma, en general, corresponde a un caso particular de aplicación del producto interno, en el cual el argumento respectivo es siempre el mismo vector. Por esta razón, decimos que un producto interno sobre un espacio vectorial $V$ siempre induce una norma en $V$. En este caso, podemos re-definir la norma en relación a cualquier producto interno, ya que, para todo $v\\in V$, se tendrá que\n",
    "\n",
    "$$\\left\\Vert v\\right\\Vert  :=\\sqrt{\\left< v,v\\right>  }$$\n",
    "<p style=\"text-align: right;\">$(2.8)$</p>\n",
    "\n",
    "La norma corresponde a un concepto que, por tanto, se desprende de forma natural para cualquier espacio vectorial con producto interno. Sin embargo, no todas las normas son inducidas a partir de un producto interno; la norma $\\ell_{1}$ es un ejemplo de norma que no se corresponde con un producto interno y que resulta importante en procedimientos fundamentales propios de muchos algoritmos de machine learning tales como la regularización de hiperparámetros (donde, mediante un procedimiento iterativo, intentamos evitar que nuestros modelos sobreajusten o aprendan de memoria un patrón extremadamente variable dado un conjunto de datos que deseamos representar). Sin embargo, para definir conceptos geométricos claves, como longitudes, distancias y ángulos, nos limitaremos momentáneamente al uso de normas inducidas. Para ello, partiremos con un importante teorema, que generaliza la desigualdad triangular vista en la definición (2.1).\n",
    "\n",
    "**<font color='crimson'>Teorema 2.1 – Desigualdad de Cauchy-Schwarz:</font>** *Sea $V$ un espacio vectorial normado. Para todo par de vectores $u,v\\in V$ se tiene que*\n",
    "\n",
    "$$\\left| \\left< u,v\\right>  \\right|^{2}  \\leq \\left< u,u\\right>  \\left< v,v\\right>  \\Longleftrightarrow \\left| \\left< u,v\\right>  \\right|^{2}  \\leq \\left\\Vert u\\right\\Vert  \\left\\Vert v\\right\\Vert$$\n",
    "<p style=\"text-align: right;\">$(2.9)$</p>\n",
    "◆ \n",
    "\n",
    "**Ejemplo 2.6:** En el campo de la geometría analítica, con frecuencia, estamos interesados en la longitud de un vector. Podemos utilizar el producto interno para calcular tales longitudes por medio de la ecuación (2.8). Por ejemplo, consideremos el vector $\\mathbf{u}=(1, 1)^{\\top}\\in \\mathbb{R}^{2}$. En este caso, a partir de la definición de norma inducida y, en este caso, utilizando la norma $\\ell_{2}$ (que es, de hecho, el producto interno definido en el ejemplo (2.3)), obenemos\n",
    "\n",
    "$$\\left\\Vert \\mathbf{u} \\right\\Vert  =\\sqrt{\\left< \\mathbf{u} ,\\mathbf{u} \\right>  } =\\sqrt{1^{2}+1^{2}} =\\sqrt{2}$$\n",
    "<p style=\"text-align: right;\">$(2.10)$</p>\n",
    "\n",
    "y que corresponde a la longitud del vector $\\mathbf{u}$. Por otro lado, es posible demostrar que la expresión\n",
    "\n",
    "$$\\mathbf{u}^{\\top } \\mathbf{A} \\mathbf{v} ;\\forall \\mathbf{u} ,\\mathbf{v} \\in V\\wedge \\mathbf{A} \\in \\mathbb{R}^{n\\times n}$$\n",
    "<p style=\"text-align: right;\">$(2.11)$</p>\n",
    "\n",
    "donde $V$ es un espacio vectorial normado, es de hecho un producto interno siempre que la matriz $\\mathbf{A} =\\left\\{ a_{ij}\\right\\}$ sea definida positiva; es decir, si las submatrices $\\tilde{\\mathbf{A} } =\\left\\{ \\tilde{a}_{ij} \\right\\}$, con $i=1,...,n-r$ y $j=1,...,n-r$, para $r = 1,...,n-1$, son todas invertibles. Una matriz que cumple con este criterio es\n",
    "\n",
    "$$\\mathbf{A} =\\left( \\begin{array}{rr}1&-\\frac{1}{2} \\\\ -\\frac{1}{2} &1\\end{array} \\right)$$\n",
    "<p style=\"text-align: right;\">$(2.12)$</p>\n",
    "\n",
    "El producto interno (2.11) induce la norma $\\| \\mathbf{u} \\| =\\mathbf{u}^{\\top } \\mathbf{A} \\mathbf{u}$ para todo $\\mathbf{u}\\in V$. Con esta norma, obtenemos $\\| \\mathbf{u} \\|=\\sqrt{1}=1$. Por lo tanto, la longitud del vector $\\mathbf{u}$ será dependiente de la norma con la cual se defina. Esto abre la posibilidad de pensar en que la geometría no necesariamente tiene que ser Euclidiana, ya que los conceptos de distancia, como veremos un poco más adelante, dependerán de la métrica con la cual equipemos al espacio donde estamos trabajando. ◼︎\n",
    "\n",
    "**<font color='blue'>Definición 2.3 – Distancia y métrica:</font>** Sea $V$ un espacio vectorial normado y sean $u,v\\in V$. Definimos la **distancia** entre los vectores $u$ y $v$ como\n",
    "\n",
    "$$d\\left( u,v\\right)  :=\\left\\Vert u-v\\right\\Vert  =\\sqrt{\\left< u-v,u-v\\right>  }$$\n",
    "<p style=\"text-align: right;\">$(2.13)$</p>\n",
    "\n",
    "Si $V=\\mathbb{R}^{n}$, y utilizamos la norma $\\ell_{2}$, la distancia así definida será llamada **distancia Euclidiana** entre los vectores respectivos. Por otro lado, la función $d$ definida como\n",
    "\n",
    "$$\\begin{array}{ll}d:&V\\times V\\longmapsto \\mathbb{R} \\\\ &\\left( u,v\\right)  \\longmapsto d\\left( u,v\\right)  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(2.14)$</p>\n",
    "\n",
    "será llamada **métrica** del espacio vectorial $V$.\n",
    "\n",
    "De manera similar al concepto de longitud de un vector, la distancia entre dos vectores no requiere de un producto interno para ser definida. Bastará siempre con el concepto de norma, independiente de si ésta fue inducida por un producto interno o no. En cualquier caso, si la norma sí es inducida, el valor de la distancia variará en función del producto interno utilizado.\n",
    "\n",
    "Una métrica $d$ satisface las siguiente condiciones:\n",
    "\n",
    "- **(C1):** La métrica $d$ es **definida positiva**. Es decir, $d(u,v)\\geq 0$ para todo $u,v\\in V$, siendo $d(u,v)=0 \\Longleftrightarrow u=v$.\n",
    "- **(C2):** La métrica $d$ es **simétrica**. Es decir, $d(u,v)=d(v,u); \\forall u,v\\in V$.\n",
    "- **(C3):** La métrica $d$ satisface la **desigualdad triangular**. Es decir, $d(u,w)\\leq d(u,v)+d(v,w); \\forall u,v,w \\in V$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c49d6bcc",
   "metadata": {},
   "source": [
    "## Ortogonalidad.\n",
    "\n",
    "### Ángulo entre vectores.\n",
    "En adición a la posibilidad de definir longitudes de vectores y la distancia entre ellos, los productos internos (y, puntualmente, las normas) también permiten capturar la noción de geometría de un espacio vectorial mediante la definición del *ángulo* $\\omega$ entre dos vectores. Este concepto tiene una interpretación geométrica que resulta natural cuando $V=\\mathbb{R}^{2}$ o $V=\\mathbb{R}^{3}$ y, para definirla, utilizaremos la desigualdad de Cauchy-Schwarz.\n",
    "\n",
    "**<font color='blue'>Definición 2.4 – Ángulo (entre dos vectores):</font>** Sea $V$ un $\\mathbb{K}$-espacio vectorial normado y $u,v\\in V$ tales que $u\\neq O_{V}$ y $v\\neq O_{V}$. Conforme la desigualdad de Cauchy-Schwarz, es posible establecer que\n",
    "\n",
    "$$-1\\leq \\frac{\\left< u,v\\right>  }{\\left\\Vert u\\right\\Vert  \\left\\Vert v\\right\\Vert  } \\leq 1$$\n",
    "<p style=\"text-align: right;\">$(2.15)$</p>\n",
    "\n",
    "Definimos el **ángulo** entre los vectores $u$ y $v$ como el único valor $\\omega$ que satisface la ecuación\n",
    "\n",
    "$$\\cos \\left( \\omega \\right)  =\\frac{\\left< u,v\\right>  }{\\left\\Vert u\\right\\Vert  \\left\\Vert v\\right\\Vert  }$$\n",
    "<p style=\"text-align: right;\">$(2.16)$</p>\n",
    "\n",
    "Intuitivamente, cuando $V=\\mathbb{R}^{2}$ o $V=\\mathbb{R}^{3}$, el concepto de ángulo nos permite entender qué tan similares son las orientaciones de los vectores $\\mathbf{u}$ y $\\mathbf{v}$, cuando $\\mathbf{u}, \\mathbf{v}\\in \\mathbb{R}^{n}$ ($n=2, 3$). Cuando $v$ es un vector arbitrario, para todo espacio vectorial abstracto, el concepto de ángulo es más general y sirve como base para construir elementos más representativos de la geometría de dicho espacio. Volveremos a retomar estos conceptos más adelante, cuando estudiemos las *bases ortogonales*.\n",
    "\n",
    "**Ejemplo 2.7:** Vamos a calcular el ángulo entre los vectores $\\mathbf{x}=(1, 1)^{\\top}\\in \\mathbb{R}^{2}$ e $\\mathbf{y}=(1,2)^{\\top}\\in \\mathbb{R}^{2}$, los que se ilustran en la Fig. (2.3). Utilizando la norma $\\ell_{2}$ y el producto interno canónico de $\\mathbb{R}^{2}$, obtenemos\n",
    "\n",
    "$$\\cos \\left( \\omega \\right)  =\\frac{\\left< \\mathbf{x} ,\\mathbf{y} \\right>  }{\\left\\Vert \\mathbf{x} \\right\\Vert  \\left\\Vert \\mathbf{y} \\right\\Vert  } =\\frac{3}{\\sqrt{10} } \\Longleftrightarrow \\omega =\\arccos \\left( \\frac{3}{\\sqrt{10} } \\right)  \\approx 0.23\\  \\mathrm{rad}$$\n",
    "<p style=\"text-align: right;\">$(2.17)$</p>\n",
    "\n",
    "Así que el ángulo $\\omega$ entre los vectores $\\mathbf{x}$ e $\\mathbf{y}$ es igual a 0.23 radianes, equivalente a unos 18º aproximadamente. ◼︎\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"figures/fig_2_3.png\" width=\"150\"></p>\n",
    "<p style=\"text-align: center;\">Fig. (2.3): El ángulo $\\omega$ entre los vectores $\\mathbf{x}$ e $\\mathbf{y}$</p>\n",
    "\n",
    "Un último concepto clave en relación a la caracterización de espacios vectoriales corresponde al concepto de ortogonalidad, el cual también, como sabemos, admite una interpretación geométrica. En esta primera aproximación, construiremos este concepto aprovechando la definición de ángulo vista previamente, pero más adelante lo formalizaremos cuando estudiemos las *bases ortogonales*.\n",
    "\n",
    "**<font color='blue'>Definición 2.5 – Ortogonalidad:</font>** Sea $V$ un espacio vectorial normado y sean $u,v\\in V$. Diremos que los vectores $u$ y $v$ son **ortogonales** si y sólo si $\\left< u,v\\right>  =O_{V}$. En tal caso, escribiremos $u\\bot v$ para denotar la condición de ortogonalidad entre ambos vectores.\n",
    "\n",
    "Notemos que también podemos establecer que dos vectores son ortogonales si el ángulo $\\omega$ entre ellos es tal que $\\cos(\\omega)=0$.\n",
    "\n",
    "Debemos observar que dos vectores son ortogonales siempre respecto de una determinada definición de producto interno."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e47154a4",
   "metadata": {},
   "source": [
    "### Bases ortogonales.\n",
    "Sea $\\alpha =\\left( \\mathbf{v}_{1} ,\\mathbf{v}_{2} \\right)$ una base ordenada de $\\mathbb{R}^{2}$ tal que $\\left< \\mathbf{v}_{1} ,\\mathbf{v}_{2} \\right>  =0$. Entonces, como $\\alpha$ es una base de $\\mathbb{R}^{2}$, para $\\mathbf{v}\\in \\mathbb{R}^{2}$, existen escalares únicos $a_{1}, a_{2}\\in \\mathbb{R}$ tales que $\\mathbf{v}=a_{1}\\mathbf{v}_{1} +a_{2}\\mathbf{v}_{2}$. Como $\\left< \\mathbf{v}_{1} ,\\mathbf{v}_{2} \\right>  =0$, entonces podemos *multiplicar* la combinación lineal que genera $\\mathbf{v}$ (usando el producto interno) por $\\mathbf{v}_{1}$, para obtener\n",
    "\n",
    "$$\\left< \\mathbf{v} ,\\mathbf{v}_{1} \\right>  =\\left< a_{1}\\mathbf{v}_{1} +a_{2}\\mathbf{v}_{2} ,\\mathbf{v}_{1} \\right>  =a_{1}\\left< \\mathbf{v}_{1} ,\\mathbf{v}_{1} \\right>  +a_{2}\\underbrace{\\left< \\mathbf{v}_{2} ,\\mathbf{v}_{1} \\right>  }_{=0} =a_{1}\\left< \\mathbf{v}_{1} ,\\mathbf{v}_{1} \\right>$$\n",
    "<p style=\"text-align: right;\">$(2.18)$</p>\n",
    "\n",
    "De donde se tiene que\n",
    "\n",
    "$$a_{1}=\\frac{\\left< \\mathbf{v} ,\\mathbf{v}_{1} \\right>  }{\\left< \\mathbf{v}_{1} ,\\mathbf{v}_{1} \\right>  }$$\n",
    "<p style=\"text-align: right;\">$(2.19)$</p>\n",
    "\n",
    "Notemos que, como $\\mathbf{v}_{1}$ es un vector no nulo, se tiene que $\\left< \\mathbf{v}_{1} ,\\mathbf{v}_{1} \\right> > 0$. Análogamente, siguiendo un procedimiento similar, podemos obtener que\n",
    "\n",
    "$$a_{2}=\\frac{\\left< \\mathbf{v} ,\\mathbf{v}_{2} \\right>  }{\\left< \\mathbf{v}_{2} ,\\mathbf{v}_{2} \\right>  }$$\n",
    "<p style=\"text-align: right;\">$(2.20)$</p>\n",
    "\n",
    "Hemos demostrado pues que, si $\\alpha =(\\mathbf{v}_{1},\\mathbf{v}_{2})$ es una base ordenada de $\\mathbb{R}^{2}$ tal que $\\left< \\mathbf{v}_{1} ,\\mathbf{v}_{2} \\right>  =0$, entonces el vector $\\mathbf{v}\\in \\mathbb{R}^{2}$ puede expresarse como una combinación lineal del tipo\n",
    "\n",
    "$$\\mathbf{v} =\\frac{\\left< \\mathbf{v} ,\\mathbf{v}_{1} \\right>  }{\\left< \\mathbf{v}_{1} ,\\mathbf{v}_{1} \\right>  } \\mathbf{v}_{1} +\\frac{\\left< \\mathbf{v} ,\\mathbf{v}_{2} \\right>  }{\\left< \\mathbf{v}_{2} ,\\mathbf{v}_{2} \\right>  } \\mathbf{v}_{2}$$\n",
    "<p style=\"text-align: right;\">$(2.21)$</p>\n",
    "\n",
    "Vamos a intentar generalizar el resultado anterior para un vector en $\\mathbb{R}^{n}$. Supongamos entonces que $\\alpha = (\\mathbf{v}_{1},...,\\mathbf{v}_{n})$ es una base ordenada de $\\mathbb{R}^{n}$ tal que $\\left< \\mathbf{v}_{i} ,\\mathbf{v}_{j} \\right>  =0$ para $i\\neq j$. Entonces, a partir del hecho de que $\\alpha$ es una base de $\\mathbb{R}^{n}$, se tiene que existe una colección de escalares $\\left\\{ a_{i}\\right\\}^{n}_{i=1}$ tales que, para todo $\\mathbf{u}\\in \\mathbb{R}^{n}$, se tendrá que $\\mathbf{u} =\\sum^{n}_{i=1} a_{i}\\mathbf{v}_{i}$. Por lo tanto, podemos escribir\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{u} =\\displaystyle \\sum^{n}_{i=1} a_{i}\\mathbf{v}_{i} &\\Longrightarrow &\\left< \\mathbf{u} ,\\mathbf{v}_{j} \\right>  =\\left< \\displaystyle \\sum^{n}_{i=1} a_{i}\\mathbf{v}_{i} ,\\mathbf{v}_{j} \\right>  =\\displaystyle \\sum^{n}_{i=1} a_{i}\\overbrace{\\left< \\mathbf{v}_{i} ,\\mathbf{v}_{j} \\right>  }^{=0\\Longleftrightarrow i\\neq j} \\\\ &\\Longrightarrow &\\left< \\mathbf{u} ,\\mathbf{v}_{j} \\right>  =a_{j}\\left< \\mathbf{v}_{j} ,\\mathbf{v}_{j} \\right>  \\\\ &\\Longrightarrow &a_{j}=\\displaystyle \\frac{\\left< \\mathbf{u} ,\\mathbf{v}_{j} \\right>  }{\\left< \\mathbf{v}_{j} ,\\mathbf{v}_{j} \\right>  } ;j=1,...,n\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(2.22)$</p>\n",
    "\n",
    "Hemos demostrado, pues, el siguiente resultado fundamental:\n",
    "\n",
    "$$\\mathbf{u} =\\sum^{n}_{i=1} \\frac{\\left< \\mathbf{u} ,\\mathbf{v}_{i} \\right>  }{\\left< \\mathbf{v}_{i} ,\\mathbf{v}_{i} \\right>  } \\mathbf{v}_{i} \\Longleftrightarrow \\left[ \\mathbf{u} \\right]_{\\alpha }  =\\left( \\begin{array}{c}\\displaystyle \\frac{\\left< \\mathbf{u} ,\\mathbf{v}_{1} \\right>  }{\\left< \\mathbf{v}_{1} ,\\mathbf{v}_{1} \\right>  } \\\\ \\displaystyle \\frac{\\left< \\mathbf{u} ,\\mathbf{v}_{2} \\right>  }{\\left< \\mathbf{v}_{2} ,\\mathbf{v}_{2} \\right>  } \\\\ \\vdots \\\\ \\displaystyle \\frac{\\left< \\mathbf{u} ,\\mathbf{v}_{n} \\right>  }{\\left< \\mathbf{v}_{n} ,\\mathbf{v}_{n} \\right>  } \\end{array} \\right)$$\n",
    "<p style=\"text-align: right;\">$(2.23)$</p>\n",
    "\n",
    "Este desarrollo motiva la siguiente definición.\n",
    "\n",
    "**<font color='blue'>Definición 2.6 – Base ortogonal (general):</font>** Sea $V$ un $\\mathbb{K}$-espacio vectorial normado. Una base ordenada $\\alpha=(v_{1},...,v_{n})\\subset V$ será llamada **base ortogonal** de $V$ si se cumplen las siguientes condiciones:\n",
    "\n",
    "- **(C1):** $\\alpha$ es una base de $V$.\n",
    "- **(C2):** Si $i\\neq j$, entonces $\\left< v_{i},v_{j}\\right>  =0$.\n",
    "\n",
    "Para un vector arbitrario $u\\in V$, la **coordenada** respecto de la base ortogonal $\\alpha$, digamos $a_{i}=\\frac{\\left< u,v_{i}\\right>  }{\\left< v_{i},v_{i}\\right>  }$, será llamada $i$-ésimo coeficiente de Fourier del vector $u$.\n",
    "\n",
    "**Ejemplo 2.8:** La base canónica de $\\mathbb{R}^{n}$ definida como $\\mathbf{e} \\left( n\\right)  =\\left\\{ \\mathbf{e}_{1} ,...,\\mathbf{e}_{n} \\right\\}$, donde $\\mathbf{e}_{k} =\\left( e_{1},...,e_{n}\\right)^{\\top }$, y\n",
    "\n",
    "$$e_{j}=\\begin{cases}1&;\\  \\mathrm{si} \\  j=k\\\\ 0&;\\  \\mathrm{si} \\  j\\neq k\\end{cases}$$\n",
    "<p style=\"text-align: right;\">$(2.24)$</p>\n",
    "\n",
    "es también una base ortogonal, ya que\n",
    "\n",
    "$$\\left< \\mathbf{e}_{i} ,\\mathbf{e}_{j} \\right>  =\\begin{cases}1&;\\  \\mathrm{si} \\  i=j\\\\ 0&;\\  \\mathrm{si} \\  i\\neq j\\end{cases}$$\n",
    "<p style=\"text-align: right;\">$(2.25)$</p>\n",
    "◼︎\n",
    "\n",
    "**Ejemplo 2.9:** Sea $V=\\left< \\left\\{ 1,\\mathrm{sen} \\left( x\\right)  ,\\cos \\left( x\\right)  ,\\mathrm{sen} \\left( 2x\\right)  ,\\cos \\left( 2x\\right)  ,...,\\mathrm{sen} \\left( nx\\right)  ,\\cos \\left( nx\\right)  \\right\\}  \\right>$ y definimos en $V$ el producto interno\n",
    "\n",
    "$$\\left< f,g\\right>  =\\int^{\\pi }_{-\\pi } f\\left( x\\right)  g\\left( x\\right)  dx$$\n",
    "<p style=\"text-align: right;\">$(2.26)$</p>\n",
    "\n",
    "Donde $f$ y $g$ son funciones integrables en el intervalo cerrado $[-\\pi, \\pi]$. Entonces $\\alpha =\\left\\{ 1,\\mathrm{sen} \\left( x\\right)  ,\\cos \\left( x\\right)  ,\\mathrm{sen} \\left( 2x\\right)  ,\\cos \\left( 2x\\right)  ,...,\\mathrm{sen} \\left( nx\\right)  ,\\cos \\left( nx\\right)  \\right\\}$ es una base ortogonal de $V$ respecto de dicho producto interno, ya que\n",
    "\n",
    "$$\\begin{array}{lll}\\left< \\mathrm{sen} \\left( px\\right)  ,\\cos \\left( qx\\right)  \\right>  &=&\\displaystyle \\int^{\\pi }_{-\\pi } \\mathrm{sen} \\left( px\\right)  \\cos \\left( qx\\right)  dx\\  ;\\  p\\neq q\\wedge p,q\\in \\mathbb{N} +\\left\\{ 0\\right\\}  \\\\ &=&\\displaystyle \\frac{1}{2} \\displaystyle \\int^{\\pi }_{-\\pi } \\left[ \\mathrm{sen} \\left( p+q\\right)  x+\\mathrm{sen} \\left( p-q\\right)  x\\right]  dx\\\\ &=&-\\displaystyle \\frac{1}{2} \\left( \\left[ \\displaystyle \\frac{1}{p+q} \\cos \\left( p+q\\right)  x\\right]^{x=\\pi }_{x=-\\pi }  +\\left[ \\displaystyle \\frac{1}{p-q} \\cos \\left( p-q\\right)  x\\right]^{x=\\pi }_{x=-\\pi }  \\right)  \\\\ &=&0\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(2.27)$</p>\n",
    "\n",
    "Mediante procedimientos similares, podemos demostrar además que $\\left< \\mathrm{sen} \\left( px\\right)  ,\\mathrm{sen} \\left( qx\\right)  \\right>  =\\left< \\cos \\left( px\\right)  ,\\cos \\left( qx\\right)  \\right>  =0$ para $p\\neq q$. Luego, efectivamente, $\\alpha$ es una base ortogonal de $V$ con respecto al producto interno (2.26). ◼︎"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1341661a",
   "metadata": {},
   "source": [
    "### Proceso de ortogonalización de Gram-Schmidt.\n",
    "Observamos que la fortaleza de las bases ortogonales, a la hora de determinar las componentes (coordenadas) de un vector, radica en el hecho de que los productos internos entre dichas componentes es cero, salvo que dicho producto se aplique sobre la misma componente (en cuyo caso obtenemos la norma inducida por este producto interno). Cabe preguntarse, por lo tanto: ¿Qué significa, en términos geométricos, el hecho de que $\\left< v_{i},v_{j}\\right>  =0;\\forall i\\neq j$? Para responder esta pregunta, observemos la situación en la Fig. (2.4) y, a partir de dicha representación, utilicemos el producto interno canónico de $\\mathbb{R}^{2}$. De esta manera, tenemos\n",
    "\n",
    "$$\\begin{array}{lll}\\left< \\mathbf{v}_{1} ,\\mathbf{v}_{2} \\right>  =0&\\Longleftrightarrow &x_{1}x_{2}+y_{1}y_{2}=0\\  ;\\  \\left( \\mathrm{sea} \\  l\\left( \\mathbf{v}_{i} \\right)  \\  \\mathrm{la} \\  \\mathrm{longitud} \\  \\mathrm{del} \\  \\mathrm{vector} \\  \\mathbf{v}_{i} \\right)  \\\\ &\\Longleftrightarrow &l\\left( \\mathbf{v}_{1} \\right)  \\cos \\left( \\omega_{1} \\right)  l\\left( \\mathbf{v}_{2} \\right)  \\cos \\left( \\omega_{2} \\right)  +l\\left( \\mathbf{v}_{1} \\right)  \\mathrm{sen} \\left( \\omega_{1} \\right)  l\\left( \\mathbf{v}_{2} \\right)  \\mathrm{sen} \\left( \\omega_{2} \\right)  =0\\\\ &\\Longleftrightarrow &l\\left( \\mathbf{v}_{1} \\right)  l\\left( \\mathbf{v}_{2} \\right)  \\cos \\left( \\omega_{2} -\\omega_{1} \\right)  =0\\\\ &\\Longleftrightarrow &l\\left( \\mathbf{v}_{1} \\right)  l\\left( \\mathbf{v}_{2} \\right)  \\cos \\left( \\omega \\right)  =0\\Longleftrightarrow \\omega =\\frac{\\pi }{2} \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(2.28)$</p>\n",
    "\n",
    "Así que la condición, en el plano $\\mathbb{R}^{2}$, para que los vectores $\\mathbf{v}_{1}$ y $\\mathbf{v}_{2}$ tengan un producto interno nulo (si tal producto interno es el canónico), es que sus trazas en $\\mathbb{R}^{2}$ (que son rectas) sean perpendiculares, que es precisamente lo que establecimos unas líneas atrás.\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"figures/fig_2_4.png\" width=\"300\"></p>\n",
    "<p style=\"text-align: center;\">Fig. (2.4): Relaciones angulares entre los vectores $\\mathbf{v}_{1}, \\mathbf{v}_{2}$ y el origen del plano $\\mathbb{R}^{2}$</p>\n",
    "\n",
    "Uno podría preguntarse, por supuesto, si dados los beneficios que traen consigo las bases ortogonales para la caracterización conveniente de cualquier vector en un espacio vectorial $V$, si éstas son sencillas de construir o si son abundantes, en caso de que no sean posibles de construir.\n",
    "\n",
    "Para responder las preguntas anteriores, podemos utilizar el resultado anterior para establecer la condición de que, si un espacio vectorial no tiene una base $v  \\subset V$, entonces $\\left< v_{i},v_{j}\\right>  \\neq 0$ para $i\\neq j$.\n",
    "\n",
    "Vamos a intentar trabajar sobre ésto, considerando –nuevamente– el espacio $\\mathbb{R}^{2}$, a fin de tener una noción intuitiva de donde queremos llegar. Entonces, si $\\left< \\mathbf{v}_{1},\\mathbf{v}_{2}\\right>  \\neq 0$, de acuerdo a la Fig. (2.4), sabemos que $\\mathbf{v}_{1}$ y $\\mathbf{v}_{2}$ no son perpendiculares. Así que, en este contexto, podemos suponer que estos vectores son como los que se muestran en la Fig. (2.5).\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"figures/fig_2_5.png\" width=\"800\"></p>\n",
    "<p style=\"text-align: center;\">Fig. (2.5): Los vectores $\\mathbf{v}_{1}$ y $\\mathbf{v}_{2}$ que se han asumido como no perpendiculares</p>\n",
    "\n",
    "Entonces,\n",
    "\n",
    "$$\\mathbf{v}_{2} =\\mathbf{v}^{\\prime }_{2} +a\\mathbf{v}_{1}$$\n",
    "<p style=\"text-align: right;\">$(2.29)$</p>\n",
    "\n",
    "Lamentablemente, la ecuación (2.29) contiene tres variables, y sólo conocemos una de ellas. Sin embargo, en virtud de las propiedades del producto interno, podemos escribir\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{v}_{2} =\\mathbf{v}^{\\prime }_{2} +a\\mathbf{v}_{1} &\\Longrightarrow &\\left< \\mathbf{v}_{2} ,\\mathbf{v}_{2} \\right>  =\\left< \\mathbf{v}^{\\prime }_{2} ,\\mathbf{v}_{1} \\right>  +\\left< a\\mathbf{v}_{1} ,\\mathbf{v}_{1} \\right>  \\\\ &\\Longrightarrow &\\left< \\mathbf{v}_{2} ,\\mathbf{v}_{1} \\right>  =a\\left< \\mathbf{v}_{1} ,\\mathbf{v}_{1} \\right>  \\\\ &\\Longrightarrow &a=\\displaystyle \\frac{\\left< \\mathbf{v}_{2} ,\\mathbf{v}_{1} \\right>  }{\\left< \\mathbf{v}_{1} ,\\mathbf{v}_{1} \\right>  } \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(2.30)$</p>\n",
    "\n",
    "Y sustituyendo en la ecuación (2.29), obtenemos\n",
    "\n",
    "$$\\mathbf{v}^{\\prime }_{2} =\\mathbf{v}_{2} -\\frac{\\left< \\mathbf{v}_{2} ,\\mathbf{v}_{1} \\right>  }{\\left< \\mathbf{v}_{1} ,\\mathbf{v}_{1} \\right>  } \\mathbf{v}_{1}$$\n",
    "<p style=\"text-align: right;\">$(2.31)$</p>\n",
    "\n",
    "De esta manera, hemos probado un caso particular del siguiente teorema.\n",
    "\n",
    "**<font color='crimson'>Teorema 2.2 – Proceso de ortogonalización de Gram-Schmidt:</font>** *Sea $V$ un $\\mathbb{K}$-espacio vectorial equipado con un producto interno y $\\alpha =\\left\\{ v_{1},...,v_{n}\\right\\}$ una base de $V$. Entonces $\\alpha^{\\prime } =\\left\\{ v^{\\prime }_{1},...,v^{\\prime }_{n}\\right\\}$ es una base ortogonal para $V$, donde los elementos de $\\alpha^{\\prime }$ satisfacen todos la ecuación vectorial*\n",
    "\n",
    "$$\\begin{cases}v^{\\prime }_{1}=v_{1}&\\\\ v^{\\prime }_{j}=v_{j}-\\displaystyle \\frac{\\left< v_{j},v^{\\prime }_{j-1}\\right>  }{\\left< v^{\\prime }_{j-1},v^{\\prime }_{j-1}\\right>  } v^{\\prime }_{j-1}-\\cdots -\\displaystyle \\frac{\\left< v_{j},v^{\\prime }_{1}\\right>  }{\\left< v^{\\prime }_{1},v^{\\prime }_{1}\\right>  } v^{\\prime }_{1}&;\\  \\left( 2\\leq j\\leq n\\right)  \\end{cases}$$\n",
    "<p style=\"text-align: right;\">$(2.32)$</p>\n",
    "◆\n",
    "\n",
    "**Ejemplo 2.10:** Consideremos el subespacio $W=\\left\\{ \\left( x,y,z,t\\right)  \\in \\mathbb{R}^{4} :x+y+z+t=0\\right\\}$. Vamos a determinar una base ortogonal para $W$ utilizando el producto interno canónico de $\\mathbb{R}^{4}$. De esta manera, en primer lugar, determinamos una base para $W$:\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{u} \\in W&\\Longleftrightarrow &\\mathbf{u} =\\left( x,y,z,t\\right)  \\in \\mathbb{R}^{4} \\wedge x+y+z+t=0\\\\ &\\Longleftrightarrow &\\mathbf{u} =\\left( x,y,z,t\\right)  \\in \\mathbb{R}^{4} \\wedge t=-x-y-z\\\\ &\\Longleftrightarrow &\\mathbf{u} =\\left( x,y,z,-x-y-z\\right)  ;\\left( x,y,z\\right)  \\in \\mathbb{R}^{3} \\\\ &\\Longleftrightarrow &\\mathbf{u} =x\\left( 1,0,0,-1\\right)  +y\\left( 0,1,0,-1\\right)  +z\\left( 0,0,1,-1\\right)  ;\\left( x,y,z\\right)  \\in \\mathbb{R}^{3} \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(2.32)$</p>\n",
    "\n",
    "Luego $W=\\left< \\left\\{ \\left( 1,0,0,-1\\right)  ,\\left( 0,1,0,-1\\right)  ,\\left( 0,0,1,-1\\right)  \\right\\}  \\right>$. Por lo tanto, $\\alpha =\\left\\{ \\left( 1,0,0,-1\\right)  ,\\left( 0,1,0,-1\\right)  ,\\left( 0,0,1,-1\\right)  \\right\\}$ es una base de $W$, ya que sus componentes son LI (esto se deja como ejercicio al lector). Ahora construiremos una base ortogonal siguiendo el proceso de Gram-Schmidt:\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{v}^{\\prime }_{1} &=&\\left( 1,0,0,-1\\right)  \\\\ \\mathbf{v}^{\\prime }_{2} &=&\\left( 0,1,0,-1\\right)  -\\displaystyle \\frac{\\left< \\left( 0,1,0,-1\\right)  ,\\left( 1,0,0,-1\\right)  \\right>  }{\\left< \\left( 1,0,0,-1\\right)  ,\\left( 1,0,0,-1\\right)  \\right>  } \\left( 1,0,0,-1\\right)  \\\\ &=&\\left( 0,1,0,-1\\right)  -\\displaystyle \\frac{1}{2} \\left( 1,0,0,-1\\right)  \\\\ &=&\\left( -\\displaystyle \\frac{1}{2} ,1,0,-\\displaystyle \\frac{1}{2} \\right)  ;\\mathrm{tomaremos} \\  \\mathbf{v}^{\\prime }_{2} =\\left( -1,2,0,-1\\right)  \\\\ \\mathbf{v}^{\\prime }_{3} &=&\\left( 0,1,0,-1\\right)  -\\displaystyle \\frac{\\left< \\left( 0,0,1,-1\\right)  ,\\left( -1,2,0,-1\\right)  \\right>  }{\\left< \\left( -1,2,0,-1\\right)  ,\\left( -1,2,0,-1\\right)  \\right>  } \\left( -1,2,0,-1\\right)  -\\displaystyle \\frac{\\left< \\left( 0,1,0,-1\\right)  ,\\left( 1,0,0,-1\\right)  \\right>  }{\\left< \\left( 1,0,0,-1\\right)  ,\\left( 1,0,0,-1\\right)  \\right>  } \\left( 1,0,0,-1\\right)  \\\\ &=&\\left( 0,1,0,-1\\right)  -\\displaystyle \\frac{1}{6} \\left( -1,2,0,-1\\right)  -\\displaystyle \\frac{1}{2} \\left( 1,0,0,-1\\right)  =\\left( -\\displaystyle \\frac{1}{3} ,-\\displaystyle \\frac{1}{3} ,1,-\\displaystyle \\frac{1}{3} \\right)  ;\\mathrm{tomaremos} \\  \\mathbf{v}^{\\prime }_{3} =\\left( -1,-1,3,-1\\right)  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(2.33)$</p>\n",
    "\n",
    "Así que $\\alpha^{\\prime } =\\left\\{ \\left( 1,0,0,-1\\right)  ,\\left( -1,2,0,-1\\right)  ,\\left( -1,-1,3,-1\\right)  \\right\\}$ es una base ortogonal de $W$. ◼︎\n",
    "\n",
    "Sea $\\alpha =\\left\\{ v_{1},...,v_{n}\\right\\}  \\subset V$ una base ortogonal de un $\\mathbb{K}$-espacio vectorial $V$. Para cada $u\\in V$ tenemos la representación única en términos de la base $\\alpha$\n",
    "\n",
    "$$u=\\sum^{n}_{i=1} \\frac{\\left< u,v_{i}\\right>  }{\\left< v_{i},v_{i}\\right>  } v_{i}$$\n",
    "<p style=\"text-align: right;\">$(2.34)$</p>\n",
    "\n",
    "Sin embargo, resulta interesante observar que el denominador del término común de esta sumatoria es, de hecho, el cuadrado de la norma inducida por el correspondiente producto interno (es decir, $\\left\\Vert v_{i}\\right\\Vert^{2}  =\\left< v_{i},v_{i}\\right>$). Podemos, por tanto, mejorar la definición de base ortogonal considerando el siguiente teorema.\n",
    "\n",
    "**<font color='crimson'>Teorema 2.3:</font>** *Sea $V$ un $\\mathbb{K}$-espacio vectorial y $\\alpha =\\left\\{ v_{1},...,v_{n}\\right\\}$ una base de $V$. Sea $\\beta =\\left\\{ v^{\\prime }_{1},...,v^{\\prime }_{n}\\right\\}$, donde $v^{\\prime }_{i}=v_{i}/\\left< v_{i},v_{i}\\right>$ para cada $i=1,...,n$. Entonces,*\n",
    "\n",
    "- **(T1):** $\\left< v^{\\prime }_{i},v^{\\prime }_{j}\\right>  =0\\Longleftrightarrow i\\neq j$.\n",
    "- **(T2):** $u=\\sum^{n}_{i=1} \\left< u,v_{i}\\right>  v^{\\prime }_{i}$.\n",
    "◆\n",
    "\n",
    "Lo anterior motiva la siguiente definición.\n",
    "\n",
    "**<font color='blue'>Definición 2.7 – Base ortonormal (general):</font>** Sea $V$ un $\\mathbb{K}$-espacio vectorial normado y sea $\\beta =\\left\\{ w_{1},...,w_{n}\\right\\}  \\subset V$. Diremos que $\\beta$ es una **base ortonormal** de $V$ si se cumplen con las siguientes condiciones:\n",
    "\n",
    "- **(C1):** $\\beta$ es una base ortogonal de $V$.\n",
    "- **(C2):** $\\left\\Vert v_{i}\\right\\Vert  =1;\\forall i,i=1,...,n$.\n",
    "\n",
    "Equivalentemente, podemos establecer que $\\beta=\\left\\{ w_{1},...,w_{j}\\right\\}  $ es una base ortonormal de $V$ si y sólo si $\\left< w_{i},w_{j}\\right>  =1$ para $i=j$, y $\\left< w_{i},w_{j}\\right>  =0$ para $i\\neq j$.\n",
    "\n",
    "**Ejemplo 2.11:** La base $\\beta =\\left\\{ \\left( 1,1\\right)  ,\\left( 1,-1\\right)  \\right\\}$ es ortogonal respecto del producto interno canónico en $\\mathbb{R}^{2}$, puesto que $\\left< \\left( 1,1\\right)  ,\\left( 1,-1\\right)  \\right>  =1-1=0$. Pero como $\\left\\Vert \\left( 1,1\\right)  \\right\\Vert  =\\left\\Vert \\left( 1,-1\\right)  \\right\\Vert  =\\sqrt{2}$, se tiene que $\\beta$ no es una base ortonormal. No obstante, la base $\\gamma$, definida como\n",
    "\n",
    "$$\\gamma =\\left\\{ \\left( \\frac{1}{\\sqrt{2} } ,\\frac{1}{\\sqrt{2} } \\right)  ,\\left( \\frac{1}{\\sqrt{2} } ,-\\frac{1}{\\sqrt{2} } \\right)  \\right\\}$$\n",
    "<p style=\"text-align: right;\">$(2.35)$</p>\n",
    "\n",
    "sí es una base ortonormal para $\\mathbb{R}^{2}$. ◼︎\n",
    "\n",
    "Una de las más importantes consecuencias devenida de la existencia de las bases ortonormales, es la siguiente.\n",
    "\n",
    "**<font color='crimson'>Teorema 2.4:</font>** *Sea $V$ un $\\mathbb{K}$-espacio vectorial normado. Si $\\alpha$ y $\\beta$ son dos bases ortonormales de $V$, entonces se cumple que*\n",
    "\n",
    "$$\\left( \\left[ \\mathbf{I} \\right]^{\\beta }_{\\alpha }  \\right)^{-1}  =\\left( \\left[ \\mathbf{I} \\right]^{\\beta }_{\\alpha }  \\right)^{\\top }$$\n",
    "<p style=\"text-align: right;\">$(2.36)$</p>\n",
    "◆\n",
    "\n",
    "Vamos a tomarnos el tiempo de demostrar el teorema (2.4) a fin de entender completamente este resultado. En efecto, por una parte, si $\\alpha =\\left\\{ v_{1},...,v_{n}\\right\\}$ y $\\beta =\\left\\{ w_{1},...,w_{n}\\right\\}$ son las representaciones explícitas de ambas bases ortonormales, entonces, por definición de la matriz de cambio de base $\\left[ \\mathbf{I} \\right]^{\\beta }_{\\alpha }$, tenemos que\n",
    "\n",
    "$$\\left[ \\mathbf{I} \\right]^{\\beta }_{\\alpha }  =\\left( \\left[ v_{1}\\right]_{\\beta }  ,\\left[ v_{2}\\right]_{\\beta }  ,...,\\left[ v_{n}\\right]_{\\beta }  \\right)  =\\left( \\begin{matrix}\\left< v_{1},w_{1}\\right>  &\\left< v_{2},w_{1}\\right>  &\\cdots &\\left< v_{n},w_{1}\\right>  \\\\ \\left< v_{1},w_{2}\\right>  &\\left< v_{2},w_{2}\\right>  &\\cdots &\\left< v_{n},w_{2}\\right>  \\\\ \\vdots &\\vdots &\\ddots &\\vdots \\\\ \\left< v_{1},w_{n}\\right>  &\\left< v_{2},w_{n}\\right>  &\\cdots &\\left< v_{n},w_{n}\\right>  \\end{matrix} \\right)  \\in \\mathbb{R}^{n\\times n}$$\n",
    "<p style=\"text-align: right;\">$(2.37)$</p>\n",
    "\n",
    "Sin embargo, expresando la amtriz de cambio de base desde $\\beta$ hacia $\\alpha$, también por definición, tenemos que\n",
    "\n",
    "$$\\begin{array}{lll}\\left[ \\mathbf{I} \\right]^{\\alpha }_{\\beta }  &=&\\left( \\left[ w_{1}\\right]_{\\alpha }  ,\\left[ w_{2}\\right]_{\\alpha }  ,...,\\left[ w_{n}\\right]_{\\alpha }  \\right)  \\\\ &=&\\left( \\begin{matrix}\\left< w_{1},v_{1}\\right>  &\\left< w_{2},v_{1}\\right>  &\\cdots &\\left< w_{n},v_{1}\\right>  \\\\ \\left< w_{1},v_{2}\\right>  &\\left< w_{2},v_{2}\\right>  &\\cdots &\\left< w_{n},v_{2}\\right>  \\\\ \\vdots &\\vdots &\\ddots &\\vdots \\\\ \\left< w_{1},v_{n}\\right>  &\\left< w_{2},v_{n}\\right>  &\\cdots &\\left< w_{n},v_{n}\\right>  \\end{matrix} \\right)  \\\\ &=&\\left( \\begin{matrix}\\left< v_{1},w_{1}\\right>  &\\left< v_{1},w_{2}\\right>  &\\cdots &\\left< v_{1},w_{n}\\right>  \\\\ \\left< v_{2},w_{1}\\right>  &\\left< v_{2},w_{2}\\right>  &\\cdots &\\left< v_{2},w_{n}\\right>  \\\\ \\vdots &\\vdots &\\ddots &\\vdots \\\\ \\left< v_{n},w_{1}\\right>  &\\left< v_{n},w_{2}\\right>  &\\cdots &\\left< v_{n},w_{n}\\right>  \\end{matrix} \\right)^{\\top }  \\\\ &=&\\left( \\left[ \\mathbf{I} \\right]^{\\beta }_{\\alpha }  \\right)^{\\top }  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(2.38)$</p>\n",
    "\n",
    "Por lo tanto, se tiene que, conforme el teorema (1.7), $\\left( \\left[ \\mathbf{I} \\right]^{\\beta }_{\\alpha }  \\right)^{\\top }  =\\left[ \\mathbf{I} \\right]^{\\alpha }_{\\beta }  =\\left( \\left[ \\mathbf{I} \\right]^{\\beta }_{\\alpha }  \\right)^{-1}$, lo que prueba el teorema (2.4)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "039fcb33",
   "metadata": {},
   "source": [
    "## Proyecciones.\n",
    "Los conceptos trabajados previamente han tenido como gran objetivo construir un concepto fundamental, primero, del álgebra lineal, y segundo, en el contexto de los modelos de machine learning, conocido como **proyección**. Las proyecciones corresponden a un tipo importante de transformaciones lineales (además de las rotaciones y reflexiones) y juegan un rol importante en la visualización de información, teoría de la información, estadística y modelos de aprendizaje no supervisado. En machine learning, con frecuencia, nos vemos enfrentados a problemas caracterizados por conjuntos de datos que tienen un elevado número de variables (es decir, de alta dimensión). Este tipo de conjuntos de datos resultan, en general, difíciles de visualizar e incluso analizar. Sin embargo, también, en general, puede ocurrir que la mayor parte de la información de interés esté contenida en un subconjunto muy reducido de variables que caracterizan el conjunto completo (es decir, en un número reducido de dimensiones; o en el lenguaje abstracto que hemos ido construyendo en estos apuntes, en un subespacio vectorial de menor dimensión). Esto implica, en muchos problemas, la necesidad de *comprimir* esta data, lo que, aunque en muchos casos no será muy dañino, implicará la pérdida de información de aquellas dimensiones donde hemos estimado que no existe información realmente relevante.\n",
    "\n",
    "Para minimizar la pérdida de información debida a esta compresión, idealmente, debemos encontrar aquellas dimensiones que maximizan la cantidad de información importante contenida en nuestra data. Como ya lo discutimos en la sección introductoria, la data puede ser representada por vectores agrupados en una gran matriz que caracteriza nuestra información y, en esta subsección, estudiaremos la base de varios algoritmos fundamentales en la compresión de conjuntos de datos, lo que constituye un campo de estudio en machine learning por sí mismo, conocido como **reducción de dimensionalidad** y que, a su vez, conforma una parte importante de los llamados **modelos de aprendizaje no supervisado**. También es clave en la construcción de modelos generativos de gran complejidad conocidos como **autoencoders**, que a su vez conforman parte importante en la **teoría de redes neuronales** y que son los instrumentos por antonomasia del campo del **aprendizaje profundo** o **deep learning**.\n",
    "\n",
    "En la Fig. (2.6) se observa una ilustración del concepto de proyección ortogonal de los puntos que representan a un conjunto de datos en $\\mathbb{R}^{3}$. En tal conjunto, podemos observar que la mayoría de la información contenida en el mismo puede conservarse en un subespacio de $\\mathbb{R}^{2}$ que describe un plano sobre el cual, convenientemente, proyectamos los puntos de manera normal (ortogonal) a él. De esta manera, esta compresión transforma a un conjunto de datos en $\\mathbb{R}^{3}$ en otro, $\\mathbb{R}^{2}$, donde las nuevas variables representan componentes que conservan una fracción importante de la varianza del conjunto original. Esta es la base del **análisis de componentes principales**, que es uno de los modelos de aprendizaje no supervisado más simples e importantes en machine learning.\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"figures/fig_2_6.png\" width=\"500\"></p>\n",
    "<p style=\"text-align: center;\">Fig. (2.6): Un conjunto de datos en $\\mathbb{R}^{3}$ cuya estructura es tal que éste puede proyectarse en un subespacio de $\\mathbb{R}^{2}$ sin perder información relevante referida a él</p>\n",
    "\n",
    "**<font color='blue'>Definición 2.8 – Proyección (general):</font>** Sea $V$ un $\\mathbb{K}$-espacio vectorial y $U\\subset V$ un subespacio de $V$. Una transformación lineal $\\pi_{U}: V\\longrightarrow U$ se denomina **proyección** de $V$ sobre $U$, si $\\pi_{U}^{2} =\\left( \\pi_{U} \\circ \\pi_{U} \\right)  =\\pi_{U}$. Es decir, si $\\pi$ se comporta como la función identidad sobre $U$. La matriz de cambio de base asociada a la proyección $\\pi_{U}$ se denomina como $P_{\\pi}$, y es llamada **matriz proyectiva**.\n",
    "\n",
    "Notemos que, por extensión, se tendrá que $\\pi_{U}(u)=u$ para todo $u\\in U$.\n",
    "\n",
    "A continuación, derivaremos las proyecciones ortogonales de vectores en el espacio vectorial normado $\\left( \\mathbb{R}^{n} ,\\left< \\  ,\\  \\right>  \\right)$ sobre determinados subespacios de $\\mathbb{R}^{n}$. Comenzaremos con subespacios unidimensionales (los que, en un contexto más geométrico, representan rectas en $\\mathbb{R}^{n}$). En estos desarrollos, trabajaremos con el producto interno canónico en $\\mathbb{R}^{n}$. Es decir, para todo par de vectores $\\mathbf{u}, \\mathbf{v}\\in \\mathbb{R}^{n}$, definiremos $\\left< \\mathbf{u} ,\\mathbf{v} \\right>  =\\mathbf{u}^{\\top } \\mathbf{v}$.\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"figures/fig_2_7.png\" width=\"700\"></p>\n",
    "<p style=\"text-align: center;\">Fig. (2.7): (a) El vector $\\mathbf{x}$ proyectado sobre el vector $\\mathbf{b}$; (b) Proyección ortogonal del vector $\\mathbf{x}$ sobre $\\mathbf{b}$, donde $\\mathbf{x}$ es un vector unitario </p>\n",
    "\n",
    "### Proyección sobre espacios unidimensionales.\n",
    "Dada una recta que pasa por el origen generada por el vector $\\mathbf{b}\\in \\mathbb{R}^{n}$, sabemos que dicha recta es un subespacio, digamos $U$, de $\\mathbb{R}^{n}$, generado por la base $\\mathbf{b}$. Cuando proyectamos un vector $\\mathbf{x}\\in \\mathbb{R}\\in \\mathbb{R}^{n}$ sobre $U$, nuestro objetivo es buscar el vector $\\pi_{U}(\\mathbf{x})\\in U$ más cercano a $\\mathbf{x}$. Por medio de argumentos geométricos, vamos a caracterizar algunas propiedades de la proyección $\\pi_{U}(\\mathbf{x})$ conforme a la situación que se muestra en la Fig. (2.7a).\n",
    "\n",
    "- La proyección $\\pi_{U}(\\mathbf{x})$ es la más cercana a $\\mathbf{x}$, donde \"más cercana\" significa que la distancia $\\left\\Vert \\mathbf{x} -\\pi_{U} \\left( \\mathbf{x} \\right)  \\right\\Vert$ es mínima. Se tiene entonces que el segmento $\\pi_{U} \\left( \\mathbf{x} \\right)  -\\mathbf{x}$ que va desde $\\pi_{U}(\\mathbf{x})$ a $\\mathbf{x}$ es ortogonal a $U$ y, por tanto, la base $\\mathbf{b}$ de $U$ es también ortogonal. La condición de ortogonalidad nos permite establecer que $\\left< \\pi_{U} \\left( \\mathbf{x} \\right)  -\\mathbf{x} ,\\mathbf{b} \\right>  =0$, ya que los ángulos entre vectores siguen la definición del producto interno canónico en $\\mathbb{R}^{n}$.\n",
    "\n",
    "- La proyección $\\pi_{U}(\\mathbf{x})$ de $\\mathbf{x}$ sobre $U$ debe ser un elemento de $U$ y, por lo tanto, un múltiplo de la base $\\mathbf{b}$ que genera $U$. Por lo tanto, $\\pi_{U}(\\mathbf{x})=\\lambda \\mathbf{b}$ para algún $\\lambda \\in \\mathbb{R}$.\n",
    "\n",
    "Debemos, por tanto, determinar el factor $\\lambda$, la proyección $\\pi_{U}(\\mathbf{x})\\in U$ y la matriz proyectiva $\\mathbf{P}_{\\pi}$ que aplica cualquier vector $\\mathbf{x}\\in \\mathbb{R}^{n}$ sobre $U$. Para ello, procederemos mediante tres pasos.\n",
    "\n",
    "<font color='green'>Etapa 1:</font> Encontrar el factor $\\lambda$. La condición de ortogonalidad nos da\n",
    "\n",
    "$$\\begin{array}{ccc}\\left< \\pi_{U} \\left( \\mathbf{x} \\right)  -\\mathbf{x} ,\\mathbf{b} \\right>  =0&\\overbrace{\\Longleftrightarrow }^{\\pi_{U} \\left( \\mathbf{x} \\right)  =\\lambda \\mathbf{b} } &\\left< \\mathbf{x} -\\lambda \\mathbf{b} ,\\mathbf{b} \\right>  =0\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(2.39)$</p>\n",
    "\n",
    "Aprovecharemos la propiedad de linealidad del producto interno para escribir\n",
    "\n",
    "$$\\left< \\mathbf{x} ,\\mathbf{b} \\right>  -\\lambda \\left< \\mathbf{b} ,\\mathbf{b} \\right>  =0\\Longleftrightarrow \\lambda =\\frac{\\left< \\mathbf{x} ,\\mathbf{b} \\right>  }{\\left< \\mathbf{b} ,\\mathbf{b} \\right>  } =\\frac{\\left< \\mathbf{b} ,\\mathbf{x} \\right>  }{\\| \\mathbf{b} \\|^{2} }$$\n",
    "<p style=\"text-align: right;\">$(2.40)$</p>\n",
    "\n",
    "En el último paso, aprovechamos el hecho de que el producto interno es simétrico. Si el producto interno es el usual de $\\mathbb{R}^{n}$, obtenemos\n",
    "\n",
    "$$\\lambda =\\frac{\\mathbf{b}^{\\top } \\mathbf{x} }{\\mathbf{b}^{\\top } \\mathbf{b} }$$\n",
    "<p style=\"text-align: right;\">$(2.41)$</p>\n",
    "\n",
    "Si $\\mathbf{b}$ es un vector ortonormal (es decir, si $\\| \\mathbf{b} \\| =1$), entonces el **factor proyectivo** $\\lambda$ está dado por $\\lambda=\\mathbf{b}^{\\top}\\mathbf{x}$.\n",
    "\n",
    "<font color='green'>Etapa 2:</font> Encontrar la proyección $\\pi_{U}(\\mathbf{x})\\in U$. Dado que $\\pi_{U}(\\mathbf{x})=\\lambda \\mathbf{b}$, obtenemos inmediatamente, por la ecuación (2.39), que\n",
    "\n",
    "$$\\pi_{U} \\left( \\mathbf{x} \\right)  =\\lambda \\mathbf{b} =\\frac{\\left< \\mathbf{x} ,\\mathbf{b} \\right>  }{\\| \\mathbf{b} \\|^{2} } \\mathbf{b} =\\frac{\\mathbf{b}^{\\top } \\mathbf{x} }{\\| \\mathbf{b} \\|^{2} } \\mathbf{b}$$\n",
    "<p style=\"text-align: right;\">$(2.42)$</p>\n",
    "\n",
    "Donde la última igualdad es válida sólo para el caso del producto interno canónico de $\\mathbb{R}^{n}$. También podemos calcular la longitud de $\\pi_{U}(\\mathbf{x})$, aprovechando las propiedades de la norma inducida por el producto interno, como\n",
    "\n",
    "$$\\left\\Vert \\pi_{U} \\left( \\mathbf{x} \\right)  \\right\\Vert  =\\left\\Vert \\lambda \\mathbf{b} \\right\\Vert  =\\left| \\lambda \\right|  \\left\\Vert \\mathbf{b} \\right\\Vert$$\n",
    "<p style=\"text-align: right;\">$(2.43)$</p>\n",
    "\n",
    "Por lo tanto, nuestra proyección tiene longitud igual a $\\lambda$ veces la magnitud del vector $\\mathbf{b}$. Aquello también permite establecer la noción intuitiva de que $\\lambda$ es la coordenada de $\\pi_{U}(\\mathbf{x})$ con respecto a la base $\\mathbf{b}$ que genera nuestro subespacio $U$.\n",
    "\n",
    "Si usamos el producto interno canónico en $\\mathbb{R}^{n}$, obtenemos\n",
    "\n",
    "$$\\left\\Vert \\pi_{U} \\left( \\mathbf{x} \\right)  \\right\\Vert  \\overbrace{=}^{\\mathrm{Ec} .\\  (2.42)} \\frac{\\left| \\mathbf{b}^{\\top } \\mathbf{x} \\right|  }{\\| \\mathbf{b} \\|^{2} } \\| \\mathbf{b} \\| \\overbrace{=}^{\\mathrm{Ec} .\\  \\left( 2.16\\right)  } \\left| \\cos \\left( \\omega \\right)  \\right|  \\| \\mathbf{x} \\| \\| \\mathbf{b} \\| \\frac{\\| \\mathbf{b} \\| }{\\| \\mathbf{b} \\|^{2} } =\\left| \\cos \\left( \\omega \\right)  \\right|  \\| \\mathbf{x} \\|$$\n",
    "<p style=\"text-align: right;\">$(2.44)$</p>\n",
    "\n",
    "Donde $\\omega$ es el ángulo entre $\\mathbf{x}$ y $\\mathbf{b}$. Esta ecuación debería sernos familiar desde la perspectiva de la trigonometría: Si $\\| \\mathbf{x} \\| =1$, entonces $\\mathbf{x}$ reside en un círculo de radio unitario. Se tiene entonces que la proyección sobre el eje horizontal generado por $\\mathbf{b}$ es exactamente igual a $\\cos(\\omega)$, y la longitud del vector correspondiente es $\\pi_{U}(\\mathbf{x})=\\left| \\cos \\left( \\omega \\right)  \\right|$. Este caso se ilustra en la Fig. (2.7b).\n",
    "\n",
    "<font color='green'>Etapa 3:</font> Encontrar la matriz proyectiva $\\mathbf{P}_{\\pi}$. Sabemos, de la definición (2.8), que la proyección $\\pi_{U}(\\mathbf{x})$ corresponde a una transformación lineal. Por lo tanto, tiene una matriz de cambio de base $\\mathbf{P}_{\\pi}$ tal que $\\pi_{U}(\\mathbf{x})=\\mathbf{P}_{\\pi}\\mathbf{x}$. Usando el producto canónico en $\\mathbb{R}^{n}$ y el hecho de que\n",
    "\n",
    "$$\\pi_{U} \\left( \\mathbf{x} \\right)  =\\lambda \\mathbf{b} =\\mathbf{b} \\frac{\\mathbf{b}^{\\top } \\mathbf{x} }{\\left\\Vert \\mathbf{b} \\right\\Vert^{2}  } =\\frac{\\mathbf{b} \\mathbf{b}^{\\top } }{\\left\\Vert \\mathbf{b} \\right\\Vert^{2}  } \\mathbf{x}$$\n",
    "<p style=\"text-align: right;\">$(2.45)$</p>\n",
    "\n",
    "Observamos inmediatamente que\n",
    "\n",
    "$$\\mathbf{P}_{\\pi } =\\frac{\\mathbf{b} \\mathbf{b}^{\\top } }{\\left\\Vert \\mathbf{b} \\right\\Vert^{2}  }$$\n",
    "<p style=\"text-align: right;\">$(2.46)$</p>\n",
    "\n",
    "Notemos que $\\mathbf{b} \\mathbf{b}^{\\top }$ (y, por extensión, $\\mathbf{P}_{\\pi }$) es una matriz simétrica (de rango 1) y $\\left\\Vert \\mathbf{b} \\right\\Vert^{2}$ es un escalar.\n",
    "\n",
    "La matriz proyectiva $\\mathbf{P}_{\\pi }$ permite proyectar cualquier vector $\\mathbf{x}\\in \\mathbb{R}^{n}$ sobre la recta que pasa por el origen y coincide con la dirección del vector $\\mathbf{b}$ (o, equivalentemente, sobre el subespacio $U$ generado por $\\mathbf{b}$).\n",
    "\n",
    "Debemos remarcar que la proyección $\\pi_{U}(\\mathbf{x})$ trabajada previamente sigue siendo un vector en $\\mathbb{R}^{n}$ y no un escalar. Sin embargo, no requerimos de las $n$ coordenadas de dicho vector para representar esta proyección, sino que únicamente una si queremos expresarla con respecto al vector $\\mathbf{b}$ que genera el subespacio $U$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9c3138a9",
   "metadata": {},
   "source": [
    "### Proyección sobre subespacios más generales.\n",
    "A continuación vamos a estudiar la situación general en la cual queremos construir proyecciones de vectores $\\mathbf{x}\\in \\mathbb{R}^{n}$ sobre subespacios $U \\subseteq \\mathbb{R}^{m}$ de dimensión $1\\leq m\\leq n$. Esta situación se ilustra en la Fig. (2.8).\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"figures/fig_2_8.png\" width=\"700\"></p>\n",
    "<p style=\"text-align: center;\">Fig. (2.8): El vector $\\mathbf{x}$ proyectado sobre un subespacio $U$ (representado, en este caso, por un plano)</p>\n",
    "\n",
    "Asumamos entonces que $\\alpha =\\left\\{ \\mathbf{b}_{1} ,...,\\mathbf{b}_{m} \\right\\}$ es una base de $U$. Cualquier proyección $\\pi_{U}(\\mathbf{x})$ sobre $U$, para $\\mathbf{x}\\in \\mathbb{R}^{n}$, es un elemento de $U$. Por lo tanto, tales proyecciones pueden ser representadas como combinaciones lineales de los vectores generadores $\\mathbf{b}_{1} ,...,\\mathbf{b}_{m}$ de $U$ con una colección de escalares $\\lambda_{1},...,\\lambda_{m}$. Por lo tanto, podemos escribir\n",
    "\n",
    "$$\\pi_{U} \\left( \\mathbf{x} \\right)  =\\sum^{m}_{i=1} \\lambda_{i} \\mathbf{b}_{i}$$\n",
    "<p style=\"text-align: right;\">$(2.47)$</p>\n",
    "\n",
    "Como en el caso unidimensional, seguiremos un procedimiento de tres pasos para caracterizar la proyección $\\pi_{U}(\\mathbf{x})$ y la correspondiente matriz proyectiva $\\mathbf{P}_{\\pi}$.\n",
    "\n",
    "<font color='green'>Etapa 1:</font> Encontrar las coordenadas $\\lambda_{1},...,\\lambda_{m}$ de la proyección (con respecto a la base de $U$), tales que la combinación lineal\n",
    "\n",
    "$$\\pi_{U} \\left( \\mathbf{x} \\right)  =\\sum^{m}_{i=1} \\lambda_{i} \\mathbf{b}_{i} =\\mathbf{B} \\mathbf{\\lambda } \\  ;\\  \\mathbf{B} =\\left( \\mathbf{b}_{1} ,...,\\mathbf{b}_{m} \\right)  \\in \\mathbb{R}^{n\\times m} \\wedge \\mathbf{\\lambda } =\\left( \\lambda_{1} ,...,\\lambda_{m} \\right)  \\in \\mathbb{R}^{m}$$\n",
    "<p style=\"text-align: right;\">$(2.48)$</p>\n",
    "\n",
    "minimiza la distancia entre la proyección $\\pi_{U}(\\mathbf{x})$ y el vector $\\mathbf{x}$. Como en el caso unidimensional, esto implica que todos los vectores que conectan $\\pi_{U}(\\mathbf{x})\\in U$ y $\\mathbf{x}\\in \\mathbb{R}^{n}$ deben ser ortogonales a todos los vectores que constituyen la base del subespacio $U$. Por lo tanto, tenemos $m$ condiciones simultáneas que se deben satisfacer, considerando como antes el producto interno canónico en $\\mathbb{R}^{n}$:\n",
    "\n",
    "$$\\begin{array}{ccc}\\left< \\mathbf{b}_{1} ,\\mathbf{x} -\\pi_{U} \\left( \\mathbf{x} \\right)  \\right>  &=&\\mathbf{b}^{\\top }_{1} \\left( \\mathbf{x} -\\pi_{U} \\left( \\mathbf{x} \\right)  \\right)  =0\\\\ \\vdots &\\vdots &\\vdots \\\\ \\left< \\mathbf{b}_{m} ,\\mathbf{x} -\\pi_{U} \\left( \\mathbf{x} \\right)  \\right>  &=&\\mathbf{b}^{\\top }_{M} \\left( \\mathbf{x} -\\pi_{U} \\left( \\mathbf{x} \\right)  \\right)  =0\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(2.49)$</p>\n",
    "\n",
    "Poniendo $\\pi_{U} \\left( \\mathbf{x} \\right)  =\\mathbf{B} \\mathbf{\\lambda }$, el sistema (2.49) puede reescribirse como\n",
    "\n",
    "$$\\begin{array}{ccc}\\mathbf{b}^{\\top }_{1} \\left( \\mathbf{x} -\\mathbf{B} \\mathbf{\\lambda } \\right)  &=&0\\\\ &\\vdots &\\\\ \\mathbf{b}^{\\top }_{m} \\left( \\mathbf{x} -\\mathbf{B} \\mathbf{\\lambda } \\right)  &=&0\\end{array}$$\n",
    "<p style=\"text-align: right;\">$(2.50)$</p>\n",
    "\n",
    "Lo que implica que,\n",
    "\n",
    "$$\\begin{array}{ccl}\\left( \\begin{matrix}\\mathbf{b}^{\\top }_{1} \\\\ \\vdots \\\\ \\mathbf{b}^{\\top }_{m} \\end{matrix} \\right)  \\left( \\mathbf{x} -\\mathbf{B} \\mathbf{\\lambda } \\right)  =\\mathbf{0} &\\Longleftrightarrow &\\mathbf{B}^{\\top } \\left( \\mathbf{x} -\\mathbf{B} \\mathbf{\\lambda } \\right)  =\\mathbf{0} \\\\ &\\Longleftrightarrow &\\mathbf{B}^{\\top } \\mathbf{B} \\mathbf{\\lambda } =\\mathbf{B}^{\\top } \\mathbf{x} \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(2.51)$</p>\n",
    "\n",
    "La última expresión en la ecuación (2.51) se conoce como **ecuación normal**. Debido a que el conjunto $\\alpha =\\left\\{ \\mathbf{b}_{1} ,...,\\mathbf{b}_{m} \\right\\}$ es una base de $U$ y, por extensión, sus elementos son linealmente independientes, entonces la matriz $\\mathbf{B}^{\\top } \\mathbf{B} \\in \\mathbb{R}^{m\\times m}$ es no singular (y, por tanto, invertible). Podemos pues escribir las coordenadas $\\lambda_{i}$ agrupadas en el vector $\\mathbf{\\lambda}$ como\n",
    "\n",
    "$$\\mathbf{\\lambda } =\\left( \\mathbf{B}^{\\top } \\mathbf{B} \\right)^{-1}  \\mathbf{B}^{\\top } \\mathbf{x}$$\n",
    "<p style=\"text-align: right;\">$(2.52)$</p>\n",
    "\n",
    "La matriz $\\left( \\mathbf{B}^{\\top } \\mathbf{B} \\right)^{-1}  \\mathbf{B}^{\\top }$ es llamada **pseudo-inversa** de la matriz $\\mathbf{B}$, y podemos calcularla aunque la dicha matriz no sea cuadrada. Simplemente necesitamos que $\\mathbf{B}$ sea definida positiva (abordaremos este concepto en detalle más adelante).\n",
    "\n",
    "<font color='green'>Etapa 2:</font> Encontrar la proyección $\\pi_{U}(\\mathbf{x})$. Ya establecimos que $\\pi_{U}(\\mathbf{x})=\\mathbf{B} \\mathbf{\\lambda}$. Por lo tanto, a partir de la ecuación (2.52), obtenemos\n",
    "\n",
    "$$\\pi_{U} \\left( \\mathbf{x} \\right)  =\\mathbf{B} \\left( \\mathbf{B}^{\\top } \\mathbf{B} \\right)^{-1}  \\mathbf{B}^{\\top } \\mathbf{x}$$\n",
    "<p style=\"text-align: right;\">$(2.53)$</p>\n",
    "\n",
    "<font color='green'>Etapa 3:</font> Encontrar la matriz proyectiva $\\mathbf{P}_{\\pi}$. De la ecuación (2.53), podemos ver inmediatamente que la matriz proyectiva se obtiene como solución de la ecuación $\\mathbf{P}_{\\pi} \\mathbf{x} = \\pi_{U}(\\mathbf{x})$. Luego,\n",
    "\n",
    "$$\\mathbf{P}_{\\pi } =\\mathbf{B} \\left( \\mathbf{B}^{\\top } \\mathbf{B} \\right)^{-1}  \\mathbf{B}^{\\top }$$\n",
    "<p style=\"text-align: right;\">$(2.54)$</p>\n",
    "\n",
    "Todo lo anterior nos permite establecer la siguiente definición más general.\n",
    "\n",
    "**<font color='blue'>Definición 2.9 – Proyección ortogonal (general):</font>** Sea $V$ un $\\mathbb{K}$-espacio vectorial normado y $U$ un subespacio de $V$, tal que $\\dim(U)\\leq \\dim(V)$. Sea $\\alpha =\\left\\{ w_{1},...,w_{s}\\right\\}$ una base ortogonal de $U$. Llamaremos **proyección ortogonal** de $V$ sobre $U$ a la transformación lineal $\\pi_{U}:V\\longrightarrow U$ definida explícitamente como\n",
    "\n",
    "$$\\pi_{U} \\left( v\\right)  =\\sum^{s}_{j=1} \\frac{\\left< v,w_{j}\\right>  }{\\left\\Vert w_{j}\\right\\Vert^{2}  } w_{j}\\  \\  \\  ;\\  \\  \\  v\\in U$$\n",
    "<p style=\"text-align: right;\">$(2.55)$</p>\n",
    "\n",
    "**Ejemplo 2.12:** Hasta ahora hemos trabajado intensivamente la definición de lo que es una proyección ortogonal, pero no hemos recurrido a ejemplos demasiado prácticos. Sea pues $U=\\left\\{ \\left( x,y,z,t\\right)  \\in \\mathbb{R}^{4} :x+y+z+t=0\\wedge x+y+z-2t=0\\right\\}  \\subset \\mathbb{R}^{4}$ un subespacio de $\\mathbb{R}^{4}$. Utilizando como base el producto interno canónico de $\\mathbb{R}^{4}$, vamos a construir la proyección ortogonal $\\pi_{U}(\\mathbf{x})$ para $\\mathbf{x}\\in \\mathbb{R}^{4}$. Para ello, debemos determinar una base para $U$ de la forma usual\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{x} \\in U&\\Longleftrightarrow &\\mathbf{x} =\\left( x,y,z,t\\right)  \\in \\mathbb{R}^{4} \\wedge x+y+z+t=0\\wedge x+y+z-2t=0\\\\ &\\Longleftrightarrow &\\mathbf{x} =\\left( x,y,z,t\\right)  \\in \\mathbb{R}^{4} \\wedge x+y+z=-t\\wedge x+y+z=2t\\\\ &\\Longleftrightarrow &\\mathbf{x} =\\left( x,y,z,t\\right)  \\in \\mathbb{R}^{4} \\wedge t=0\\wedge x+y+z=0\\\\ &\\Longleftrightarrow &\\mathbf{x} =\\left( x,y,z,0\\right)  \\in \\mathbb{R}^{4} \\wedge z=-x-y\\\\ &\\Longleftrightarrow &\\mathbf{x} =\\left( x,y,-x-y,0\\right)  ;\\left( x,y\\right)  \\in \\mathbb{R}^{2} \\\\ &\\Longleftrightarrow &\\mathbf{x} =\\left( x,0,-x,0\\right)  +\\left( 0,y,-y,0\\right)  ;\\left( x,y\\right)  \\in \\mathbb{R}^{2} \\\\ &\\Longleftrightarrow &\\mathbf{x} =x\\left( 1,0,-1,0\\right)  +y\\left( 0,1,-1,0\\right)  ;\\left( x,y\\right)  \\in \\mathbb{R}^{2} \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(2.56)$</p>\n",
    "\n",
    "Luego,\n",
    "\n",
    "$$U=\\left< \\left\\{ \\left( 1,0,-1,0\\right)  ,\\left( 0,1,-1,0\\right)  \\right\\}  \\right>$$\n",
    "<p style=\"text-align: right;\">$(2.57)$</p>\n",
    "\n",
    "El conjunto $\\alpha =\\left\\{ \\left( 1,0,-1,0\\right)  ,\\left( 0,1,-1,0\\right)  \\right\\}$ es sin duda una base de $U$, ya que sus componentes son linealmente independientes. En efecto,\n",
    "\n",
    "$$x\\left( 1,0,-1,0\\right)  +y\\left( 0,1,-1,0\\right)  =\\left( 0,0,0,0\\right)  \\Longleftrightarrow \\left( x,y,-x-y,0\\right)  =\\left( 0,0,0,0\\right)  \\Longleftrightarrow x=y=0$$\n",
    "<p style=\"text-align: right;\">$(2.58)$</p>\n",
    "\n",
    "Vamos a usar la base $\\alpha$ recién calculada para construir otra base $\\beta$ para $U$ que sea ortogonal. Para ello, procederemos conforme el método de Gram-Schmidt. De esta manera,\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{v}_{1} &=&\\left( 1,0,-1,0\\right)  \\\\ \\mathbf{v}_{2} &=&\\left( 0,1,-1,0\\right)  -\\displaystyle \\frac{\\left< \\left( 0,1,-1,0\\right)  ,\\left( 1,0,-1,0\\right)  \\right>  }{\\left< \\left( 1,0,-1,0\\right)  ,\\left( 1,0,-1,0\\right)  \\right>  } \\left( 1,0,-1,0\\right)  \\\\ &=&\\left( 0,1,-1,0\\right)  -\\displaystyle \\frac{1}{2} \\left( 1,0,-1,0\\right)  =\\left( -\\frac{1}{2} ,1,-\\displaystyle \\frac{1}{2} ,0\\right)  =\\displaystyle \\frac{1}{2} \\left( -1,2,-1,0\\right)  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(2.59)$</p>\n",
    "\n",
    "Luego podemos usar la base ortogonal $\\beta =\\left\\{ \\left( 1,0,-1,0\\right)  ,\\left( -1,2,-1,0\\right)  \\right\\}$ para determinar la proyección ortogonal $\\pi_{U}(\\mathbf{x})$. Aplicando directamente la definición (2.9), obtenemos\n",
    "\n",
    "$$\\begin{array}{lll}\\pi_{U} \\left( x,y,z,t\\right)  &=&\\displaystyle \\frac{\\left< \\left( x,y,z,t\\right)  ,\\left( 1,0,-1,0\\right)  \\right>  }{\\left\\Vert \\left( 1,0,-1,0\\right)  \\right\\Vert^{2}  } \\left( 1,0,-1,0\\right)  +\\displaystyle \\frac{\\left< \\left( x,y,z,t\\right)  ,\\left( -1,2,-1,0\\right)  \\right>  }{\\left\\Vert \\left( -1,2,-1,0\\right)  \\displaystyle \\right\\Vert^{2}  } \\left( -1,2,-1,0\\right)  \\\\ &=&\\displaystyle \\frac{x-z}{2} \\left( 1,0,-1,0\\right)  +\\displaystyle \\frac{-x+2y-z}{6} \\left( -1,2,-1,0\\right)  \\\\ &=&\\left( \\displaystyle \\frac{x-z}{2} ,0,\\displaystyle \\frac{z-x}{2} ,0\\right)  +\\left( \\displaystyle \\frac{-x+2y-z}{6} ,\\displaystyle \\frac{-x+2y+z}{3} ,\\displaystyle \\frac{-x+2y-z}{6} ,0\\right)  \\\\ &=&\\left( \\displaystyle \\frac{4x-2y-2z}{6} ,\\displaystyle \\frac{-x+2y-z}{3} ,\\displaystyle \\frac{-2x-2y+4z}{6} ,0\\right)  \\\\ &=&\\left( \\displaystyle \\frac{2x-y-z}{3} ,\\displaystyle \\frac{-x+2y-z}{3} ,\\displaystyle \\frac{-x-y+2z}{3} ,0\\right)  \\end{array} $$\n",
    "<p style=\"text-align: right;\">$(2.60)$</p>\n",
    "◼︎\n",
    "\n",
    "**Ejemplo 2.13:** Definimos en $\\mathbb{R}^{4\\times 1}$ el producto interno\n",
    "\n",
    "$$\\left< \\left( \\begin{matrix}x_{1}\\\\ x_{2}\\\\ x_{3}\\\\ x_{4}\\end{matrix} \\right)  ,\\left( \\begin{matrix}y_{1}\\\\ y_{2}\\\\ y_{3}\\\\ y_{4}\\end{matrix} \\right)  \\right>  =\\sum^{4}_{j=1} x_{j}y_{j}=x_{1}y_{1}+\\cdots +x_{4}y_{4}$$\n",
    "<p style=\"text-align: right;\">$(2.61)$</p>\n",
    "\n",
    "Considerando el subespacio $W$, definido como\n",
    "\n",
    "$$W=\\left\\{ \\mathbf{A} =\\left( \\begin{matrix}x\\\\ y\\\\ z\\\\ t\\end{matrix} \\right)  \\in \\mathbb{R}^{4\\times 1} :x+2y-z-t=0\\wedge x-2y+3z+t=0\\right\\}$$\n",
    "<p style=\"text-align: right;\">$(2.62)$</p>\n",
    "\n",
    "Vamos a construir la proyección ortogonal $\\pi_{W}(\\mathbf{A})$ para $\\mathbf{A}\\in \\mathbb{R}^{4\\times 1}$. Este es un ejemplo donde el concepto de proyección ortogonal va más allá de la mera intuición geométrica construida para el caso de $\\mathbb{R}^{n}$.\n",
    "\n",
    "Procedemos entonces determinando una base para el subespacio $W$. En efecto,\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{A} \\in W&\\Longleftrightarrow &\\mathbf{A} =\\left( \\begin{matrix}x\\\\ y\\\\ z\\\\ t\\end{matrix} \\right)  \\in \\mathbb{R}^{4\\times 1} \\wedge x+2y-z-t=0\\wedge x-2y+3z+t=0\\\\ &\\Longleftrightarrow &\\mathbf{A} =\\left( \\begin{matrix}x\\\\ y\\\\ z\\\\ t\\end{matrix} \\right)  \\in \\mathbb{R}^{4\\times 1} \\wedge \\begin{cases}\\begin{array}{rll}x+2y-z-t&=&0\\\\ x-2y+3z+t&=&0\\end{array} &\\end{cases} \\\\ &\\Longleftrightarrow &\\mathbf{A} =\\left( \\begin{matrix}x\\\\ y\\\\ z\\\\ t\\end{matrix} \\right)  \\in \\mathbb{R}^{4\\times 1} \\wedge \\underbrace{\\left( \\begin{matrix}1&2&-1&-1\\\\ 1&-2&3&1\\end{matrix} \\right)  }_{=\\mathbf{C} } \\left( \\begin{matrix}x\\\\ y\\\\ z\\\\ t\\end{matrix} \\right)  =\\left( \\begin{matrix}0\\\\ 0\\end{matrix} \\right)  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(2.63)$</p>\n",
    "\n",
    "Vamos a escalonar la matriz de coeficientes del sistema de ecuaciones resultante (que hemos llamado $\\mathbf{C}$), a fin de resolverlo por medio de la aplicación del teorema del rango. De esta manera,\n",
    "\n",
    "$$\\mathbf{C} =\\left( \\begin{matrix}1&2&-1&-1\\\\ 1&-2&3&1\\end{matrix} \\right)  \\overbrace{=}^{F_{21}\\left( -1\\right)  } \\left( \\begin{matrix}1&2&-1&-1\\\\ 0&-4&4&2\\end{matrix} \\right)  \\overbrace{=}^{F_{2}\\left( -\\frac{1}{4} \\right)  } \\left( \\begin{matrix}1&2&-1&-1\\\\ 0&1&-1&-1/2\\end{matrix} \\right)  \\overbrace{=}^{F_{12}\\left( -2\\right)  } \\left( \\begin{matrix}1&0&1&0\\\\ 0&1&-1&-1/2\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(2.64)$</p>\n",
    "\n",
    "De donde se tiene que,\n",
    "\n",
    "$$W=\\left< \\left\\{ \\left( \\begin{matrix}-1\\\\ 1\\\\ 1\\\\ 0\\end{matrix} \\right)  ,\\left( \\begin{matrix}0\\\\ 1/2\\\\ 0\\\\ 1\\end{matrix} \\right)  \\right\\}  \\right>  \\leq \\mathbb{R}^{4\\times 1}$$\n",
    "<p style=\"text-align: right;\">$(2.65)$</p>\n",
    "\n",
    "Así que el conjunto $\\alpha =\\left\\{ \\left( \\begin{matrix}-1\\\\ 1\\\\ 1\\\\ 0\\end{matrix} \\right)  ,\\left( \\begin{matrix}0\\\\ 1/2\\\\ 0\\\\ 1\\end{matrix} \\right)  \\right\\}$ es un sistema de generadores para $W$ que, de hecho, es una base, pues sus componentes son linealmente independientes. En efecto,\n",
    "\n",
    "$$z\\left( \\begin{matrix}-1\\\\ 1\\\\ 1\\\\ 0\\end{matrix} \\right)  +t\\left( \\begin{matrix}0\\\\ 1/2\\\\ 0\\\\ 1\\end{matrix} \\right)  =\\left( \\begin{matrix}0\\\\ 0\\\\ 0\\\\ 0\\end{matrix} \\right)  \\Longrightarrow \\left( \\begin{matrix}-z\\\\ z+t/2\\\\ z\\\\ t\\end{matrix} \\right)  =\\left( \\begin{matrix}0\\\\ 0\\\\ 0\\\\ 0\\end{matrix} \\right)  \\Longleftrightarrow z=t=0$$\n",
    "<p style=\"text-align: right;\">$(2.66)$</p>\n",
    "\n",
    "$$\\left< \\left( \\begin{matrix}-1\\\\ 1\\\\ 1\\\\ 0\\end{matrix} \\right)  ,\\left( \\begin{matrix}0\\\\ 1/2\\\\ 0\\\\ 1\\end{matrix} \\right)  \\right>  =-1\\cdot 0+1\\cdot \\frac{1}{2} +1\\cdot 0+0\\cdot 1=\\frac{1}{2}$$\n",
    "<p style=\"text-align: right;\">$(2.67)$</p>\n",
    "\n",
    "Vamos a obtener una base ortogonal para $W$ a partir de $\\alpha$ por medio del proceso de Gram-Schmidt. Si $\\beta =\\left\\{ \\mathbf{A}_{1} ,\\mathbf{A}_{2} \\right\\}$ es tal base, tenemos que:\n",
    "\n",
    "$$\\begin{array}{lll}\\mathbf{A}_{1} &=&\\left( \\begin{matrix}-1\\\\ 1\\\\ 1\\\\ 0\\end{matrix} \\right)  \\\\ \\mathbf{A}_{2} &=&\\left( \\begin{matrix}0\\\\ 1/2\\\\ 0\\\\ 1\\end{matrix} \\right)  -\\displaystyle \\frac{\\left< \\left( \\begin{matrix}0\\\\ 1/2\\\\ 0\\\\ 1\\end{matrix} \\right)  ,\\left( \\begin{matrix}-1\\\\ 1\\\\ 1\\\\ 0\\end{matrix} \\right)  \\right>  }{\\left< \\left( \\begin{matrix}-1\\\\ 1\\\\ 1\\\\ 0\\end{matrix} \\right)  ,\\left( \\begin{matrix}-1\\\\ 1\\\\ 1\\\\ 0\\end{matrix} \\right)  \\right>  } \\left( \\begin{matrix}-1\\\\ 1\\\\ 1\\\\ 0\\end{matrix} \\right)  =\\left( \\begin{matrix}0\\\\ 1/2\\\\ 0\\\\ 1\\end{matrix} \\right)  -\\displaystyle \\frac{1/2}{3} \\left( \\begin{matrix}-1\\\\ 1\\\\ 1\\\\ 0\\end{matrix} \\right)  =\\left( \\begin{matrix}0\\\\ 1/2\\\\ 0\\\\ 1\\end{matrix} \\right)  -\\displaystyle \\frac{1}{6} \\left( \\begin{matrix}-1\\\\ 1\\\\ 1\\\\ 0\\end{matrix} \\right)  \\\\ &=&\\left( \\begin{matrix}1/6\\\\ 1/3\\\\ -1/6\\\\ 1\\end{matrix} \\right)  =\\displaystyle \\frac{1}{6} \\left( \\begin{matrix}1\\\\ 2\\\\ -1\\\\ 6\\end{matrix} \\right)  \\end{array}$$\n",
    "<p style=\"text-align: right;\">$(2.68)$</p>\n",
    "\n",
    "La base $\\beta =\\left\\{ \\mathbf{A}_{1} ,\\mathbf{A}_{2} \\right\\}$ construida previamente es ortogonal, ya que\n",
    "\n",
    "$$\\left< \\left( \\begin{matrix}-1\\\\ 1\\\\ 1\\\\ 0\\end{matrix} \\right)  ,\\left( \\begin{matrix}1\\\\ 2\\\\ -1\\\\ 6\\end{matrix} \\right)  \\right>  =-1+2-1=0$$\n",
    "<p style=\"text-align: right;\">$(2.69)$</p>\n",
    "\n",
    "Finalmente, definimos la proyección ortogonal $\\pi_{W}(\\mathbf{A})$,\n",
    "\n",
    "$$\\pi_{W} \\left( \\mathbf{A} \\right)  =\\left( \\frac{\\left< \\left( \\begin{matrix}x\\\\ y\\\\ z\\\\ t\\end{matrix} \\right)  ,\\left( \\begin{matrix}-1\\\\ 1\\\\ 1\\\\ 0\\end{matrix} \\right)  \\right>  }{\\left< \\left( \\begin{matrix}-1\\\\ 1\\\\ 1\\\\ 0\\end{matrix} \\right)  ,\\left( \\begin{matrix}-1\\\\ 1\\\\ 1\\\\ 0\\end{matrix} \\right)  \\right>  } \\right)  \\left( \\begin{matrix}-1\\\\ 1\\\\ 1\\\\ 0\\end{matrix} \\right)  +\\left( \\frac{\\left< \\left( \\begin{matrix}x\\\\ y\\\\ z\\\\ t\\end{matrix} \\right)  ,\\left( \\begin{matrix}1\\\\ 2\\\\ -1\\\\ 6\\end{matrix} \\right)  \\right>  }{\\left< \\left( \\begin{matrix}1\\\\ 2\\\\ -1\\\\ 6\\end{matrix} \\right)  ,\\left( \\begin{matrix}1\\\\ 2\\\\ -1\\\\ 6\\end{matrix} \\right)  \\right>  } \\right)  \\left( \\begin{matrix}1\\\\ 2\\\\ -1\\\\ 6\\end{matrix} \\right)  =\\frac{-x+y+z}{3} \\left( \\begin{matrix}-1\\\\ 1\\\\ 1\\\\ 0\\end{matrix} \\right)  +\\frac{x+2y-z+6t}{42} \\left( \\begin{matrix}1\\\\ 2\\\\ -1\\\\ 6\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(2.70)$</p>\n",
    "\n",
    "Lo que nos conduce a\n",
    "\n",
    "$$\\pi_{W} \\left( \\begin{matrix}x\\\\ y\\\\ z\\\\ t\\end{matrix} \\right)  =\\left( \\begin{array}{c}\\frac{x-y-z}{3} +\\frac{x+2y-z+6t}{42} \\\\ \\frac{-x+y+z}{3} +\\frac{x+2y-z+6t}{21} \\\\ \\frac{-x+y+z}{3} +\\frac{-x-2y+z-6t}{42} \\\\ \\frac{x+2y-z+6t}{7} \\end{array} \\right)  =\\left( \\begin{array}{c}\\frac{14x-14y-14z+x+2y-z+6t}{42} \\\\ \\frac{-7x+7y+7z+x+2y-z+6t}{21} \\\\ \\frac{-14x+14y+14z-x-2y+z-6t}{42} \\\\ \\frac{x+2y-z+6t}{7} \\end{array} \\right)  =\\left( \\begin{array}{c}\\frac{15x-12y-15z+6t}{42} \\\\ \\frac{-6x+9y+6z+6t}{21} \\\\ \\frac{-15x+12y+15z-6t}{42} \\\\ \\frac{x+2y-z+6t}{7} \\end{array} \\right)  =\\left( \\begin{array}{c}\\frac{5x-4y-5z+2t}{14} \\\\ \\frac{-2x+3y+2z+2t}{7} \\\\ \\frac{-5x+4y+5z-2t}{14} \\\\ \\frac{x+2y-z+6t}{7} \\end{array} \\right)$$\n",
    "<p style=\"text-align: right;\">$(2.71)$</p>\n",
    "\n",
    "Así que la función $\\pi_{W}(\\mathbf{A})$ es la proyección buscada. ◼︎"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ddf4c17",
   "metadata": {},
   "source": [
    "## Rotaciones.\n",
    "\n",
    "### Rotaciones en $\\mathbb{R}^{2}$.\n",
    "La preservación de longitudes y ángulos son dos de las principales características de las transformaciones lineales con matrices de cambio de base ortogonales. Sin embargo, estamos interesados en un cierto tipo de matrices de este tipo que permiten describir movimientos conocidos como rotaciones.\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"figures/fig_2_9.png\" width=\"400\"></p>\n",
    "<p style=\"text-align: center;\">Fig. (2.9): El cuadrado de la figura afectado por una rotación cuyo sentido está definido por la matriz $\\mathbf{R}$</p>\n",
    "\n",
    "Una rotación es una transformación lineal que permite, como su nombre lo indica, rotar un plano en un ángulo $\\theta$ con respecto a un determinado punto fijo que sirve como referencia de la propia rotación. Para un ángulo positivo $\\theta >0$, por convención, la rotación tiene sentido antihorario. Un ejemplo de ello se muestra en la Fig (2.9), donde la matriz de rotación asociada es:\n",
    "\n",
    "$$\\mathbf{R} =\\left( \\begin{matrix}-0.38&-0.92\\\\ 0.92&-0.38\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(2.72)$</p>\n",
    "\n",
    "Algunos campos de aplicación importantes de las rotaciones son la visualización de información y la robótica. Por ejemplo, en el área de la robótica, es importante saber cómo rotar las uniones de un brazo robótico a fin de poder levantar o colocar un objeto determinado desde o en algún lugar, respectivamente.\n",
    "\n",
    "Consideremos la base canónica de $\\mathbb{R}^{2}$, la que, como sabemos, es $\\mathbf{e}(2)=\\left\\{ \\mathbf{e}_{1} ,\\mathbf{e}_{2} \\right\\}$, donde $\\mathbf{e}_{1}=(1,0)$ y $\\mathbf{e}_{2}=(0,1)$, y que define al sistema de coordenadas cartesianas típico que caracteriza al plano $\\mathbb{R}^{2}$. Queremos generar una rotación de este sistema de coordenadas en un ángulo $\\theta$, como se ilustra en la Fig. (2.10). Notemos que los vectores rotados en el nuevo sistema aún son linealmente independientes y, por tanto, constituyen un base de $\\mathbf{R}^{2}$. Por esta razón, las rotaciones implican igualmente cambios de base.\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"figures/fig_2_10.png\" width=\"600\"></p>\n",
    "<p style=\"text-align: center;\">Fig. (2.10): Rotación de un sistema de coordenadas rectagulares en el plano $\\mathbf{R}^{2}$ en un ángulo $\\theta$</p>\n",
    "\n",
    "Como dijimos previamente, las rotaciones son transformaciones lineales que podemos expresar por medio de una matriz de rotación, que denotamos como $\\mathbf{R}(\\theta)$. Las funciones trigonométricas (como se observa en la Fig. (2.10)) nos permiten determinar las coordenadas de los ejes rotados (la imagen de la respectiva transformación lineal) con respecto a la base canónica de $\\mathbb{R}^{2}$. Si llamamos $T$ a la función de rotación, obtenemos\n",
    "\n",
    "$$T\\left( \\mathbf{e}_{1} \\right)  =\\left( \\begin{matrix}\\cos \\left( \\theta \\right)  \\\\ \\mathrm{sen} \\left( \\theta \\right)  \\end{matrix} \\right)  \\wedge T\\left( \\mathbf{e}_{2} \\right)  =\\left( \\begin{matrix}-\\mathrm{sen} \\left( \\theta \\right)  \\\\ \\cos \\left( \\theta \\right)  \\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(2.73)$</p>\n",
    "\n",
    "Para disponer de un ángulo de rotación $\\theta$ significativo, tenemos que definir a lo que nos referimos con “antihorario” cuando operamos en más de dos dimensiones. Cuando usamos la convención de que una rotación antihoraria (planar), en general, tomamos como referencia la famosa regla de la mano derecha, que ilustramos en la Fig. (2.11).\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"figures/fig_2_11.png\" width=\"350\"></p>\n",
    "<p style=\"text-align: center;\">Fig. (2.11): La regla de la mano derecha</p>\n",
    "\n",
    "Por lo tanto, la matriz de rotación que aplica el cambio de base entre el sistema de coordenadas original (estándar) y el rotado está dada por\n",
    "\n",
    "$$\\mathbf{R} \\left( \\theta \\right)  =\\left( T\\left( \\mathbf{e}_{1} \\right)  ,T\\left( \\mathbf{e}_{2} \\right)  \\right)  =\\left( \\begin{array}{ll}\\cos \\left( \\theta \\right)  &-\\mathrm{sen} \\left( \\theta \\right)  \\\\ \\mathrm{sen} \\left( \\theta \\right)  &\\cos \\left( \\theta \\right)  \\end{array} \\right)$$\n",
    "<p style=\"text-align: right;\">$(2.74)$</p>\n",
    "\n",
    "**Ejemplo 2.14:** Vamos a determinar una expresión analítica para la transformación en $\\mathbb{R}^{2}$ definida como un giro del sistema de coordenadas con centro en $(1,1)$ y ángulo $\\theta=\\frac{\\pi}{2}$.\n",
    "\n",
    "Sea pues $f$ el giro especificado y $\\mathbf{R}$ su matriz de rotación asociada. Debido a que la rotación que queremos aplicar está definida en $\\mathbb{R}^{2}$, sabemos que $\\mathbf{R}$ en la base canónica está dada por\n",
    "\n",
    "$$\\mathbf{R} \\left( \\frac{\\pi }{2} \\right)  =\\left( \\begin{array}{ll}\\cos \\left( \\frac{\\pi }{2} \\right)  &-\\mathrm{sen} \\left( \\frac{\\pi }{2} \\right)  \\\\ \\mathrm{sen} \\left( \\frac{\\pi }{2} \\right)  &\\cos \\left( \\frac{\\pi }{2} \\right)  \\end{array} \\right)  =\\left( \\begin{matrix}0&-1\\\\ 1&0\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(2.75)$</p>\n",
    "\n",
    "La expresión analítica de $f$ es de la forma $f\\left( \\mathbf{x} \\right)  =\\mathbf{a} +\\mathbf{R} \\left( \\theta \\right)  \\mathbf{x}$, para todo $\\mathbf{x}\\in \\mathbb{R}^{2}$ y algún punto $\\mathbf{a}=(a_{1},a_{2})\\in \\mathbb{R}^2$ que debemos determinar en términos del centro de giro. De esta manera se tiene que \n",
    "\n",
    "$$f\\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  =\\left( \\begin{matrix}a_{1}\\\\ a_{2}\\end{matrix} \\right)  +\\left( \\begin{matrix}0&-1\\\\ 1&0\\end{matrix} \\right)  \\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(2.76)$</p>\n",
    "\n",
    "Donde el punto $\\mathbf{a}=(a_{1},a_{2})\\in \\mathbb{R}^2$ se calcula usando el punto $\\mathbf{x_{0}}$ respecto del cual se aplica la rotación. De esta manera, se tiene que\n",
    "\n",
    "$$\\mathbf{a} =f\\left( \\mathbf{x}_{0} \\right)  -\\mathbf{R} \\mathbf{x}_{0} \\Longleftrightarrow \\left( \\begin{matrix}a_{1}\\\\ a_{2}\\end{matrix} \\right)  =\\underbrace{f\\left( \\begin{matrix}1\\\\ 1\\end{matrix} \\right)  }_{=\\left( \\begin{matrix}1\\\\ 1\\end{matrix} \\right)  } -\\left( \\begin{matrix}0&-1\\\\ 1&0\\end{matrix} \\right)  \\left( \\begin{matrix}1\\\\ 1\\end{matrix} \\right)  =\\left( \\begin{matrix}1\\\\ 1\\end{matrix} \\right)  -\\left( \\begin{matrix}-1\\\\ 1\\end{matrix} \\right)  =\\left( \\begin{matrix}2\\\\ 0\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(2.77)$</p>\n",
    "\n",
    "Así que, sustituyendo en la expresión anterior, obtenemos\n",
    "\n",
    "$$f\\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)  +\\left( \\begin{matrix}2\\\\ 0\\end{matrix} \\right)  +\\left( \\begin{matrix}0&-1\\\\ 1&0\\end{matrix} \\right)  \\left( \\begin{matrix}x\\\\ y\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(2.78)$</p>\n",
    "\n",
    "La transformación lineal $f$, por tanto, puede tomar cualquier punto $(x,y)\\in \\mathbb{R}^{2}$ y rotarlo en 90º con respecto al punto $(1, 1)$. ◼︎"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3bce7a3d",
   "metadata": {},
   "source": [
    "### Rotaciones en $\\mathbb{R}^{3}$.\n",
    "En contraste a lo que ocurre en $\\mathbb{R}^{2}$, en $\\mathbb{R}^{3}$ podemos rotar cualquier plano bidimensional con respecto a un eje unidimensional. La forma más sencilla de definir una matriz general de rotación es mediante la especificación de cómo las imágenes relativas a la base canónica de $\\mathbb{R}^{3}$, a saber, $\\mathbf{e} \\left( 3\\right)  =\\left( \\mathbf{e}_{1} ,\\mathbf{e}_{2} ,\\mathbf{e}_{3} \\right)  =\\left\\{ \\left( 1,0,0\\right)  ,\\left( 0,1,0\\right)  ,\\left( 0,0,1\\right)  \\right\\}$ deben rotarse, garantizando que dichas imágenes conformen igualmente una base ortonormal. Combinando estas imágenes, podemos obtener la matriz de rotación general. \n",
    "\n",
    "Para tener un ángulo de rotación con sentido, tenemos igualmente que darle sentido al significado de la dirección de giro representada por dicho ángulo. Nuevamente nos basaremos en la regla de la mano derecha para ello, pero considerando los distintos ejes definidos a partir de la base ortonormal canónica de $\\mathbb{R}^{3}$. Tenemos, por tanto, tres posibles ejes de rotación:\n",
    "\n",
    "- Rotación con respecto al eje X (o vector $\\mathbf{e}_{1}$), la que se define mediante la matriz $\\mathbf{R}_{1} \\left( \\theta \\right)$ definida a continuación. Notemos que, para este tipo de rotaciones, la coordenada $\\mathbf{e}_{1}$ es fija, y conforme la regla de la mano derecha, es antihoraria cuando se realiza en el plano $\\mathbf{e}_{2}\\mathbf{e}_{3}$:\n",
    "\n",
    "$$\\mathbf{R}_{1} \\left( \\theta \\right)  =\\left( T\\left( \\mathbf{e}_{1} \\right)  ,T\\left( \\mathbf{e}_{2} \\right)  ,T\\left( \\mathbf{e}_{3} \\right)  \\right)  =\\left( \\begin{matrix}1&0&0\\\\ 0&\\cos \\left( \\theta \\right)  &-\\mathrm{sen} \\left( \\theta \\right)  \\\\ 0&\\mathrm{sen} \\left( \\theta \\right)  &\\cos \\left( \\theta \\right)  \\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(2.79)$</p>\n",
    "\n",
    "- Rotación con respecto al eje Y (o vector $\\mathbf{e}_{2}$), la que se define mediante la matriz $\\mathbf{R}_{2} \\left( \\theta \\right)$ definida a continuación. Notemos que, para este tipo de rotaciones, la coordenada $\\mathbf{e}_{2}$ es fija, y conforme la regla de la mano derecha, es antihoraria cuando se realiza en el plano $\\mathbf{e}_{1}\\mathbf{e}_{3}$:\n",
    "\n",
    "$$\\mathbf{R}_{2} \\left( \\theta \\right)  =\\left( \\begin{matrix}\\cos \\left( \\theta \\right)  &0&\\mathrm{sen} \\left( \\theta \\right)  \\\\ 0&1&0\\\\ -\\mathrm{sen} \\left( \\theta \\right)  &0&\\cos \\left( \\theta \\right)  \\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(2.80)$</p>\n",
    "\n",
    "- Rotación con respecto al eje Z (o vector $\\mathbf{e}_{3}$), la que se define mediante la matriz $\\mathbf{R}_{3} \\left( \\theta \\right)$ definida a continuación. Notemos que, para este tipo de rotaciones, la coordenada $\\mathbf{e}_{3}$ es fija, y conforme la regla de la mano derecha, es antihoraria cuando se realiza en el plano $\\mathbf{e}_{1}\\mathbf{e}_{2}$:\n",
    "\n",
    "$$\\mathbf{R}_{3} \\left( \\theta \\right)  =\\left( \\begin{matrix}\\cos \\left( \\theta \\right)  &-\\mathrm{sen} \\left( \\theta \\right)  &0\\\\ \\mathrm{sen} \\left( \\theta \\right)  &\\cos \\left( \\theta \\right)  &0\\\\ 0&0&1\\end{matrix} \\right)$$\n",
    "<p style=\"text-align: right;\">$(2.81)$</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b5e402",
   "metadata": {},
   "source": [
    "## Comentarios finales.\n",
    "En esta sección nos hemos abstraido de escribir código en favor de aprender una serie de conceptos esenciales en el estudio de los espacios vectoriales normados y sus interpretaciones (y aplicaciones) geométricas, los cuales nos servirán para definir métodos de gran utilidad en el estudio de problemas de reducción de dimensionalidad, como lo comentamos previamente. Tales problemas, como veremos más adelante, constituyen uno de los sujetos centrales de los modelos de aprendizaje no supervisado, y motivan la necesidad de desarrollar metodologías que permitan *comprimir* un conjunto de datos de alta dimensión en otro con una menor cantidad de variables, pero sin perder una cantidad innecesaria de información. Para caracterizar qué información *perdemos* con estas transformaciones, será necesario desarrollar otros conceptos relativos al campo de la estadística y probabilidades. Pero previo a eso, debemos enfocarnos en cuestiones un tanto más elementales... ¡Pero tiempo al tiempo!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
