# PARTE 1: Fundamentos matemáticos elementales.
---

## Índice de contenidos.
- [**CLASE 1.1:** Un repaso (consciente) de álgebra lineal.](https://github.com/rquezadac/udd_data_science_lectures/blob/main/PARTE%20I%20-%20Fundamentos%20matem%C3%A1ticos%20elementales/clase_1_1.ipynb)
    - 1.1 Introducción.
    - 1.2 Sistemas de ecuaciones lineales.
    - 1.3 Matrices.
        - 1.3.1 Adición y multiplicación de matrices.
        - 1.3.2 Matriz inversa y transpuesta.
        - 1.3.3 Multiplicación de una matriz por un escalar.
    - 1.4 Solución de un sistema lineal de ecuaciones.
        - 1.4.1 Representación matricial de un sistema lineal de ecuaciones.
        - 1.4.2 Transformaciones elementales sobre una matriz.
        - 1.4.3 Método de eliminación Gaussiana.
    - 1.5 Espacios vectoriales.
        - 1.5.1 Grupos.
        - 1.5.2 Definición de un espacio vectorial.
        - 1.5.3 Subespacios.
    - 1.6 Independencia lineal.
        - 1.6.1 Generadores.
        - 1.6.2 Sistemas de generadores.
        - 1.6.3 Definición de independencia lineal.
    - 1.7 Base y dimensión.
    - 1.8 Transformaciones lineales.
        - 1.8.1 Propiedades de las transformaciones lineales.
        - 1.8.2 Teorema de la dimensión.
        - 1.8.3 Representación matricial de las transformaciones lineales.
    - 1.9 Comentarios.

- [**CLASE 1.2:** Espacios vectoriales con producto interno.](https://github.com/rquezadac/udd_data_science_lectures/blob/main/PARTE%20I%20-%20Fundamentos%20matem%C3%A1ticos%20elementales/clase_1_2.ipynb)
    - 2.1 Introducción.
    - 2.2 Producto interno.
    - 2.3 Longitud y distancia.
    - 2.4 Ortogonalidad.
        - 2.4.1 Ángulo entre vectores.
        - 2.4.2 Bases ortogonales.
        - 2.4.3 Proceso de ortogonalización de Gram-Schmidt.
    - 2.5 Proyecciones.
        - 2.5.1 Proyección sobre espacios unidimensionales.
        - 2.5.2 Proyección sobre subespacios más generales.
    - 2.6 Rotaciones.
        - 2.6.1 Rotaciones en $\mathbb{R}^{2}$
        - 2.6.2 Rotaciones en $\mathbb{R}^{3}$
    - 2.7 Comentarios.

 - [**CLASE 1.3:** Descomposiciones matriciales.](https://github.com/rquezadac/udd_data_science_lectures/blob/main/PARTE%20I%20-%20Fundamentos%20matem%C3%A1ticos%20elementales/clase_1_3.ipynb)
    - 3.1 Introducción.
    - 3.2 Determinantes.
        - 3.2.1 Un interludio previo.
        - 3.2.2 Interpretación geométrica del determinante.
        - 3.2.3 Propiedades de los determinantes.
    - 3.3 Diagonalización de matrices.
        - 3.3.1 Autovalores y autovectores.
        - 3.3.2 Espectro de una matriz simétrica.
    - 3.4 Una introducción a las formas.
        - 3.4.1 Formas lineales.
        - 3.4.2 Formas bilineales.
        - 3.4.3 Formas cuadráticas.
        - 3.4.4 Una aplicación de las formas a la geometría analítica.
    - 3.5 Descomposición de Cholesky.
    - 3.6 Descomposición QR.
    - 3.7 Descomposición en valores singulares.
        - 3.7.1 Una primera explicación geométrica.
        - 3.7.2 Construcción de la descomposición.
        - 3.7.3 Comparación con la diagonalización.
    - 3.8 Aproximaciones matriciales.
    - 3.9 Comentarios.

- [**CLASE 1.4:** Un repaso (consciente) de cálculo diferencial.](https://github.com/rquezadac/udd_data_science_lectures/blob/main/PARTE%20I%20-%20Fundamentos%20matem%C3%A1ticos%20elementales/clase_1_4.ipynb)
    - 4.1 Introducción.
    - 4.2 Diferenciación de funciones de una variable.
        - 4.2.1 Álgebra de derivadas.
        - 4.2.2 Regla de la cadena.
        - 4.2.3 Series de Taylor.
    - 4.3 Derivadas parciales.
    - 4.4 Campo gradiente.
        - 4.4.1 Gradiente de un campo escalar.
        - 4.4.2 Regla de la cadena para campos escalares.
        - 4.4.3 Gradiente de un campo vectorial.
        - 4.4.4 Gradiente de funciones matriciales.
        - 4.4.5 Algunos gradientes usuales en machine learning.
    - 4.5 Retropropagación.
        - 4.5.1 El concepto de red neuronal artificial.
            - 4.5.1.1 Modelo de McCulloch & Pitts.
            - 4.5.1.2 Perceptrón de Rosenblatt.
            - 4.5.1.3 Perceptrón multicapa.
        - 4.5.2 Gradientes en una red neuronal profunda.
    - 4.6 Diferenciación automática.
        - 4.6.1 Acumulación progresiva.
        - 4.6.2 Acumulación regresiva.
    - 4.7 Matriz Hessiana.
    - 4.8 Serie multivariable de Taylor.
    - 4.9 Comentarios.

- [**CLASE 1.5:** Una introducción (generosa) al cálculo de probabilidades.](https://github.com/rquezadac/udd_data_science_lectures/blob/main/PARTE%20I%20-%20Fundamentos%20matem%C3%A1ticos%20elementales/clase_1_5.ipynb)
    - 5.1 Introducción.
    - 5.2 Teoría clásica de probabilidad.
        - 5.2.1 El concepto de probabilidad.
        - 5.2.2 Asignación de probabilidades.
        - 5.2.3 Probabilidad condicional.
    - 5.3 Teoría moderna de probabilidad.
        - 5.3.1 Funciones de conjunto con aditividad finita.
        - 5.3.2 Medidas con aditividad finita.
        - 5.3.3 Definición de probabilidad.
    - 5.4 Experimentos o pruebas compuestas.
        - 5.4.1 Pruebas de Bernoulli.
        - 5.4.2 Número más probable de éxitos en $n$ pruebas de Bernoulli.
    - 5.5 Conjuntos numerables y no numerables.
        - 5.5.1 Definición de probabilidad para espacios muestrales infinitos numerables.
        - 5.5.2 Definición de probabilidad para espacios muestrales infinitos no numerables.
        - 5.5.3 Numerabilidad de un conjunto de puntos con probabilidad positiva.
    - 5.6 Variables aleatorias.
    - 5.7 Funciones de distribución.
        - 5.7.1 Distribuciones discretas.
        - 5.7.2 Distribuciones continuas.
    - 5.8 Valor esperado de una variable aleatoria.
    - 5.9 Varianza de una variable aleatoria.
    - 5.10 Distribuciones multivariables.
    - 5.11 Distribuciones condicionales.
    - 5.12 Fórmula de Bayes.
    - 5.13 Esperanza y varianza para variables aleatorias multidimensionales.
    - 5.14 Transformación de variables aleatorias.
    - 5.15 Producto interno de variables aleatorias.
    - 5.16 La distribución normal.
        - 5.16.1 Producto de distribuciones Gaussianas.
        - 5.16.2 Transformaciones lineales entre variables aleatorias Gaussianas.
        - 5.16.3 Muestreo desde distribuciones Gaussianas multivariantes.
    - 5.17 La familia exponencial de distribuciones.
        - 5.17.1 Distribuciones conjugadas.
        - 5.17.2 Estadígrafos suficientes.
        - 5.17.3 Distribuciones exponenciales.
    - 5.18 Transformaciones inversas.
        - 5.18.1 Técnica basada en la función de distribución.
        - 5.18.2 Cambio de variables.
    - 5.19 Desigualdad de Chebyshev.
    - 5.20 Ley de los grandes números.
    - 5.21 Teorema central del límite.
    - 5.22 Comentarios.

- [**CLASE 1.6:** Un repaso (generoso) de la teoría de optimización.](https://github.com/rquezadac/udd_data_science_lectures/blob/main/PARTE%20I%20-%20Fundamentos%20matem%C3%A1ticos%20elementales/clase_1_6.ipynb)
    - 6.1 Introducción.
    - 6.2 Algoritmo de gradiente descendente.
        - 6.2.1 Tasa de aprendizaje.
        - 6.2.2 Gradiente descendente con moméntum.
        - 6.2.3 Gradiente descendente estocástico.
    - 6.3 Optimización con restricciones.
    - 6.4 Optimización de funciones convexas.
        - 6.4.1 Programación lineal.
        - 6.4.2 Programación cuadrática.
        - 6.4.3 Función conjugada convexa.
    - 6.5 Comentarios.

- [**CLASE 1.7:** Una (no tan breve) introducción a la teoría de grafos.](https://github.com/rquezadac/udd_data_science_lectures/blob/main/PARTE%20I%20-%20Fundamentos%20matem%C3%A1ticos%20elementales/clase_1_7.ipynb)
    - 7.1 Introducción.
    - 7.2 Elementos básicos de un grafo.
    - 7.3 Potencias de la matriz de adyacencia de un grafo.
    - 7.4 Teorema de Perron-Frobenius.
    - 7.5 Autovectores *derechos* de las matrices de un grafo.
        - 7.5.1 Un ejemplo: Agrupamiento espectral.
            - 7.5.1.1 Relación entre los encajes de Shi-Malik y Ng-Jordan.
        - 7.5.2 Perspectiva Laplaciana del agrupamiento espectral.
            - 7.5.2.1 Laplaciano de un grafo.
            - 7.5.2.2 Modelo de optimización Laplaciano.
        - 7.5.3 Perspectiva matricial del agrupamiento espectral.
            - 7.5.3.1 Predicción de la dirección de un enlace.
        - 7.5.4 Una perspectiva informativa del agrupamiento espectral.
    - 7.6 Autovectores *izquierdos* de las matrices de un grafo.
        - 7.6.1 Algoritmo PageRank.
        - 7.6.2 Métricas relacionadas de prestigio y centralización.
        - 7.6.3 Aplicación de los autovectores izquierdos en la predicción de dirección de un enlace.
    - 7.7 Autovectores de matrices reductibles.
        - 7.7.1 Caso I: Grafos no dirigidos.
        - 7.7.2 Caso II: Grafos dirigidos.
    - 7.8 Algunas aplicaciones en el contexto del aprendizaje automático.
        - 7.8.1 Clasificación de vértices.
        - 7.8.2 Conjuntos de datos multidimensionales.
    - 7.9 Comentarios.

- [**CLASE 1.8:** Una introducción (generosa) a la teoría de la información.](https://github.com/rquezadac/udd_data_science_lectures/blob/main/PARTE%20I%20-%20Fundamentos%20matem%C3%A1ticos%20elementales/clase_1_8.ipynb)
    - 8.1 Introducción.
    - 8.2 El concepto de entropía (de la información).
        - 8.2.1 Entropía de una variable aleatoria discreta.
        - 8.2.2 Entropía diferencial de variables aleatorias continuas.
    - 8.3 Principio de máxima entropía.
    - 8.4 Información mutua.
    - 8.5 Divergencia de Kullback-Leibler.
        - 8.5.1 Relación de la divergencia KL con la información mutua.
    - 8.6 Entropía cruzada.
    - 8.7 Perplejidad.
    - 8.8 El problema de optimización de la información mutua.
    - 8.9 Principio de la máxima información mutua.
    - 8.10 Infomax y reducción de redundancia.
        - 8.10.1 Modelamento de un sistema perceptual.
    - 8.11 Variables espacialmente coherentes.
    - 8.12 Variables espacialmente incoherentes.
    - 8.13 Comentarios.

---

## Bibliografía y referencias.
- Aggarwal, Ch. (2020). Linear Algebra and Optimization for Machine Learning --> Este es un libro maravilloso y lleno de detalles para introducirnos en la teoría matemática que conforma la base de los algoritmos de aprendizaje. Se trata de un libro *no tan fácil de digerir* como otros del mismo campo, pero contiene material extremadamente valioso que suele pasarse por alto en textos más amables con el lector nóvel (como la teoría de grafos). Es un tremendo recurso y recomiendo a cualquier practicante del modelamiento predictivo mediante algoritmos de machine learning que lo tenga en su biblioteca.
- Apostol, T. M. (1991). Calculus, volume 1. John Wiley & Sons.
- Baeza, R. S. (2003). Algebra: álgebra lineal: apuntes. Universidad de Santiago de Chile, Departamento de Matemática y Ciencia de la Computación.
- Deisenroth, M. P., Faisal, A. A., & Ong, C. S. (2020). Mathematics for Machine Learning --> Este libro es un *must-have* para cualquier entusiasta del modelamiento predictivo mediante algoritmos de aprendizaje, ya que, mediante un lenguaje sencillo, sienta las bases teóricas necesarias para poder desarrollar más adelante los conceptos necesarios para comprender a cabalidad todo lo relativo a los modelos de machine learning. El índice que hemos construido para estos apuntes sigue su estructura básica y varias de las secciones tratadas en el repositorio se han traducido (espero que lo más fielmente posible) de este grandioso texto.
- Géron, A. (2022). Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow. " O'Reilly Media, Inc.".
- Ruiz, C. P., & de Jesús, C. (1995). Cálculo vectorial. Prentice-Hall Hispanoamericana.
- VanderPlas, J. (2016). Python data science handbook: Essential tools for working with data. " O'Reilly Media, Inc.".
- https://github.com/ageron/handson-ml3 --> El respositorio online del libro *Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow* del gran Aurélien Geron (tercera edición). Si bien este libro está orientado a construir un *background* de tipo más práctico y orientado al código, posee recursos de aprendizaje teóricos muy potentes y que siempre valdrá la pena tener en consideración.