# PARTE 2: Algoritmos de aprendizaje supervisado.
---

## Índice de contenidos.
- [**CLASE 2.1:** Una introducción (sencilla) a los algoritmos de aprendizaje.](https://github.com/rquezadac/udd_data_science_lectures/blob/main/PARTE%20II%20-%20Modelos%20de%20aprendizaje%20supervisado/clase_2_1.ipynb)
    - 1.1 Motivación.
    - 1.2 Algoritmos de aprendizaje.
        - 1.2.1 La tarea o problema, $T$.
        - 1.2.2 La métrica de desempeño, $P$.
        - 1.2.3 La experiencia, $E$.
    - 1.3 Principio de minimización del riesgo empírico.
        - 1.3.1 Funciones permisibles como hipótesis.
        - 1.3.2 Función de costo.
        - 1.3.3 Regularización y overfitting.
        - 1.3.4 Validación cruzada.
    - 1.4 Estimación de parámetros.
        - 1.4.1 Estimación de máxima verosimilitud (MLE).
        - 1.4.2 Estimación máxima a posteriori (MAP).
        - 1.4.3 Ajuste de modelos.
    - 1.5 Modelos probabilísticos.
        - 1.5.1 Conceptualización.
        - 1.5.2 Inferencia Bayesiana.
        - 1.5.3 Modelos con *variables latentes*.
    - 1.6 Una (breve) introducción a la teoría algebraica de grafos.
        - 1.6.1 Algunos principios semánticos.
        - 1.6.2 Independencia condicional.
    - 1.7 Una introducción teórica a la selección de modelos.
        - 1.7.1 Validación cruzada anidada.
        - 1.7.2 Selección Bayesiana.
        - 1.7.3 Factor de Bayes.
    - 1.8 Comentarios.

- [**CLASE 2.2:** Una introducción a la librería **Scikit-Learn**.](https://github.com/rquezadac/udd_data_science_lectures/blob/main/PARTE%20II%20-%20Modelos%20de%20aprendizaje%20supervisado/clase_2_2.ipynb)
    - 2.1 Introducción.
    - 2.2 Representación de la data en **Scikit-Learn**.
        - 2.2.1 Data con formato de tabla.
        - 2.2.2 Matriz de atributos (variables independientes).
        - 2.2.3 Arreglo de valores objetivo o etiquetas.
    - 2.3 API estimadora de **Scikit-Learn**.
        - 2.3.1 Elementos básicos de la API.
        - 2.3.2 Algunos ejemplos sencillos.
    - 2.4 Hiperparámetros y validación.
        - 2.4.1 Lo que no debemos hacer...
        - 2.4.2 El conjunto de validación.
        - 2.4.3 Implementación (básica) de la validación cruzada.
    - 2.5 Una introducción práctica a la selección de modelos.
        - 2.5.1 El trade-off entre varianza y sesgo.
        - 2.5.2 Curvas de aprendizaje.
        - 2.5.3 Optimización de hiperparámetros por medio de grillas.
    - 2.6 Una (breve) introducción al pre-procesamiento de datos categóricos.
    - 2.7 Comentarios.

- [**CLASE 2.3:** Modelos de clasificación.](https://github.com/rquezadac/udd_data_science_lectures/blob/main/PARTE%20II%20-%20Modelos%20de%20aprendizaje%20supervisado/clase_2_3.ipynb)
    - 3.1 Introducción.
    - 3.2 El problema.
    - 3.3 Función de costo.
        - 3.3.1 El problema de usar funciones de costo aptas para problemas de regresión.
        - 3.3.2 Función de entropía cruzada.
    - 3.4 El algoritmo de gradiente descendente.
    - 3.5 Muestreo estratificado.
    - 3.6 Implementación del algoritmo de gradiente descendente en **Scikit-Learn**.
    - 3.7 Métricas de desempeño.
        - 3.7.1 La matriz de confusión.
        - 3.7.2 El efecto de la validación cruzada.
        - 3.7.3 Exactitud.
        - 3.7.4 Precisión y sensibilidad.
        - 3.7.5 El trade-off entre precisión y sensibilidad.
        - 3.7.6 Curva característica relativa - operativa (ROC).
    - 3.8 Modelo de regresión logística binaria.
    - 3.9 Caso de estudio I: Predicción de colapsos.
    - 3.9 Clasificación multinomial.
        - 3.9.1 La función softmax.
        - 3.9.2 Función de costo para el caso multinomial.
        - 3.9.1 Estrategias *one-versus-one* y *ove-versus-rest*.
        - 3.9.2 Análisis del error.
            - 3.9.2.1 Matriz de confusión generalizada.
            - 3.9.2.2 Curvas ROC para problemas multinomiales.
        - 3.9.3 Fronteras de decisión.
    - 3.10 Caso de estudio II: Clasificación de rocas a partir de análisis de trazas de tierras raras.
    - 3.11 Comentarios.

- [**CLASE 2.4:** Regresión.](https://github.com/rquezadac/udd_data_science_lectures/blob/main/PARTE%20II%20-%20Modelos%20de%20aprendizaje%20supervisado/clase_2_4.ipynb)
    - 4.1 Introducción.
    - 4.2 Regresión lineal.
        - 4.2.1 Descripción del problema.
        - 4.2.2 Estimación por mínimos cuadrados.
            - 4.2.2.1 Formulación.
            - 4.2.2.2 Ecuación normal.
        - 4.2.3 Solución aproximada por medio del algoritmo de gradiente descendente.
        - 4.2.4 Gradiente descendente estocástico.
        - 4.2.5 Implementación del algoritmo de GD usando *mini-batches*.
        - 4.2.6 Implementaciones del modelo de regresión lineal en **Scikit-Learn** (Parte I).
        - 4.2.6 Estimación por máxima verosimilitud.
            - 4.2.6.1 Formulación.
            - 4.5.6.2 Estimación de parámetros.
            - 4.2.6.3 Extensión a problemas no lineales por medio de transformaciones.
            - 4.2.6.4 Estimación de la varianza del ruido.
            - 4.2.6.5 Un breve comentario acerca del *overfitting*.
    - 4.4 Métricas de desempeño.
    - 4.5 Modelos lineales regularizados.
        - 4.5.1 El modelo de tipo *"ridge"*.
        - 4.5.2 Estimación máxima a posteriori.
        - 4.5.4 El modelo de tipo *LASSO*.
        - 4.5.4 El modelo de red elástica.
    - 4.6 Un enfoque Bayesiano para la regresión lineal.
        - 4.6.1 Formulación.
        - 4.6.2 Obtención de predicciones (a priori).
        - 4.6.3 Distribución a posteriori.
        - 4.6.4 Obtención de predicciones (a posteriori).
        - 4.6.5 Implementaciones más avanzadas.
    - 4.7 Modelos lineales generalizados.
        - 4.7.1 Formulación.
        - 4.7.2 Distribución de probabilidad de la variable de respuesta.
        - 4.7.3 El modelo de Poisson.
    - 4.8 Comentarios.

- [**CLASE 2.5:** Máquinas de soporte vectorial](https://github.com/rquezadac/udd_data_science_lectures/blob/main/PARTE%20II%20-%20Modelos%20de%20aprendizaje%20supervisado/clase_2_5.ipynb).
    - 5.1 Introducción.
    - 5.2 El hiperplano de separación.
    - 5.3 Formulación del problema primal.
        - 5.3.1 El concepto de margen.
        - 5.3.2 Clasificación con márgenes rígidos.
        - 5.3.3 Una justificación (razonable) de por qué $d=1$.
        - 5.3.4 Clasificación con márgenes blandos.
        - 5.3.5 Función de costo de Hinge.
    - 5.4 Formulación del problema dual.
        - 5.4.1 Problema dual vía multiplicadores de Lagrange.
        - 5.4.2 Un enfoque basado en el concepto de envolvente convexa.
    - 5.5 Problemas no linealmente separables.
        - 5.5.1 Implementación de funciones kernel.
        - 5.5.2 Truco del kernel.
    - 5.6 Formulación estándar de una máquina de soporte vectorial como un programa cuadrático.
        - 5.6.1 Problema dual.
        - 5.6.2 Problema primal.
    - 5.7 Ejemplos de implementación en **Scikit-Learn**.
    - 5.8 Aplicación a problemas de regresión.
    - 5.9 Comentarios.

- [**CLASE 2.6:** Árboles de decisión](https://github.com/rquezadac/udd_data_science_lectures/blob/main/PARTE%20II%20-%20Modelos%20de%20aprendizaje%20supervisado/clase_2_6.ipynb).
    - 6.1 Introducción.
    - 6.2 Modelos con estructura de tipo árbol.
    - 6.3 Inducción de árboles de decisión.
    - 6.4 Reglas de asignación.
        - 6.4.1 Problema de clasificación.
        - 6.4.2 Problema de regresión.
    - 6.5 Criterios de detención.
    - 6.6 Reglas de *splitting*.
        - 6.6.1 Familias $\mathcal{P}$ de reglas de *splitting*.
        - 6.6.2 Bondad o idoneidad de un *split*.
            - 6.6.2.1 Problema de clasificación.
            - 6.6.2.2 Problema de regresión.
        - 6.6.3 Determinación del mejor *split* binario.
            - 6.6.3.1 Caso de variables ordinales.
            - 6.6.3.2 Caso de variables categóricas.
    - 6.7 Ejemplos de implementación en **Scikit-Learn**.
    - 6.8 Comentarios.

- [**CLASE 2.7:** Métodos de ensamble](https://github.com/rquezadac/udd_data_science_lectures/blob/main/PARTE%20II%20-%20Modelos%20de%20aprendizaje%20supervisado/clase_2_6.ipynb).
    - 7.1 Introducción.
    - 7.2 Voting.
    - 7.3 Bagging.
        - 7.3.1 Intuición.
        - 7.3.2 Algunos paradigmas de "ensamblaje".
        - 7.3.3 Algoritmo de bagging.
        - 7.3.3 Aspectos teóricos.
        - 7.3.4 Modelo de bosque aleatorizado o *random forest*.
        - 7.3.5 Espectro de aleatorización.
    - 7.4 Boosting.
        - 7.4.1 Procedimiento general.
        - 7.4.2 Algoritmo AdaBoost.
        - 7.4.3 Aspectos teóricos.
            - 7.4.3.1 Análisis inicial.
            - 7.4.3.2 Un explicación relativa a los márgenes.
            - 7.4.3.3 Un punto de vista estadístico.
        - 7.4.4 Extensión a problemas multinomiales.
        - 7.4.5 Tolerancia al ruido.


---

## Bibliografía y referencias.
- Apostol, T. M. (1991). Calculus, volume 1. John Wiley & Sons.
- Baeza, R. S. (2003). Algebra: álgebra lineal: apuntes. Universidad de Santiago de Chile, Departamento de Matemática y Ciencia de la Computación.
- Boyd, S. & Vandenberghe, L. (2004). Convex optimization. Cambridge university press --> Este libro es un *must-have* para cualquier entusiasta de las matemáticas que esté interesado en aprender y/o profundizar en la teoría de optimización convexa, con aplicaciones en todo tipo de problemas (con especial cuidado en problemas geométricos y estadísticos).
- Deisenroth, M. P., Faisal, A. A., & Ong, C. S. (2020). Mathematics for Machine Learning --> Este libro es un *must-have* para cualquier entusiasta del modelamiento predictivo mediante algoritmos de aprendizaje, ya que, mediante un lenguaje sencillo, sienta las bases teóricas necesarias para poder desarrollar más adelante los conceptos necesarios para comprender a cabalidad todo lo relativo a los modelos de machine learning. El índice que hemos construido para estos apuntes sigue su estructura básica y varias de las secciones tratadas en el repositorio se han traducido (espero que lo más fielmente posible) de este grandioso texto.
- Géron, A. (2022). Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow. " O'Reilly Media, Inc." --> Este libro es un *must-have* para cualquier entusiasta de la ciencia de datos, ya que es una excelente manera de iniciar nuestro viaje desde una perspectiva completamente práctica en Python, haciendo uso intensivo de la librería **Scikit-Learn**. Su repositorio en Github es también una forma maravillosa de aprender algunos temas avanzados relativos a gráficos, que siempre es útil cuando se trata de analizar datos.
- Harrison, M. (2023). Effective XGBoost. Tunning, Understanding and Deploying Classification Models. "Hairysun, Treading on Python Series". --> Este libro es un *must-have* para cualquier entusiasta del modelamiento predictivo y del aprendizaje automático interesado en la práctica e implementación de modelos de tipo árbol de decisión y ensambles completos.
- Ruiz, C. P., & de Jesús, C. (1995). Cálculo vectorial. Prentice-Hall Hispanoamericana.
- VanderPlas, J. (2016). Python data science handbook: Essential tools for working with data. " O'Reilly Media, Inc.".
- https://github.com/ageron/handson-ml3 --> El respositorio online del libro Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow del gran Aurélien Geron (tercera edición). Si bien este libro está orientado a construir un *background* de tipo más práctico y orientado al código, posee recursos de aprendizaje teóricos muy potentes y que siempre valdrá la pena tener en consideración.
- https://scikit-learn.org/stable/user_guide.html --> La documentación de **Scikit-Learn**. Se trata de un recurso maravilloso, puesto que es muy concisa en relación a lo que necesitamos saber antes de implementar rápidamente cualquier solución basada en algoritmos de aprendizaje.