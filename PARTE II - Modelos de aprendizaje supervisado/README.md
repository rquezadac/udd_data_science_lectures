# PARTE 2: Algoritmos de aprendizaje supervisado.
---

## Índice de contenidos.
- [**CLASE 2.1:** Una introducción (sencilla) a los algoritmos de aprendizaje.](https://github.com/rquezadac/udd_data_science_lectures/blob/main/PARTE%20II%20-%20Modelos%20de%20aprendizaje%20supervisado/clase_2_1.ipynb)
    - 1.1 Motivación.
    - 1.2 Algoritmos de aprendizaje.
        - 1.2.1 La tarea o problema, $T$.
        - 1.2.2 La métrica de desempeño, $P$.
        - 1.2.3 La experiencia, $E$.
    - 1.3 Principio de minimización del riesgo empírico.
        - 1.3.1 Funciones permisibles como hipótesis.
        - 1.3.2 Función de costo.
        - 1.3.3 Regularización y overfitting.
        - 1.3.4 Validación cruzada.
    - 1.4 Estimación de parámetros.
        - 1.4.1 Estimación de máxima verosimilitud (MLE).
        - 1.4.2 Estimación máxima a posteriori (MAP).
        - 1.4.3 Ajuste de modelos.
    - 1.5 Modelos probabilísticos.
        - 1.5.1 Conceptualización.
        - 1.5.2 Inferencia Bayesiana.
        - 1.5.3 Modelos con *variables latentes*.
    - 1.6 Una (breve) introducción a la teoría algebraica de grafos.
        - 1.6.1 Algunos principios semánticos.
        - 1.6.2 Independencia condicional.
    - 1.7 Una introducción teórica a la selección de modelos.
        - 1.7.1 Validación cruzada anidada.
        - 1.7.2 Selección Bayesiana.
        - 1.7.3 Factor de Bayes.
    - 1.8 Comentarios.

- [**CLASE 2.2:** Una introducción a la librería **Scikit-Learn**.](https://github.com/rquezadac/udd_data_science_lectures/blob/main/PARTE%20II%20-%20Modelos%20de%20aprendizaje%20supervisado/clase_2_2.ipynb)
    - 2.1 Introducción.
    - 2.2 Representación de la data en **Scikit-Learn**.
        - 2.2.1 Data con formato de tabla.
        - 2.2.2 Matriz de atributos (variables independientes).
        - 2.2.3 Arreglo de valores objetivo o etiquetas.
    - 2.3 API estimadora de **Scikit-Learn**.
        - 2.3.1 Elementos básicos de la API.
        - 2.3.2 Algunos ejemplos sencillos.
    - 2.4 Hiperparámetros y validación.
        - 2.4.1 Lo que no debemos hacer...
        - 2.4.2 El conjunto de validación.
        - 2.4.3 Implementación (básica) de la validación cruzada.
    - 2.5 Una introducción práctica a la selección de modelos.
        - 2.5.1 El trade-off entre varianza y sesgo.
        - 2.5.2 Curvas de aprendizaje.
        - 2.5.3 Optimización de hiperparámetros por medio de grillas.
    - 2.6 Una (breve) introducción al pre-procesamiento de datos categóricos.
    - 2.7 Comentarios.

- [**CLASE 2.3:** Modelos de clasificación.](https://github.com/rquezadac/udd_data_science_lectures/blob/main/PARTE%20II%20-%20Modelos%20de%20aprendizaje%20supervisado/clase_2_3.ipynb)
    - 3.1 Introducción.
    - 3.2 El problema.
    - 3.3 Función de costo.
        - 3.3.1 El problema de usar funciones de costo aptas para problemas de regresión.
        - 3.3.2 Función de entropía cruzada.
    - 3.4 El algoritmo de gradiente descendente.
    - 3.5 Muestreo estratificado.
    - 3.6 Implementación del algoritmo de gradiente descendente en **Scikit-Learn**.
    - 3.7 Métricas de desempeño.
        - 3.7.1 La matriz de confusión.
        - 3.7.2 El efecto de la validación cruzada.
        - 3.7.3 Exactitud.
        - 3.7.4 Precisión y sensibilidad.
        - 3.7.5 El trade-off entre precisión y sensibilidad.
        - 3.7.6 Curva característica relativa - operativa (ROC).
    - 3.8 Modelo de regresión logística binaria.
    - 3.9 Caso de estudio I: Predicción de colapsos.
    - 3.9 Clasificación multinomial.
        - 3.9.1 La función softmax.
        - 3.9.2 Función de costo para el caso multinomial.
        - 3.9.1 Estrategias *one-versus-one* y *ove-versus-rest*.
        - 3.9.2 Análisis del error.
            - 3.9.2.1 Matriz de confusión generalizada.
            - 3.9.2.2 Curvas ROC para problemas multinomiales.
        - 3.9.3 Fronteras de decisión.
    - 3.10 Caso de estudio II: Clasificación de rocas a partir de análisis de trazas de tierras raras.

- [**CLASE 2.4:** Regresión.](https://github.com/rquezadac/udd_data_science_lectures/blob/main/PARTE%20II%20-%20Modelos%20de%20aprendizaje%20supervisado/clase_2_4.ipynb)
    - 4.1 Introducción.
    - 4.2 Regresión lineal.
        - 4.2.1 Descripción del problema.
        - 4.2.2 Estimación por mínimos cuadrados.
            - 4.2.2.1 Formulación.
            - 4.2.2.2 Ecuación normal.
        - 4.2.3 Solución aproximada por medio del algoritmo de gradiente descendente.
        - 4.2.4 Gradiente descendente estocástico.
        - 4.2.5 Implementación del algoritmo de GD usando *mini-batches*.
        - 4.2.6 Implementaciones del modelo de regresión lineal en **Scikit-Learn** (Parte I).
        - 4.2.6 Estimación por máxima verosimilitud.
            - 4.2.6.1 Formulación.
            - 4.5.6.2 Estimación de parámetros.
            - 4.2.6.3 Extensión a problemas no lineales por medio de transformaciones.
            - 4.2.6.4 Estimación de la varianza del ruido.
            - 4.2.6.5 Un breve comentario acerca del *overfitting*.
    - 4.4 Métricas de desempeño.
    - 4.5 Modelos lineales regularizados.
        - 4.5.1 El modelo de tipo *"ridge"*.
        - 4.5.2 Estimación máxima a posteriori.
        - 4.5.4 El modelo de tipo *LASSO*.
        - 4.5.4 El modelo de tipo elástico neto.
    - 4.6 Un enfoque Bayesiano para la regresión lineal.
        - 4.6.1 Formulación.
        - 4.6.2 Predicciones *a priori*.

- [**CLASE 2.5:** Máquinas de soporte vectorial](https://github.com/rquezadac/udd_data_science_lectures/blob/main/PARTE%20II%20-%20Modelos%20de%20aprendizaje%20supervisado/clase_2_5.ipynb).
    - 5.1 Introducción.
    - 5.2 El hiperplano de separación.
    - 5.3 Formulación del problema primal.
        - 5.3.1 El concepto de margen.
        - 5.3.2 Clasificación con márgenes rígidos.
        - 5.3.3 Una justificación (razonable) de por qué $d=1$.
        - 5.3.4 Clasificación con márgenes blandos.
        - 5.3.5 Función de costo de Hinge.
    - 5.4 Formulación del problema dual.
        - 5.4.1 Problema dual vía multiplicadores de Lagrange.
        - 5.4.2 Un enfoque basado en el concepto de envolvente convexa.
    - 5.5 Problemas no linealmente separables.
        - 5.5.1 Implementación de funciones kernel.
        - 5.5.2 Truco del kernel.
    - 5.6 Formulación de una máquina de soporte vectorial como un programa cuadrático.

---

## Bibliografía y referencias.
- Apostol, T. M. (1991). Calculus, volume 1. John Wiley & Sons.
- Baeza, R. S. (2003). Algebra: álgebra lineal: apuntes. Universidad de Santiago de Chile, Departamento de Matemática y Ciencia de la Computación.
- Deisenroth, M. P., Faisal, A. A., & Ong, C. S. (2020). Mathematics for Machine Learning --> Este libro es un *must-have* para cualquier entusiasta del modelamiento predictivo mediante algoritmos de aprendizaje, ya que, mediante un lenguaje sencillo, sienta las bases teóricas necesarias para poder desarrollar más adelante los conceptos necesarios para comprender a cabalidad todo lo relativo a los modelos de machine learning. El índice que hemos construido para estos apuntes sigue su estructura básica y varias de las secciones tratadas en el repositorio se han traducido (espero que lo más fielmente posible) de este grandioso texto.
- Géron, A. (2022). Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow. " O'Reilly Media, Inc." --> Este libro es un *must-have* para cualquier entusiasta de la ciencia de datos, ya que es una excelente manera de iniciar nuestro viaje desde una perspectiva completamente práctica en Python, haciendo uso intensivo de la librería **Scikit-Learn**. Su repositorio en Github es también una forma maravillosa de aprender algunos temas avanzados relativos a gráficos, que siempre es útil cuando se trata de analizar datos.
- Ruiz, C. P., & de Jesús, C. (1995). Cálculo vectorial. Prentice-Hall Hispanoamericana.
- VanderPlas, J. (2016). Python data science handbook: Essential tools for working with data. " O'Reilly Media, Inc.".
- https://github.com/ageron/handson-ml3 --> El respositorio online del libro Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow del gran Aurélien Geron (tercera edición). Si bien este libro está orientado a construir un *background* de tipo más práctico y orientado al código, posee recursos de aprendizaje teóricos muy potentes y que siempre valdrá la pena tener en consideración.
- https://scikit-learn.org/stable/user_guide.html --> La documentación de **Scikit-Learn**. Se trata de un recurso maravilloso, puesto que es muy concisa en relación a lo que necesitamos saber antes de implementar rápidamente cualquier solución basada en algoritmos de aprendizaje.