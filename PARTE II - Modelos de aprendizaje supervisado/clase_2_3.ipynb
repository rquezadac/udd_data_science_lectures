{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55617163-7e43-4e3c-a88b-d4ad3441feeb",
   "metadata": {},
   "source": [
    "## CLASE 2.3: Modelos de clasificación.\n",
    "---\n",
    "## Introducción.\n",
    "Anteriormente establecimos que hay varios problemas propios que se derivan de los algoritmos de aprendizaje supervisado, siendo los más relevantes los problemas de regresión y de clasificación. Es común que los problemas de regresión suelan abordarse en los cursos más elementales de las carreras de ingeniería y ciencias, debido a que muchos estudios empíricos implican la construcción de ciertos tipos de modelos que intentan predecir variables continuas con funciones de densidad desconocida, razón por la cual el problema de regresión no será desarrollado desde sus bases más fundamentales. Sin embargo, este no es el caso para los modelos de clasificación, los cuales, en la literatura básica de estadística y ciencias, suelen ser mucho menos populares.\n",
    "\n",
    "En esta sección nos dedicaremos a estudiar únicamente problemas relativos a la construcción de modelos de clasificación. Esto es, modelos que intentan predecir variables que son discretas, típicamente con una cantidad muy pequeña de valores posibles, y que solemos denominar como **categorías**.\n",
    "\n",
    "## El problema.\n",
    "Como comentamos previamente, un **problema de clasificación** es uno tal donde deseamos construir una función $f$ (hipótesis, conforme la terminología aprendida al estudiar el principio de minimización del riesgo empírico), tal que, para un conjunto $\\mathcal{D} =\\left\\{ \\left( \\mathbf{X} ,\\mathbf{y} \\right)  :\\mathbf{X} =\\left\\{ x_{ij}\\right\\}  \\in \\mathbb{R}^{m\\times n} \\wedge \\mathbf{y} \\in \\mathbb{R}^{m} \\right\\}$, se tenga que $\\hat{y}_{i}\\approx f(\\mathbf{x}_{i},\\mathbf{\\theta})$ para toda instancia $i=1,...,m$, donde $\\hat{y}_{i}$ es la estimación que realiza $f$ a partir del vector $\\mathbf{x}_{i}=(x_{i1},...,x_{in})^{\\top}$. Este problema se lee exactamente igual a uno de regresión, pero hay un detalle: La variable de salida $\\mathbf{y}=\\mathbb{R}^{m}$ sólo puede tomar un número finito de valores, los que son llamados **clases** o **categorías**.\n",
    "\n",
    "### Caso binario.\n",
    "En general, los problemas de clasificación suelen asimismo fragmentarse en términos del número de clases que deseamos predecir por medio de un modelo. Un problema en el cual la variable de respuesta $y$ sólo puede tomar dos valores, que por convención se designan con los números 1 y 0, es llamado **problema binario**.\n",
    "\n",
    "Debido a la naturaleza discreta de la variable $y$, es evidente que el enfoque utilizado de forma clásica para ajustar un modelo de regresión resulta inadecuado debido precisamente a que las funciones de costo utilizadas se encargan de medir distancias entre valores reales y estimados por los correspondientes modelos, las que posteriormente son utilizadas como estimaciones de error. Esta idea pierde sentido en un problema de clasificación porque sólo existen dos valores reales de interés.\n",
    "\n",
    "Lo anterior nos motiva a seleccionar otro tipo de funciones de costo para ajustar modelos a un problema de clasificación. Para el caso binario, una elección muy popular corresponde a la **función de entropía cruzada binaria**, la cual puede desarrollarse como sigue: Para una instancia representada por el par $(\\mathbf{x}_{i},y_{i})$ ($1\\leq i\\leq m$, donde $m$ es el número total de instancias), escribimos $p_{i}=P(y_{i}=1|\\mathbf{X}=\\mathbf{x}_{i})$; es decir, $p_{i}$ describe la probabilidad de que $y_{i}$ sea igual a 1 dados los valores de $\\mathbf{x}_{i}$. Siguiendo este razonamiento, tendremos que $1-p_{i}$ será la probabilidad de que $y_{i}$ sea igual a 0, dados los valores de $\\mathbf{x}_{i}$. Definimos el valor de pérdida logarítmica (del inglés *log-loss*) para la instancia $i$, denotado como $L_{i}$, como\n",
    "\n",
    "$$L_{i}  =\\begin{cases}-\\log \\left( p_{i}\\right)  &;\\  \\mathrm{si} \\  y_{i}=1\\\\ -\\log \\left( 1-p_{i}\\right)  &;\\  \\mathrm{si} \\  y_{i}=0\\end{cases}$$\n",
    "<p style=\"text-align: right;\">$(3.1)$</p>\n",
    "\n",
    "El valor $L_{i}$ puede interpretarse como la \"sorpresa\" relativa al resultado real $y_{i}$, con respecto a la probabilidad $p_{i}$ y, en rigor, se trata de una medida de información (en el sentido de la [teoría de Shannon](https://en.wikipedia.org/wiki/Information_content)). Notemos que valor de $L_{i}$ es siempre no negativo, siendo nulo cuando $p_{i}=y_{i}$, lo que implica naturalmente una predicción *perfecta*. Por otro lado, a medida que una predicción se hace peor, el valor de $L_{i}$ crece infinitamente, lo que implica una mayor \"sorpresa\" en el resultado de un modelo. Notemos que, a diferencia de lo que ocurre en un problema de regresión, donde es posible que todas las instancias presenten un valor de costo o pérdida igual a cero para todas las instancias del conjunto de entrenamiento, en un modelo ajustado con una función de costo de tipo *log-loss*, esto no es posible, puesto que $y_{i}$ sólo puede tomar los valores 0 o 1, pero $0<p_{i}<1$.\n",
    "\n",
    "Los valores de pérdida para $y_{i}=1$ y $y_{i}=0$ pueden combinarse en una única expresión, definida como\n",
    "\n",
    "$$\\ell_{i} =-y_{i}\\log \\left( p_{i}\\right)  -\\left( 1-y_{i}\\right)  \\log \\left( 1-p_{i}\\right)$$\n",
    "<p style=\"text-align: right;\">$(3.2)$</p>\n",
    "\n",
    "**<font color='blue'>Definición 3.1 – Función de entropía cruzada binaria:</font>** La función $\\ell_{i}$ definida en la ecuación (3.2) se denomina **función de entropía cruzada binaria** asociada a la distribución predicha (que solemos denotar como $(p_{i},1-p_{i})$), y con la cual intentamos explicar la distribución real $(y_{i},1-y_{i})$, para la $i$-ésima instancia de entrenamiento. \n",
    "\n",
    "La suma de todos estos valores, a saber\n",
    "\n",
    "$$\\ell =\\sum^{m}_{i=1} \\left( -y_{i}\\log \\left( p_{i}\\right)  -\\left( 1-y_{i}\\right)  \\log \\left( 1-p_{i}\\right)  \\right)$$\n",
    "<p style=\"text-align: right;\">$(3.3)$</p>\n",
    "\n",
    "es llamada **entropía cruzada global** asociada al conjunto de entrenamiento $\\mathcal{D} =\\left\\{ \\left( \\mathbf{X} ,\\mathbf{y} \\right)  :\\mathbf{X} =\\left\\{ x_{ij}\\right\\}  \\in \\mathbb{R}^{m\\times n} \\wedge \\mathbf{y} \\in \\mathbb{R}^{m} \\right\\}$. \n",
    "\n",
    "El modelo más simple que podemos ajustar para un problema de clasificación caracterizado por el conjunto de entrenamiento $\\mathcal{D}$ es también un modelo lineal, parecido al modelo de regresión lineal. De manera más general, se trata de un **modelo lineal generalizado** (MLG), que busca ajustar una función del tipo\n",
    "\n",
    "$$p\\left( \\mathbf{y}=1|\\mathbf{X}\\right)  =\\frac{1}{1+\\exp \\left( -\\sum\\nolimits^{n}_{j=1} \\theta_{j} \\mathbf{x}_{j}+b\\right)}$$\n",
    "<p style=\"text-align: right;\">$(3.4)$</p>\n",
    "\n",
    "llamada **función logística**. Los parámetros $b$ y $\\theta_{1},...,\\theta_{n}$, aglutinados en el vector $\\mathbf{\\theta}\\in \\mathbb{R}^{n}$, son coeficientes que deseamos determinar y que son el resultado de la **minimización** del valor de la función (3.3). Como en el caso del modelo de regresión lineal, $b$ es un parámetro de sesgo que podemos incorporar en la primera posición asociada al vector $\\mathbf{\\theta}$, siempre que asociemos al lado izquierdo de la matriz $\\mathbf{X}$ una columna únicamente compuesta por 1s. El modelo así definido es llamado **modelo de regresión logística binaria**.\n",
    "\n",
    "En vez de minimizar el valor de $\\ell$, podemos optar por maximizar su inverso aditivo $-\\ell$, el cual suele denominarse en estadística como **función de verosimilitud (o pérdida) logarítmica** (positiva) (del inglés, *log-likelihood* o *log-loss*) o **función de soporte**. El nombre de esta función de costo se debe a que, por propiedades de los logaritmos, es posible reescribir el valor negativo de (3.4) como\n",
    "\n",
    "$$L=\\prod_{i:y_{i}=1} p_{i}\\prod_{i:y_{i}=0} \\left( 1-p_{i}\\right)$$\n",
    "<p style=\"text-align: right;\">$(3.5)$</p>\n",
    "\n",
    "Por tanto, la estimación de $b$ y $\\mathbf{\\theta}$ deriva en una implementación del **método de máxima verosimilitud**.\n",
    "\n",
    "No ahondaremos, por ahora, en más detalles relativos al modelo de regresión logística binaria, ya que su uso tenía como objetivo la caracterización de un problema de clasificación binaria. Puntualmente, estábamos interesados en desarrollar la función de costo más simple que podemos asociar a este problema, y que por supuesto se encuentra disponible en la librería **<font color='mediumorchid'>Scikit-Learn</font>**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f70fe23-1868-4302-8414-99804b988494",
   "metadata": {},
   "source": [
    "### Caso multinomial.\n",
    "Consideremos exactamente el mismo *set-up* anterior, donde $\\mathcal{D} =\\left\\{ \\left( \\mathbf{X} ,\\mathbf{y} \\right)  :\\mathbf{X} =\\left\\{ x_{ij}\\right\\}  \\in \\mathbb{R}^{m\\times n} \\wedge \\mathbf{y} \\in \\mathbb{R}^{m} \\right\\}$ es un conjunto de entrenamiento asociado a un problema de clasificación que deseamos resolver. Este problema es de tipo **multinomial** cuando la variable dependiente $\\mathbf{y}\\in \\mathbb{R}^{m}$ puede tomar un número finito (comúnmente pequeño), digamos $k\\in \\mathbb{N}$, de valores distintos, cada uno de los cuales es llamado **clase** o **categoría** asociada a $\\mathbf{y}$.\n",
    "\n",
    "En un contexto así, un *approach* conveniente resulta de fragmentar el vector $\\mathbf{y}$ en términos de las las $k$ clases que caracterizan sus valores en cada una de las instancias del conjunto de entrenamiento. De esta manera, un problema multinomial puede ser representado como un conjunto de $k$ problemas binarios distintos caracterizados por los conjuntos de entrenamiento $\\mathcal{D} =\\left\\{ \\left( \\mathbf{X} ,\\mathbf{y}_{s} \\right)  :\\mathbf{X} =\\left\\{ x_{ij}\\right\\}  \\in \\mathbb{R}^{m\\times n} \\wedge \\mathbf{y}_{s} \\in \\mathbb{R}^{m} \\wedge s=1,...,k\\right\\}$, donde cada problema es resuelto por medio de una estimación de máxima verosimilitud tal y como la que hemos descrito previamente. Naturalmente, hay algunos problemas que pueden derivarse de una estrategia de este tipo, siendo el más importante, en términos prácticos, el hecho de que una variable dependiente con una cantidad moderamente alta de clases implicará también una cantidad *moderamente alta* de modelos que tendremos que entrenar, lo que por supuesto implicará, eventualmente, tiempos de ejecución altos, los que pueden mitigarse en términos de cómo evaluaremos los resultados de cada modelo. Esto es algo que revisaremos más adelante, al ir describiendo las opciones que **<font color='mediumorchid'>Scikit-Learn</font>** nos ofrece para tales efectos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c1801d-d98f-463a-bb9c-831b7a83b97a",
   "metadata": {},
   "source": [
    "## Algoritmo de gradiente descendente (otra vez...).\n",
    "Consideremos un problema de clasificación binaria, caracterizado por el conjunto de entrenamiento $\\mathcal{D} =\\left\\{ \\left( \\mathbf{X} ,\\mathbf{y} \\right)  :\\mathbf{X} =\\left\\{ x_{ij}\\right\\}  \\in \\mathbb{R}^{m\\times n} \\wedge \\mathbf{y} \\in \\mathbb{R}^{m} \\right\\}$, donde $m$ es el número de instancias que constituyen dicho conjunto y $n$ el número de variables independientes que constituyen la matriz de atributos del problema. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
